{"pages":[{"title":"About Me","text":"Writing: A Soliloquy of the Soul“Writing is a soliloquy of the soul, a flow of the spirit.” - Wang Xiaobo I am ikkkp. In fact, the idea of creating my own personal website and sharing things I love on it has been in my mind for quite some time. The reason I write might not be very complicated. Words are my bridge to the world, a tool with which I think, feel, and touch life. Writing, for me, is a way to find companionship in solitude, a beacon of direction in moments of confusion. Words annotate life, and writing reveals the inner self. In this noisy world, writing has found me a sanctuary, a moment of tranquility. Writing allows me to think more clearly, to understand myself more profoundly. Every time I transform my thoughts into words, it’s like giving my soul a baptism, a process that helps me comprehend the depths of my inner world. Writing is also my way of connecting with others. Through words, I can convey my thoughts, share my experiences, and build profound emotional bonds with others. In the ocean of words, we can sail without constraints, exploring endless possibilities.","link":"/en/abouts/index.html"}],"posts":[{"title":"301/302 Redirection","text":"IntroductionWhen configuring redirects on a website, especially in the following scenarios, issues can arise: Redirecting from HTTP to HTTPS: Suppose you’ve set up an SSL certificate to upgrade your website from HTTP to HTTPS. If problems occur during this process, rendering the site inaccessible, you might consider reverting to the HTTP version. However, the challenge arises once a permanent 301 redirection is in place. Even if you remove the redirection on the server, browsers retain this information. Consequently, users’ browsers continue to enforce the HTTPS redirection, preventing them from accessing the HTTP version. Changing the Website Domain: When migrating a website from one domain (such as old-domain.com) to another (such as new-domain.com), a permanent 301 redirection is often employed. This informs search engines and browsers that the site has permanently moved to a new domain. However, if complications arise during this process, you may wish to undo the redirection, allowing users to access the old domain again. Unfortunately, due to browser hard caching of 301 redirections, users become permanently redirected to the new domain, unable to revisit the old one. To avoid such situations, it is advisable to use a temporary 302 redirection initially, ensuring everything functions correctly. Unlike a 301 redirection, a 302 redirection is not permanently cached by browsers. This means that if necessary, you can revert the redirection without users being permanently locked into the new URL. This approach eliminates the need for users to manually clear their browser caches, enhancing the overall user experience. 301 Redirection: Indicates that a resource (page) has permanently moved to a new location. The client&#x2F;browser should not attempt to request the original location but use the new location from now on. 302 Redirection: Indicates that the resource is temporarily located elsewhere, and the client&#x2F;browser should continue requesting the original URL. A 301 redirection is permanent, meaning that even if removed from the server, browsers will perpetually redirect resources to the new domain or HTTPS due to hard caching. On the other hand, a 302 redirection is not hard-cached by browsers. If you remove the redirection from your server (website), you can still access the old version. Clearing 301&#x2F;302 redirection cache typically involves clearing browser cache or the operating system’s DNS cache. Here’s how to do it on different platforms: Clearing Browser Cache (Applicable to Windows, macOS, Linux)Google Chrome: Open the Chrome browser. Click the three vertical dots in the upper right corner and select “More tools.” Choose “Clear browsing data.” In the popup window, select the “Advanced” tab. Set the time range to “All time.” Check the “Cached images and files” option. Click the “Clear data” button. Mozilla Firefox: Open the Firefox browser. Click the three horizontal lines in the upper right corner and select “Privacy &amp; Security.” In the “Cookies and Site Data” section, click “Clear Data.” Ensure the “Cache” option is checked. Click “Clear.” Microsoft Edge: Open the Edge browser. Click the three horizontal dots in the upper right corner and select “Settings.” Scroll down and click “View advanced settings.” In the “Privacy and services” section, click “Clear browsing data.” Check the “Cached images and files” option. Click the “Clear” button. Clearing Operating System’s DNS Cache (Applicable to Windows, macOS)Windows: Open Command Prompt (search for “cmd” in the Start menu and open it). Enter the following command and press Enter:ipconfig &#x2F;flushdns macOS: Open Terminal (find it in Applications &gt; Utilities folder). Enter the following command and press Enter:sudo dscacheutil -flushcache Then enter your administrator password and press Enter again. Please note that clearing browser cache might lead to loss of login sessions on websites. Ensure you have backed up essential information in case you need to log in again. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/28/en/301-302-Redirection/"},{"title":"Cloud Computing and Its Applications","text":"Definition of Cloud ComputingCloud computing is a type of service related to information technology, software, and the internet that provides on-demand, dynamically scalable, and inexpensive computing services through a network. Cloud computing is a pay-per-use model that provides available, convenient, and on-demand network access to a shared pool of configurable computing resources (including networks, servers, storage, application software, and services) that can be rapidly provisioned. History of Cloud ComputingIn March 2006, Amazon launched the Elastic Compute Cloud (EC2) service. On August 9, 2006, Google CEO Eric Schmidt first proposed the concept of “cloud computing” at the Search Engine Strategies conference (SES San Jose 2006). Google’s “cloud computing” originated from the “Google 101” project by Google engineer Christopher Beshlia. In October 2007, Google and IBM began promoting cloud computing on American university campuses. On February 1, 2008, IBM (NYSE: IBM) announced the establishment of the world’s first cloud computing center for Chinese software companies in the Wuxi Taihu New City Science and Education Industrial Park. On July 29, 2008, Yahoo, HP, and Intel announced a joint research project to launch a cloud computing research test bed to promote cloud computing. On August 3, 2008, the US Patent and Trademark Office website showed that Dell was applying for the “cloud computing” trademark to strengthen its control over the term that could reshape future technology architecture. In March 2010, Novell and the Cloud Security Alliance (CSA) jointly announced a vendor-neutral plan called the “Trusted Cloud Initiative.” In July 2010, the US National Aeronautics and Space Administration and supporting vendors such as Rackspace, AMD, Intel, and Dell announced the “OpenStack” open source code plan. Microsoft announced its support for the integration of OpenStack and Windows Server 2008 R2 in October 2010, while Ubuntu added OpenStack to version 11.04. In February 2011, Cisco Systems officially joined OpenStack and focused on developing OpenStack’s network services. Technical Background of Cloud ComputingCloud computing is the commercial implementation of concepts in computer science such as parallel computing, distributed computing, and grid computing. Cloud computing is the result of the mixed evolution and improvement of technologies such as virtualization, utility computing, Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).","link":"/2023/11/08/en/Cloud-Computing/"},{"title":"Webpack Performance Optimization-1","text":"IntroductionLet’s talk about why optimization is necessary. If your project is small and builds quickly, you might not need to worry too much about performance optimization. However, as your project grows with more pages, features, and business logic, the build time of webpack, the underlying build tool, also increases. At this point, optimizing performance becomes crucial. Webpack offers various avenues for performance optimization, which can be broadly categorized into two areas: Optimization One: Optimizing the built result for production, focusing on performance during deployment (e.g., code splitting, reducing bundle size, using CDN servers, etc.).Optimization Two: Optimizing build speed for development or production build, enhancing the speed of the build process (e.g., using exclusion, cache loaders, etc.).The performance during production directly affects user experience, whereas build time is closely related to developers’ daily workflow. If the local development server or production build takes too long, it significantly hampers productivity. Performance Optimization - Code SplittingCode splitting is a critical feature in webpack: Its primary purpose is to separate code into different bundles, which can be loaded on demand or in parallel.By default, all JavaScript code (business logic, third-party dependencies, and modules not immediately used) is loaded on the initial page load, impacting the loading speed.Code splitting allows creating smaller bundles and controlling resource loading priorities, thereby enhancing code loading performance. Webpack provides three common approaches to code splitting: Entry Points: Manually splitting code using entry configuration Preventing Duplication: Avoiding duplicate code using Entry Dependencies or SplitChunksPlugin Dynamic Imports: Splitting code using inline functions in modules Optimizing Entry Points - Entry DependenciesWhen a project has multiple entry points, there might be issues with duplicate dependencies. Some modules might be referenced in multiple entry points, causing redundancy in the final output, increasing the output file size. module.exports = &#123; entry: &#123; page1: &#123; import: './src/page1.js', dependOn: 'shared', &#125;, page2: &#123; import: './src/page2.js', dependOn: 'shared', &#125;, shared: './src/shared.js', &#125;, output: &#123; filename: '[name].bundle.js', path: __dirname + '/dist', &#125;, &#125;; Dynamic ImportsDynamic imports are a technique in webpack for lazy loading, allowing modules to load asynchronously at runtime instead of bundling all modules into a large initial file. This approach improves the initial loading speed and reduces the initial bundle size. const path = require('path'); module.exports = &#123; entry: &#123; main: './src/index.js', &#125;, output: &#123; filename: '[name].bundle.js', path: path.resolve(__dirname, 'dist'), publicPath: '/', &#125;, module: &#123; rules: [ // Add your loader rules here ], &#125;, optimization: &#123; splitChunks: &#123; chunks: 'all', &#125;, &#125;, &#125;; In the above configuration, code splitting is achieved using optimization.splitChunks with the option chunks: &#39;all&#39;. Then, dynamic imports can be used in the code like this: // Dynamically import modules where needed const loadModule = () => import('./Module'); loadModule().then(module => &#123; // Use the loaded module &#125;); Webpack will split the modules imported using import() into separate files. These files will be loaded asynchronously when needed during runtime. Custom Bundle Splitting - SplitChunksBundle splitting is an optimization strategy that allows breaking down code into smaller pieces, enabling faster content display during loading. Webpack provides various strategies for bundle splitting, one of which involves using the SplitChunksPlugin plugin. This strategy is known as splitChunks. module.exports = &#123; // ...other configurations optimization: &#123; splitChunks: &#123; chunks: 'all', minSize: 30000, // Minimum size of the module before splitting minChunks: 1, // Minimum number of times a module should be duplicated before splitting maxAsyncRequests: 5, // Maximum number of parallel requests when loading modules on demand maxInitialRequests: 3, // Maximum number of parallel requests at an entry point automaticNameDelimiter: '~', name: true, cacheGroups: &#123; vendors: &#123; test: /[\\\\/]node_modules[\\\\/]/, priority: -10, reuseExistingChunk: true, &#125;, default: &#123; minChunks: 2, priority: -20, reuseExistingChunk: true, &#125;, &#125;, &#125;, &#125;, &#125;; For more information, refer to the webpack-split-chunks-plugin documentation. Performance Optimization - CDNCDN, or Content Delivery Network, refers to a network of interconnected servers strategically placed to deliver content to users efficiently. It ensures faster and more reliable delivery of resources such as music, images, videos, applications, and other files by utilizing servers closest to each user, providing high performance, scalability, and low-cost content delivery. In development, CDN is typically used in two ways: All static resources are bundled and stored on a CDN server, and users load resources exclusively through the CDN. Some third-party resources are hosted on CDN servers. Utilizing a Content Delivery Network (CDN) is a highly effective performance optimization strategy, especially within Webpack. CDN accelerates website loading speed, reduces server load, and enhances user experience. Here’s how you can configure and use CDN in Webpack: Using CDN for Third-Party LibrariesIntegrate third-party libraries used in your project (such as React, Vue, jQuery, etc.) through CDN links directly in the HTML file: &lt;script src=\"https://cdn.jsdelivr.net/npm/react@version/dist/react.min.js\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/react-dom@version/dist/react-dom.min.js\">&lt;/script> Configuring Externals in WebpackIn your Webpack configuration, utilize the externals field to inform Webpack about externally referenced modules that shouldn’t be bundled: module.exports = &#123; // ...other configurations externals: &#123; react: 'React', 'react-dom': 'ReactDOM', &#125;, &#125;; Then, include the CDN links in the HTML file using script tags: &lt;script src=\"https://cdn.jsdelivr.net/npm/react@version/dist/react.min.js\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/react-dom@version/dist/react-dom.min.js\">&lt;/script> Configuring CDN’s publicPathIn the Webpack output field, set the publicPath to specify the URL prefix for resource imports, typically set to the CDN’s address: module.exports = &#123; // ...other configurations output: &#123; // ...other output configurations publicPath: 'https://cdn.example.com/', &#125;, &#125;; This way, during Webpack build, all resource paths will be prefixed with the CDN’s address. Performance Optimization - Extracting CSS FilesExtracting CSS files from JavaScript bundles is a common performance optimization strategy. This approach reduces the size of JavaScript files, speeds up page loading, and allows browsers to download CSS and JavaScript files in parallel, enhancing loading performance. In Webpack, you can achieve this using the mini-css-extract-plugin plugin. Webpack ConfigurationIn your Webpack configuration file, include the mini-css-extract-plugin plugin and configure the module.rules to handle CSS files: const MiniCssExtractPlugin = require('mini-css-extract-plugin'); module.exports = &#123; // ...other configurations module: &#123; rules: [ &#123; test: /\\.css$/, use: [ MiniCssExtractPlugin.loader, 'css-loader', // Additional CSS loaders like postcss-loader and sass-loader can be added here ], &#125;, ], &#125;, plugins: [ new MiniCssExtractPlugin(&#123; filename: 'styles.css', // Filename for the extracted CSS file &#125;), ], &#125;; Including CSS FilesIn your JavaScript files or entry point file, import the CSS file: import './styles.css'; Alternatively, in the HTML file, use the link tag to include the extracted CSS file: &lt;link rel=\"stylesheet\" href=\"styles.css\"> Performance Optimization - Bundling File Naming (Hash, ContentHash, ChunkHash)In Webpack, how files are named during bundling is a crucial performance optimization strategy. Proper naming ensures that browsers can cache files correctly, avoiding unnecessary network requests and improving application loading speed. Here are three common bundling file naming techniques: Hash, ContentHash, and ChunkHash. HashHash is generated based on file content. When file content changes, its corresponding hash value also changes. In Webpack, you can use the [hash] placeholder to represent the hash value. output: &#123; filename: 'bundle.[hash].js', &#125; ContentHashContentHash is generated based on file content as well, but unlike Hash, it’s solely influenced by file content and remains unaffected by file name or path changes. In Webpack, you can use the [contenthash] placeholder to represent the ContentHash value. output: &#123; filename: 'bundle.[contenthash].js', &#125; ChunkHashChunkHash is generated based on module content. Different module contents result in different ChunkHash values. In Webpack, you can use the [chunkhash] placeholder to represent the ChunkHash value. output: &#123; filename: '[name].[chunkhash].js', &#125; Performance Optimization - Implementing Tree Shaking in WebpackJavaScript Tree Shaking: Tree Shaking in JavaScript originates from the rollup bundler, a build tool. It relies on the static syntax analysis of ES Modules (no code execution) to determine module dependencies. Webpack 2 introduced native support for ES2015 modules, enhancing tree shaking capabilities. Webpack 4 extended this ability and introduced the sideEffects property in package.json to indicate which files have no side effects, allowing webpack to safely remove unused code. In Webpack 5, partial CommonJS tree shaking support was introduced. CommonJS Tree Shaking Implementing Tree Shaking in JavaScriptWebpack implements Tree Shaking through two methods: usedExports: Marking certain functions as used and optimizing them using Terser. sideEffects: Skipping entire modules&#x2F;files and checking if they have side effects. Tree Shaking for CSSTree Shaking for CSS involves using additional plugins. While PurifyCss was used previously, it’s no longer maintained. An alternative is PurgeCSS, a tool for removing unused CSS. Note: This translation includes placeholder strings like [hash], [contenthash], and [chunkhash] to represent dynamic values. Please replace these placeholders with appropriate values based on your specific use case. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/26/en/Webpack-optimization-1/"},{"title":"Webpack Performance Optimization-2","text":"Performance Optimization - JS-CSS Code Minification Terser is a toolset for JavaScript parsing, mangling, and compressing. In the early days, we used uglify-js to minify and uglify our JavaScript code. However, it is no longer maintained and does not support ES6+ syntax. Terser is a fork of uglify-es and retains most of its original APIs, compatible with uglify-es and uglify-js@3, etc. webpack-terser JavaScript Code MinificationWebpack provides the terser-webpack-plugin plugin for code optimization and minification. In production mode, TerserPlugin is used by default for code processing. const TerserPlugin = require('terser-webpack-plugin'); module.exports = &#123; // Configure other Webpack options... optimization: &#123; minimizer: [new TerserPlugin()], &#125;, &#125;; CSS Code MinificationApart from JavaScript code, CSS code can also be minified using Webpack. Use css-minimizer-webpack-plugin to compress CSS code. const CssMinimizerPlugin = require('css-minimizer-webpack-plugin'); module.exports = &#123; // Configure other Webpack options... optimization: &#123; minimizer: [ new CssMinimizerPlugin(), // You can continue adding other compression plugins... ], &#125;, &#125;; Tree Shaking Implementation in WebpackTree shaking is a term commonly used to describe the removal of dead code in JavaScript context. Tree Shaking in WebpackIn modern front-end development, optimizing code size is a crucial topic. Tree shaking is an optimization technique used to eliminate unused JavaScript modules in a project, reducing the size of the bundled files. Webpack provides built-in support, making it easy to implement tree shaking in projects. Enable ES Module SyntaxFirst, ensure your JavaScript code follows ES module syntax, as Webpack’s tree shaking feature only works with ES modules. Use import and export syntax to define modules in your project. // math.js export function square(x) &#123; return x * x; &#125; export function cube(x) &#123; return x * x * x; &#125; Webpack ConfigurationIn the Webpack configuration file, ensure the following settings to enable tree shaking: Set mode to &#39;production&#39;. Webpack will automatically enable related optimizations, including tree shaking. Implementing Tree Shaking for JavaScriptWebpack implements tree shaking using two different approaches: usedExports: Marks certain functions as used, and later optimizes them with Terser. sideEffects: Skips entire modules&#x2F;files and checks if the file has side effects. Using usedExports to Implement Tree ShakingSet the mode to production: module.exports = &#123; mode: 'production', // ...other configurations &#125;; Configure usedExports in the optimization section: const path = require('path'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: path.resolve(__dirname, 'dist'), &#125;, mode: 'development', optimization: &#123; usedExports: true, &#125;, &#125;; Using sideEffects to Implement Tree ShakingSet the sideEffects field in package.json: Set it to false to inform Webpack that it can safely remove unused exports. If there are specific files you want to keep, set it as an array. &#123; \"name\": \"your-project\", \"sideEffects\": [\"./src/some-side-effectful-file.js\"] &#125; Webpack Side Effects Understanding Tree Shaking and sideEffectssideEffects and usedExports (more commonly considered tree shaking) are two different optimization techniques. sideEffects is more efficient as it allows skipping entire modules&#x2F;files and their entire subtree. usedExports depends on terser to detect side effects in statements. It’s a more complex JavaScript task and is not as straightforward as sideEffects. Also, it cannot skip subtrees&#x2F;dependencies because side effects need to be evaluated. While exported functions work as usual, higher-order functions (HOC) in the React framework can have issues in this scenario. CSS Tree Shaking ImplementationFor CSS tree shaking, additional plugins are required. In the past, PurifyCss plugin was used for CSS tree shaking, but it’s no longer maintained (last update was 4 years ago). A different library, PurgeCSS, can now be used for CSS tree shaking, helping remove unused CSS. File Compression in WebpackWhat is HTTP CompressionHTTP compression is a technique used between servers and clients to improve transmission speed and bandwidth utilization.The process of HTTP compression is as follows: Data is compressed on the server before being sent. (Can be done in Webpack) Compatible browsers inform the server about supported compression formats during requests. The server returns the corresponding compressed file to the browser, indicating it in the response headers. Popular Compression FormatsThere are several popular compression formats: compress: Method used by UNIX’s “compress” program (historical reasons, not recommended for most applications, use gzip or deflate instead). deflate: Compression based on the deflate algorithm (defined in RFC 1951) and encapsulated in zlib data format. gzip: GNU zip format (defined in RFC 1952), widely used compression algorithm. br: A new open-source compression algorithm designed specifically for HTTP content encoding. Webpack Configuration for File CompressionWebpack essentially performs the first step of HTTP compression. You can use the CompressionPlugin for this purpose. Step 1: Install CompressionPlugin: npm install compression-webpack-plugin -D Step 2: Use CompressionPlugin in your Webpack configuration: module.exports = &#123; plugins: [ new CompressionPlugin(&#123; test: /\\.js(\\?.*)?$/i, &#125;), ], &#125;; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/Webpack-optimization-2/"},{"title":"How to Use `sideEffects` in Webpack","text":"Webpack v4 introduced a new feature called sideEffects, which allows you to declare in your package.json whether a package&#x2F;module contains side effects or not. This declaration provides more optimization space for tree-shaking. In the conventional understanding of side effects, if we are certain that the modules within our package have no side effects, we can mark the package in npm with &quot;sideEffects&quot;: false in package.json. This allows us to offer a better bundling experience for consumers. The principle behind this is that Webpack can transform imports like import &#123;a&#125; from xx into import &#123;a&#125; from &#39;xx/a&#39; for packages marked as side-effects-free, automatically trimming unnecessary imports, similar to babel-plugin-import. Tree Shaking and Side EffectsTree shaking, first introduced and implemented by Rollup in the frontend community, has been a topic of discussion in various articles about optimizing bundling. Principles of Tree ShakingES6 module imports are statically analyzable, meaning the compiler can accurately determine what code is loaded during compilation. The program flow is analyzed to identify unused or unreferenced variables, which are then removed from the code. The principle sounds perfect, so why do we sometimes find that unnecessary code in our projects isn’t eliminated? The reason is side effects. Side EffectsFor those familiar with functional programming, the term “side effect” is not unfamiliar. It can be broadly understood as any action of a function that might or might not affect variables outside its scope. For example, consider this function: function go(url) &#123; window.location.href = url; &#125; This function modifies the global variable location and even triggers a browser redirect, making it a function with side effects. // components.js export class Person &#123; constructor(&#123; name &#125;) &#123; this.className = 'Person'; this.name = name; &#125; getName() &#123; return this.name; &#125; &#125; export class Apple &#123; constructor(&#123; model &#125;) &#123; this.className = 'Apple'; this.model = model; &#125; getModel() &#123; return this.model; &#125; &#125; // main.js import &#123; Apple &#125; from './components'; const appleModel = new Apple(&#123; model: 'IphoneX' &#125;).getModel(); console.log(appleModel); In this code, the Person class is clearly unused. However, why can other tools like Rollup successfully eliminate unused code, while Webpack cannot? The answer lies in Babel compilation + Webpack bundling. I’ll provide a link here that explains in detail how Babel compilation + Webpack bundling might prevent effective code elimination: Your Tree-Shaking Isn’t Working. If you don’t want to read the article, here’s a brief explanation: Babel compilation wraps the Person class in an IIFE (Immediately Invoked Function Expression) and returns a constructor, introducing a side effect. There’s an issue related to this: Class declarations inside IIFEs are considered side effects. When I declare a class inside an IIFE and don’t use the class, UglifyJS doesn’t remove it because it’s considered a side effect. var V6Engine = (function () &#123; function V6Engine() &#123; &#125; V6Engine.prototype.toString = function () &#123; return 'V6'; &#125;; return V6Engine; &#125;()); During compilation, you might receive this warning: WARN: Side effects in initialization of unused variable V6Engine [./dist/car.bundle.js:74,4]. The reason is that UglifyJS doesn’t perform complete program flow analysis. It doesn’t remove code because you noticed a side effect. If you want a more sophisticated tree shaking, go check out Rollup! Summarizing some key points from the issue: If a function’s parameter is a reference type, any operations on its properties could potentially have side effects. This is because it’s a reference type, and any modification to its properties affects data outside the function. Additionally, accessing or modifying its properties triggers getter or setter, which are opaque and may have side effects. UglifyJS lacks complete program flow analysis. It can simple judge whether a variable is later referenced or modified but cannot determine the complete modification process of a variable. It doesn’t know if it points to an external variable, so many potentially side-effect-causing code cannot be removed. Rollup has the ability to perform program flow analysis, making it better at determining whether code truly has side effects. However, these issues were prevalent in older versions. The current Webpack tree shaking has undergone many optimizations and can perform sufficient program flow analysis for tree shaking. The purpose of Webpack’s tree shaking is to mark unused exported members as unused and not export them in the modules where they are re-exported. It sounds complicated, but looking at the code makes it clearer: // a.js export function a() &#123;&#125; // b.js export function b()&#123;&#125; // package/index.js import a from './a' import b from './b' export &#123; a, b &#125; // app.js import &#123;a&#125; from 'package' console.log(a) When using app.js as the entry point, the code after tree shaking becomes: // a.js export function a() &#123;&#125; // b.js is no longer exported: function b()&#123;&#125; function b() &#123;&#125; // package/index.js does not export module b anymore import a from './a' import './b' export &#123; a &#125; // app.js import &#123;a&#125; from 'package' console.log(a) After combining Webpack’s scope hoisting and uglify, all traces of module b will be completely eliminated. But what if module b contains some side effects, such as a simple log: // b.js export function b(v) &#123; return v &#125; console.log(b(1)) After webpack, the content of module `b` becomes: // b.js console.log(function (v)&#123;return v&#125;(1)) Although the export of module b is ignored, the code with side effects is retained. Due to various strange operations introduced by the transformer after compilation, which may cause side effects, we often find that even with tree shaking, our bundle size doesn’t significantly decrease. Usually, we expect that if module b is not being used, none of its code should be included. This is where the role of sideEffects becomes apparent: if the imported package&#x2F;module is marked as &quot;sideEffects: false&quot;, regardless of whether it truly has side effects, as long as it’s not being referenced, the entire module&#x2F;package will be completely removed. Taking mobx-react-devtools as an example, we often use it like this: import DevTools from 'mobx-react-devtools'; class MyApp extends React.Component &#123; render() &#123; return ( &lt;div> ... &#123; process.env.NODE_ENV === 'production' ? null : &lt;DevTools /> &#125; &lt;/div> ); &#125; &#125; This is a common scenario of importing modules on demand. However, without the sideEffects: false configuration, even if NODE_ENV is set to production, the bundled code will still include the mobx-react-devtools package. Although we haven’t used any of its exported members, mobx-react-devtools will still be imported because it “might” have side effects. But when we add sideEffects: false, tree shaking can safely remove it entirely from the bundle. Use Cases of sideEffectsAs mentioned earlier, it’s often difficult to guarantee whether packages&#x2F;modules published on npm contain side effects (it could be the code’s fault or the transformer’s fault). However, we can usually ensure whether a package&#x2F;module will affect objects outside of it, such as modifying properties on the window object or overwriting native object methods. If we can guarantee this, we can determine whether a package can have &quot;sideEffects: false&quot;. Whether it truly has side effects is not that important for Webpack; it’s acceptable as long as it’s marked. This explains why packages with inherent side effects, like vue, can still have &quot;sideEffects: false&quot; applied. So, in Webpack, &quot;sideEffects: false&quot; doesn’t mean that the module truly has no side effects. It’s just a way to tell Webpack during tree shaking: “I designed this package with the expectation that it has no side effects, even if it ends up having side effects after being bundled.” Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/Webpack-optimization-3/"},{"title":"Understanding Ajax and Cross-Origin Requests Easily","text":"IntroductionWhen learning to write web pages, you usually start with HTML and CSS, which are responsible for creating and beautifying the layout. Once you have a solid foundation, you start learning JavaScript to create interactive effects. In addition to user and browser interactions, don’t forget about the interaction between the client and server, which means you must learn how to use JavaScript to retrieve data from the backend server. Otherwise, your web page data will be static. The main target audience of this article is beginners in web front-end development. I hope that after reading this article, readers who do not understand how to exchange data with the server or how to connect to APIs can have a better understanding of how to connect to the backend. Let’s start with an exampleBefore we begin, let’s consider a question: Why does the front-end need to exchange data with the backend? Actually, this depends on the type of web page you are creating. If you are creating an official website, the entire website is likely to be static, and only HTML and CSS are required, without the need to retrieve data from the backend server. Let’s assume that today we want to create a web page that can browse the current Twitch live stream list, as shown below. If this web page does not retrieve data from the backend, it means that the content of the web page is fixed and will remain the same no matter when it is viewed. However, this is not correct because the goal of this web page is to display “channels that are currently live,” so the content will change accordingly. Since the content will change, we must continuously update the data, retrieve data from the server, and then display it after processing it on the front-end. After confirming the need to retrieve data, we can ask ourselves two questions: Who do we retrieve data from? How do we retrieve data? The answer to the first question is obviously Twitch because Twitch has the data we need! As for the second question, we must use the Twitch API. APIWhat is an API? You may have heard this term many times, but still don’t know what it means. Let’s start with its full name, which is “Application Programming Interface.” You may wonder what this is and why I can’t understand it in both Chinese and English. But actually, the most important thing in these few words is the word “interface.” What is an interface? An interface is used for connection. I’ll give you an example. Isn’t there a USB slot on your computer? As long as you see USB flash drives on the market, you can buy them and plug them into the USB slot, and your computer can read them. Have you ever wondered why? Although they are made by different manufacturers, they can all be read and plugged into the USB slot. This is because there is a standard called the USB interface. After this standard was established, as long as all manufacturers develop according to this standard, they can ensure that they can connect to the computer and USB flash drives. API is the same, but it becomes a connection between programs. For example, if I need to read a file in my program, how do I read it? Reading files is a function provided by the operating system, so I can connect to the “read file API” and use this function in my program. I’ll give you a few more examples. Suppose I want to allow my web page to log in with Facebook. What should I do? I need to connect to the “Facebook API,” which is a set of standards provided by Facebook to everyone who wants to access Facebook services. Any developer who wants to access Facebook services can follow these standards to obtain the data they want. This thing is called an API. Or maybe you are a developer of a hotel management system today, and your company has developed an ERP for hotels, which can manage the booking status of hotels and so on, and can know which rooms are empty now. If you only use this data yourself, it would be a pity. Therefore, the company decided to provide this data to large booking websites, which can display the room status of this hotel in real-time on those websites. Therefore, data exchange is necessary, and you need to provide a “query room status API” to other websites so that they can connect to it and obtain this information. By now, you should have some sense of what an API is. Let me give you a few more examples: I want to retrieve photos from Flickr, so I need to connect to the Flickr API. Google wants to allow other apps to log in and authenticate with Google, so Google needs to provide the “Google login API.” I want to retrieve the channels currently available on Twitch, so I need to connect to the Twitch API. API DocumentationNow that we know what an API is and that we need to connect to it, the next question is “how do we connect?” Earlier, we mentioned an example of file access. This is actually more like calling a function provided by the operating system or a programming language library. You can usually find more detailed information about these functions in the official documentation, such as reading files in Node.js: (Source: https://nodejs.org/api/fs.html#fs_fs_readdir_path_options_callback) Above, it is written which function you should call and what parameters you should pass in. API integration is the same. You must have documentation to know how to integrate, otherwise you cannot integrate at all because you don’t even know what parameters to pass. Let’s take a look at how the Twitch API documentation is written. It explains that you must have a Client ID, and the API Root URL is https://api.twitch.tv/kraken, etc. These are basic information related to the API. If you click on any API in the left column, you will see detailed information about each API: Here, it is written what the URL is, what parameters you should pass, etc. There are also reference examples below, which is a very complete API documentation. Usually, when writing web pages, we directly talk about APIs, but actually we are referring to Web APIs, which are APIs transmitted through the network. Are there non-Web APIs? Yes, like the file reading API we mentioned earlier, they are all executed locally on the computer without going through any network. But this doesn’t really matter, everyone is used to talking about APIs, as long as they can understand it. Now that we have the API documentation, we have all the information we need. Using the Twitch example above, as long as we can send a request to https://api.twitch.tv/kraken/games/top?client_id=xxx through JavaScript, Twitch will return the current list of the most popular games. We have narrowed down the scope of the problem step by step. At first, it was “how to get data from Twitch”, and now it is divided into: “how to use JavaScript to send a request”. AjaxTo send a request on the browser, you must use a technology called Ajax, which stands for “Asynchronous JavaScript and XML”, with the emphasis on the word “Asynchronous”. Before talking about what is asynchronous, let’s first mention what is synchronous. Almost all JavaScript you originally wrote is executed synchronously. This means that when it executes to a certain line, it will wait for this line to finish executing before executing the next line, ensuring the execution order. That is to say, the last line of the following code needs to wait for a long time to be executed: var count = 10000000; while(count--) &#123; // Do some time-consuming operations &#125; // Executed after a long time console.log('done') It looks reasonable. Isn’t the program executed line by line? But if it involves network operations, everyone can think about the following example: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // Executed after a long time console.log(result); When JavaScript executes sendRequest, because it is synchronous, it will wait for the response to come back before continuing to do anything. In other words, before the response comes back, the entire JavaScript engine will not execute anything! It’s scary, isn’t it? You click on anything related to JavaScript, and there is no response because JavaScript is still waiting for the response. Therefore, for operations that are expected to be very time-consuming and unstable, synchronous execution cannot be used, but asynchronous execution must be used. What does asynchronous mean? It means that after it is executed, it will not be taken care of, and it will continue to execute the next line without waiting for the result to come back: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // The above request is executed, and then it executes to this line, so result will not have anything // because the response has not returned yet console.log(result); Please note that “asynchronous functions cannot directly return results through return”. Why? Because, as in the example above, after sending a request, the next line will be executed, and at this time, there is no response yet. What should be returned? So what should we do? Let me give you a very common example! When I was eating in a food court in Singapore, there was a table number on each table. When you order, just tell the boss which table you are sitting at, and the boss will deliver it to you after the meal is ready. So I don’t need to stand at the door of the store and wait. I just continue to sit on my own things. Anyway, the boss will deliver it to me after the meal is ready. The concept of asynchronous is also like this. After I send a request (after I order), I don’t need to wait for the response to come back (I don’t need to wait for the boss to finish), I can continue to do my own thing. After the response comes back (after the meal is ready), it will help me deliver the result (the boss will deliver it by himself). In the ordering example, the boss can know where to send the data through the table number. What about in JavaScript? Through Function! And this function, we call it a Callback Function, a callback function. When the asynchronous operation is completed, this function can be called and the data can be brought in. // Assuming there is a function called sendRequest to send a request sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', callMe); function callMe (response) &#123; console.log(response); &#125; // Or write it as an anonymous function sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', function (response) &#123; console.log(response); &#125;); Now you know why network operations are asynchronous and what callback functions are. XMLHttpRequestJust mentioned the concepts of Ajax, asynchronous, and callback functions, but didn’t say how to send a request, just wrote a fake sendRequest function as a reference. To send a request, we need to use an object prepared by the browser called XMLHttpRequest. The sample code is as follows: var request = new XMLHttpRequest(); request.open('GET', `https://api.twitch.tv/kraken/games/top?client_id=xxx`, true); request.onload = function() &#123; if (request.status >= 200 &amp;&amp; request.status &lt; 400) &#123; // Success! console.log(request.responseText); &#125; &#125;; request.send(); The request.onload above actually specifies which function to use to handle the data when it comes back. With the above code, you have finally succeeded and can finally connect to the Twitch API and get data from there! It’s really gratifying. From now on, you will live a happy life with the skill of “connecting to the API”… Not really. Same Origin PolicyJust when you thought you were already familiar with connecting to APIs and wanted to try connecting to other APIs, you found that a problem occurred with just one line: XMLHttpRequest cannot load http:&#x2F;&#x2F;odata.tn.edu.tw&#x2F;ebookapi&#x2F;api&#x2F;getOdataJH&#x2F;?level&#x3D;all. No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource. Origin &#39;null&#39; is therefore not allowed access. Huh? Why is there this error? In fact, for security reasons, the browser has something called the Same-origin policy. This means that if the website you are currently on and the API website you want to call are “different sources”, the browser will still help you send the request, but it will block the response, preventing your JavaScript from receiving it and returning an error. What is a different source? Simply put, if the domain is different, it is a different source, or if one uses http and the other uses https, or if the port numbers are different, it is also a different source. So if you are using someone else’s API, in most cases it will be a different source. I want to emphasize here that “your request is still sent”, and the browser “does receive the response”, but the key point is that “due to the same-origin policy, the browser does not pass the result back to your JavaScript”. If there is no browser, there is actually no such problem. You can send it to whoever you want and get the response no matter what. Okay, since we just said that different sources will be blocked, how did we successfully connect to the Twitch API? CORSAs we all know, it is very common to transfer data between different sources, just like we connect to the Twitch API. How can we be under the same domain as the Twitch API? Therefore, the same-origin policy does regulate that non-same-origin requests will be blocked, but at the same time, there is another regulation that says: “If you want to transfer data between different origins, what should you do?” This regulation is called CORS. CORS, short for Cross-Origin Resource Sharing, is a cross-origin resource sharing protocol. This protocol tells you that if you want to open cross-origin HTTP requests, the server must add Access-Control-Allow-Origin to the response header. You should be familiar with this field. If you feel unfamiliar, you can go back and look at the error message just now, which actually mentioned this header. After the browser receives the response, it will first check the content of Access-Control-Allow-Origin. If it contains the origin of the request that is currently being initiated, it will allow it to pass and allow the program to receive the response smoothly. If you carefully check the request we sent to Twitch in the beginning, you will find that the header of the response is roughly like this: Content-Type: application&#x2F;json Content-Length: 71 Connection: keep-alive Server: nginx Access-Control-Allow-Origin: * Cache-Control: no-cache, no-store, must-revalidate, private Expires: 0 Pragma: no-cache Twitch-Trace-Id: e316ddcf2fa38a659fa95af9012c9358 X-Ctxlog-Logid: 1-5920052c-446a91950e3abed21a360bd5 Timing-Allow-Origin: https:&#x2F;&#x2F;www.twitch.tv The key point is this line: Access-Control-Allow-Origin: *, where the asterisk represents a wildcard character, meaning that any origin is accepted. Therefore, when the browser receives this response, it compares the current origin with the * rule, passes the verification, and allows us to accept the response of the cross-origin request. In addition to this header, there are actually others that can be used, such as Access-Control-Allow-Headers and Access-Control-Allow-Methods, which can define which request headers and methods are accepted. To sum up, if you want to initiate a cross-origin HTTP request and receive a response smoothly, you need to ensure that the server side has added Access-Control-Allow-Origin, otherwise the response will be blocked by the browser and an error message will be displayed. Preflight RequestDo you still remember Twitch’s API documentation? It requires a client-id parameter, and the document says that you can pass it in the GET parameter or in the header. Let’s try passing it in the header! Open Devtool, and you will see a magical phenomenon: Huh? I clearly only sent one request, why did it become two? And the method of the first one is actually OPTIONS. Why did adding one header result in an extra request? In fact, this is also related to CORS mentioned above. CORS divides requests into two types, one is a simple request. What is a simple request? There is actually a long definition, which I think you can read when you need it. But in short, if you don’t add any custom headers, and it’s a GET request, it’s definitely a simple request (isn’t this simple enough?). On the contrary, if you add some custom headers, such as the Client-ID we just added, this request is definitely not a simple request. (Definition reference: MDN: Simple Request) From the above classification, we know that the request we just initiated is not a simple request because it has a custom header. So why is there an extra request? This request is called a Preflight Request, which is used to confirm whether subsequent requests can be sent because non-simple requests may contain some user data. If this Preflight Request fails, the real request will not be sent, which is the purpose of the Preflight Request. Let me give you an example, and you will know why this Preflight Request is needed. Assuming that a server provides an API URL called: https://example.com/data/16, you can get the data with id 16 by sending a GET request to it, and you can delete this data by sending a DELETE request to it. If there is no Preflight Request mechanism, I can send a DELETE request to this API on any web page of any domain. As I emphasized earlier, the browser’s CORS mechanism will still help you send the request, but only the response will be blocked by the browser. Therefore, even though there is no response, the server did receive this request, so it will delete this data. If there is a Preflight Request, when receiving the result of the request, it will know that this API does not provide CORS, so the real DELETE request will not be sent, and it will end here. The purpose of the Preflight Request is to use an OPTIONS request to confirm whether the subsequent request can be sent. JSONPFinally, let’s talk about JSONP, which is another method for cross-origin requests besides CORS, called JSON with Padding. Do you remember the same-origin policy mentioned at the beginning? If you think about it carefully, you will find that some things are not restricted by the same-origin policy, such as the &lt;script&gt; tag. Don’t we often refer to third-party packages such as CDN or Google Analytics on web pages? The URLs are all from other domains, but they can be loaded normally. JSONP uses this feature of &lt;script&gt; to achieve cross-origin requests. Imagine you have an HTML like this: &lt;script> var response = &#123; data: 'test' &#125;; &lt;/script> &lt;script> console.log(response); &lt;/script> It’s a very easy-to-understand piece of code, so I won’t explain it much. What if you replace the above code with a URL? &lt;script src=\"https://another-origin.com/api/games\">&lt;/script> &lt;script> console.log(response); &lt;/script> If the content returned by https://another-origin.com/api/games is the same as before: var response = &#123; data: 'test' &#125;; Then can’t I get the data in the same way? And these data are still controlled by the server, so the server can give me any data. But using global variables like this is not very good. We can use the concept of Callback Function just mentioned and change it to this: &lt;script> receiveData(&#123; data: 'test' &#125;); &lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> So what is JSONP? JSONP actually uses the above format to put data in &lt;script&gt; and bring the data back through the specified function. If you think of the first &lt;script&gt; as the server’s return value, you will understand. In practice, when operating JSONP, the server usually provides a callback parameter for the client to bring over. The Twitch API provides a JSONP version, and we can directly look at the example. URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=aaa&amp;limit=1 aaa(&#123;\"_total\":1069,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65243,\"channels\":373&#125;]&#125;) URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1 receiveData(&#123;\"_total\":1067,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65622,\"channels\":376&#125;]&#125;) Have you noticed? It passes the callback parameter you brought over as the function name and passes the entire JavaScript object to the Function, so you can get the data inside the Function. Combined, it would look like this: &lt;script src=\"https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1\">&lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> Using JSONP, you can also access cross-origin data. However, the disadvantage of JSONP is that the parameters you need to pass can only be passed through the URL in a GET request, and cannot be passed through a POST request. If CORS can be used, it should be prioritized over JSONP. SummaryThe content of this article starts with the process of fetching data and tells you step by step where to fetch it and how to fetch it. If you want to fetch data using an API, what is an API? How to call Web API in JavaScript? How to access cross-origin data? Generally speaking, I have mentioned everything related to fetching data with the front-end, but there is a regret that I did not mention the Fetch API, which is a newer standard used to fetch data. The introduction on MDN is: The Fetch API provides an interface for fetching resources (including across the network). It will seem familiar to anyone who has used XMLHttpRequest, but the new API provides a more powerful and flexible feature set. Interested readers can check it out for themselves. I hope that after reading this article, you will have a better understanding of how to connect to the back-end API and the difficulties you may encounter when connecting. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2017/08/27/en/ajax-and-cors/"},{"title":"MapReduce Working Principle in Hadoop","text":"Definition of MapReduceMapReduce is a programming framework for distributed computing programs. It is the core framework for developing “Hadoop-based data analysis applications”. Its core function is to integrate the user’s written business logic code and default components into a complete distributed computing program, which runs concurrently on a Hadoop cluster. Reason for the Emergence of MapReduceWhy do we need MapReduce? Massive data cannot be processed on a single machine due to hardware resource limitations. Once the single-machine version of the program is extended to run on a cluster, it will greatly increase the complexity and development difficulty of the program. With the introduction of the MapReduce framework, developers can focus most of their work on the development of business logic, while leaving the complexity of distributed computing to the framework to handle. Consider a word count requirement in a scenario with massive data: Single-machine version: limited memory, limited disk, limited computing power Distributed: file distributed storage (HDFS), computing logic needs to be divided into at least two stages (one stage is independently concurrent, one stage is converged), how to distribute computing programs, how to allocate computing tasks (slicing), how to start the two-stage program? How to coordinate? Monitoring during the entire program running process? Fault tolerance? Retry? It can be seen that when the program is extended from a single-machine version to a distributed version, a large amount of complex work will be introduced. Relationship between MapReduce and YarnYarn is a resource scheduling platform that is responsible for providing server computing resources for computing programs, which is equivalent to a distributed operating system platform. MapReduce and other computing programs are like application programs running on top of the operating system. Important concepts of YARN: Yarn does not know the running mechanism of the program submitted by the user; Yarn only provides scheduling of computing resources (when the user program applies for resources from Yarn, Yarn is responsible for allocating resources); The supervisor role in Yarn is called ResourceManager; The role that specifically provides computing resources in Yarn is called NodeManager; In this way, Yarn is completely decoupled from the running user program, which means that various types of distributed computing programs (MapReduce is just one of them), such as MapReduce, storm programs, spark programs, tez, etc., can run on Yarn; Therefore, computing frameworks such as Spark and Storm can be integrated to run on Yarn, as long as they have resource request mechanisms that comply with Yarn specifications in their respective frameworks; Yarn becomes a universal resource scheduling platform. From then on, various computing clusters that previously existed in enterprises can be integrated on a physical cluster to improve resource utilization and facilitate data sharing. MapReduce Working PrincipleStrictly speaking, MapReduce is not an algorithm, but a computing idea. It consists of two stages: map and reduce. MapReduce ProcessTo improve development efficiency, common functions in distributed programs can be encapsulated into frameworks, allowing developers to focus on business logic. MapReduce is such a general framework for distributed programs, and its overall structure is as follows (there are three types of instance processes during distributed operation): MRAppMaster: responsible for the process scheduling and status coordination of the entire program MapTask: responsible for the entire data processing process of the map phase ReduceTask: responsible for the entire data processing process of the reduce phase MapReduce Mechanism The process is described as follows: When an MR program starts, the MRAppMaster is started first. After the MRAppMaster starts, according to the description information of this job, it calculates the number of MapTask instances required and applies to the cluster to start the corresponding number of MapTask processes. After the MapTask process is started, data processing is performed according to the given data slice range. The main process is: Use the inputformat specified by the customer to obtain the RecordReader to read the data and form input KV pairs; Pass the input KV pairs to the customer-defined map() method for logical operation, and collect the KV pairs output by the map() method to the cache; Sort the KV pairs in the cache according to K partition and continuously overflow to the disk file. After the MRAppMaster monitors that all MapTask process tasks are completed, it will start the corresponding number of ReduceTask processes according to the customer-specified parameters, and inform the ReduceTask process of the data range (data partition) to be processed. After the ReduceTask process is started, according to the location of the data to be processed notified by the MRAppMaster, it obtains several MapTask output result files from several machines where the MapTask is running, and performs re-merging and sorting locally. Then, groups the KV with the same key into one group, calls the customer-defined reduce() method for logical operation, collects the result KV output by the operation, and then calls the customer-specified outputformat to output the result data to external storage. Let’s take an example. The above figure shows a word frequency counting task. Hadoop divides the input data into several slices and assigns each split to a map task for processing. After mapping, each word and its frequency in this task are obtained. Shuffle puts the same words together, sorts them, and divides them into several slices. According to these slices, reduce is performed. The result of the reduce task is counted and output to a file. In MapReduce, two roles are required to complete these processes: JobTracker and TaskTracker. JobTracker is used to schedule and manage other TaskTrackers. JobTracker can run on any computer in the cluster. TaskTracker is responsible for executing tasks and must run on DataNode. Here is a simple MapReduce implementation example: It is used to count the number of occurrences of each word in the input file. Import necessary packages: import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; Define the Mapper class: public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable> &#123; protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); // Split each line of text into words and send them to the Reducer String[] words = line.split(\"\\\\s+\"); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125; &#125; The Mapper class is responsible for splitting the input text data into words and outputting a key-value pair (word, 1) for each word. Define the Reducer class: public static class MyReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable> &#123; protected void reduce(Text key, Iterable&lt;IntWritable> values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // Accumulate the number of occurrences of the same word for (IntWritable value : values) &#123; sum += value.get(); &#125; // Output the word and its total number of occurrences context.write(key, new IntWritable(sum)); &#125; &#125; The Reducer class receives key-value pairs from the Mapper, accumulates the values of the same key, and then outputs the word and its total number of occurrences. Main function (main method): public static void main(String[] args) throws InterruptedException, IOException, ClassNotFoundException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(word.class); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(MyReduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // Set the input and output paths FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // Submit the job and wait for it to complete job.waitForCompletion(true); &#125; In the entire Hadoop architecture, the computing framework plays a crucial role, on the one hand, it can operate on the data in HDFS, on the other hand, it can be encapsulated to provide calls from upper-level components such as Hive and Pig. Let’s briefly introduce some of the more important components. HBase: originated from Google’s BigTable; it is a highly reliable, high-performance, column-oriented, and scalable distributed database. Hive: is a data warehouse tool that can map structured data files to a database table, and quickly implement simple MapReduce statistics through SQL-like statements, without the need to develop dedicated MapReduce applications, which is very suitable for statistical analysis of data warehouses. Pig: is a large-scale data analysis tool based on Hadoop. It provides a SQL-LIKE language called Pig Latin. The compiler of this language converts SQL-like data analysis requests into a series of optimized MapReduce operations. ZooKeeper: originated from Google’s Chubby; it is mainly used to solve some data management problems frequently encountered in distributed applications, simplifying the difficulty of coordinating and managing distributed application. Ambari: Hadoop management tool, which can monitor, deploy, and manage clusters quickly. Sqoop: used to transfer data between Hadoop and traditional databases. Mahout: an extensible machine learning and data mining library. Advantages and Applications of Hadoop Overall, Hadoop has the following advantages: High reliability: This is determined by its genes. Its genes come from Google. The best thing Google is good at is “garbage utilization.” When Google started, it was poor and couldn’t afford high-end servers, so it especially likes to deploy this kind of large system on ordinary computers. Although the hardware is unreliable, the system is very reliable. High scalability: Hadoop distributes data and completes computing tasks among available computer clusters, and these clusters can be easily expanded. In other words, it is easy to become larger. High efficiency: Hadoop can dynamically move data between nodes and ensure dynamic balance of each node, so the processing speed is very fast. High fault tolerance: Hadoop can automatically save multiple copies of data and automatically redistribute failed tasks. This is also considered high reliability. Low cost: Hadoop is open source and relies on community services, so the cost of use is relatively low. Based on these advantages, Hadoop is suitable for applications in large data storage and large data analysis, suitable for running on clusters of several thousand to tens of thousands of servers, and supports PB-level storage capacity. Hadoop’s applications are very extensive, including: search, log processing, recommendation systems, data analysis, video and image analysis, data storage, etc., can be deployed using it.","link":"/2023/11/08/en/hadoop-2/"},{"title":"Understanding Hadoop in One Article","text":"What is Hadoop?Hadoop is a distributed system infrastructure developed by the Apache Foundation. It is a software framework that combines a storage system and a computing framework. It mainly solves the problem of storing and computing massive data and is the cornerstone of big data technology. Hadoop processes data in a reliable, efficient, and scalable way. Users can develop distributed programs on Hadoop without understanding the underlying details of the distributed system. Users can easily develop and run applications that process massive data on Hadoop. What problems can Hadoop solve? Massive data storage HDFS has high fault tolerance and is designed to be deployed on low-cost hardware. It provides high throughput for accessing data and is suitable for applications with large data sets. It consists of n machines running DataNode and one machine running NameNode (another standby). Each DataNode manages a portion of the data, and NameNode is responsible for managing the information (metadata) of the entire HDFS cluster. Resource management, scheduling, and allocation Apache Hadoop YARN (Yet Another Resource Negotiator) is a new Hadoop resource manager. It is a general resource management system and scheduling platform that provides unified resource management and scheduling for upper-layer applications. Its introduction has brought huge benefits to the cluster in terms of utilization, unified resource management, and data sharing. The origin of Hadoop The core architecture of HadoopThe core of Hadoop is HDFS and MapReduce. HDFS provides storage for massive data, and MapReduce provides a computing framework for massive data. HDFS The entire HDFS has three important roles: NameNode, DataNode, and Client. Typical master-slave architecture, using TCP&#x2F;IP communication. NameNode: The master node of the distributed file system, responsible for managing the namespace of the file system, cluster configuration information, and storage block replication. The NameNode stores the metadata of the file system in memory, including file information, block information for each file, and information about each block in the DataNode. DataNode: The slave node of the distributed file system, which is the basic unit of file storage. It stores blocks in the local file system and saves the metadata of the blocks. It also periodically sends information about all existing blocks to the NameNode. Client: Splits files, accesses HDFS, interacts with the NameNode to obtain file location information, and interacts with the DataNode to read and write data. There is also the concept of a block: a block is the basic read and write unit in HDFS. Files in HDFS are stored as blocks, which are replicated to multiple DataNodes. The size of a block (usually 64MB) and the number of replicated blocks are determined by the client when the file is created. MapReduceMapReduce is a distributed computing model that divides large data sets (greater than 1TB) into many small data blocks, and then performs parallel processing on various nodes in the cluster, and finally aggregates the results. The MapReduce calculation process can be divided into two stages: the Map stage and the Reduce stage. Map stage: The input data is divided into several small data blocks, and then multiple Map tasks process them in parallel. Each Map task outputs the processing result as several key-value pairs. Reduce stage: The output results of the Map stage are grouped according to the keys in the key-value pairs, and then multiple Reduce tasks process them in parallel. Each Reduce task outputs the processing result as several key-value pairs. SummaryHadoop is a distributed system infrastructure that mainly solves the problem of storing and computing massive data. Its core is HDFS and MapReduce, where HDFS provides storage for massive data, and MapReduce provides a computing framework for massive data. In addition, Hadoop also has an important component-YARN, which is a general resource management system and scheduling platform that provides unified resource management and scheduling for upper-layer applications.","link":"/2023/11/08/en/hadoop-1/"},{"title":"Hadoop 2.0 Architecture - Distributed File System HDFS","text":"HDFS Design PrinciplesDesign GoalsStore very large files: “very large” here means several hundred M, G, or even TB. Adopt a stream-based data access method: HDFS is based on the assumption that the most effective data processing mode is to generate or copy a data set once and then do a lot of analysis work on it. Analysis work often reads most of the data in the data set, even if not all of it. Therefore, the time required to read the entire data set is more important than the delay in reading the first record. Run on commercial hardware: Hadoop does not require special expensive, reliable machines and can run on ordinary commercial machines (which can be purchased from multiple vendors). Commercial machines do not mean low-end machines. In a cluster (especially a large one), the node failure rate is relatively high. HDFS’s goal is to ensure that the cluster does not cause significant interruptions to users when nodes fail. Application Types Not Suitable for HDFSSome scenarios are not suitable for storing data in HDFS. Here are a few examples: Low-latency data accessApplications that require latency in the millisecond range are not suitable for HDFS. HDFS is designed for high-throughput data transmission, so latency may be sacrificed. HBase is more suitable for low-latency data access. A large number of small filesThe metadata of files (such as directory structure, node list of file blocks, and block-node mapping) is stored in the memory of the NameNode. The number of files in the entire file system is limited by the memory size of the NameNode. As a rule of thumb, a file&#x2F;directory&#x2F;file block generally occupies 150 bytes of metadata memory space. If there are one million files, each file occupies one file block, which requires about 300M of memory. Therefore, the number of files in the billions is difficult to support on existing commercial machines. Multiple reads and writes, requiring arbitrary file modificationHDFS writes data in an append-only manner. It does not support arbitrary offset modification of files. It does not support multiple writers. HDFS PositioningTo improve scalability, HDFS uses a master&#x2F;slave architecture to build a distributed storage cluster, which makes it easy to add or remove slaves to the cluster. HDFS is an important component of the Hadoop ecosystem. It is a distributed file system designed to store large amounts of data and provide high-throughput data access. HDFS is designed to store data on inexpensive hardware and provide high fault tolerance. It achieves this goal by distributing data to multiple nodes in the cluster. HDFS is positioned as a batch processing system suitable for offline processing of large-scale data. The main features of HDFS include: High fault tolerance: HDFS distributes data to multiple nodes, so even if a node fails, data can still be accessed through other nodes. High throughput: HDFS is designed to support batch processing of large-scale data, so it provides high-throughput data access. Suitable for large files: HDFS is suitable for storing large files because it divides files into multiple blocks for storage and distributes these blocks to multiple nodes. Stream data access: HDFS supports stream data access, which means it can efficiently process large amounts of data streams. HDFS ArchitectureHDFS uses a master&#x2F;slave architecture to build a distributed storage service, which improves the scalability of HDFS and simplifies the architecture design. HDFS stores files in blocks, optimizing storage granularity. The NameNode manages the storage space of all slave machines, while the DataNode is responsible for actual data storage and read&#x2F;write operations. BlocksThere is a concept of blocks in physical disks. The physical block of a disk is the smallest unit of disk operation for reading and writing, usually 512 bytes. The file system abstracts another layer of concepts on top of the physical block of the disk, and the file system block is an integer multiple of the physical disk block. Generally, it is several KB. The blocks in Hadoop are much larger than those in general single-machine file systems, with a default size of 128M. The file in HDFS is split into block-sized chunks for storage, and these chunks are scattered across multiple nodes. If the size of a file is smaller than the block size, the file will not occupy the entire block, only the actual size. For example, if a file is 1M in size, it will only occupy 1M of space in HDFS, not 128M. Why are HDFS blocks so large?To minimize the seek time and control the ratio of time spent locating and transmitting files. Assuming that the time required to locate a block is 10ms and the disk transmission speed is 100M&#x2F;s. If the proportion of time spent locating a block to the transmission time is controlled to 1%, the block size needs to be about 100M. However, if the block is set too large, in MapReduce tasks, if the number of Map or Reduce tasks is less than the number of cluster machines, the job efficiency will be very low. Benefits of block abstraction The splitting of blocks allows a single file size to be larger than the capacity of the entire disk, and the blocks that make up the file can be distributed across the entire cluster. In theory, a single file can occupy the disk of all machines in the cluster. Block abstraction also simplifies the storage system, without worrying about its permissions, owner, and other content (these contents are controlled at the file level). Blocks are the unit of replication in fault tolerance and high availability mechanisms. Namenode &amp; DatanodeThe entire HDFS cluster consists of a master-slave model of Namenode and Datanode. The Namenode stores the file system tree and metadata of all files and directories. The metadata is persisted in two forms: Namespace image Edit log However, the persistent data does not include the node list where the block is located and which nodes the file blocks are distributed to in the cluster. This information is reconstructed when the system is restarted (through the block information reported by the Datanode). In HDFS, the Namenode may become a single point of failure for the cluster. When the Namenode is unavailable, the entire file system is unavailable. HDFS provides two solutions to single point of failure: Backup persistent metadataWrite the file system metadata to multiple file systems at the same time, such as writing metadata to both the local file system and NFS at the same time. These backup operations are synchronous and atomic. Secondary NamenodeThe Secondary node periodically merges the namespace image and edit log of the main Namenode to avoid the edit log being too large, and merges them by creating a checkpoint. It maintains a merged namespace image replica that can be used to recover data when the Namenode completely crashes. The following figure shows the management interface of the Secondary Namenode: Internal Features of HDFSData Redundancy HDFS stores each file as a series of data blocks, with a default block size of 64MB (configurable). For fault tolerance, all data blocks of a file have replicas (the replication factor is configurable). HDFS files are written once and strictly limited to only one write user at any time. Replica Placement HDFS clusters usually run on multiple racks, and communication between machines on different racks requires switches. HDFS uses a rack-aware strategy to improve data reliability, availability, and network bandwidth utilization. Rack failures are much less common than node failures, and this strategy can prevent data loss when an entire rack fails, improve data reliability and availability, and ensure performance. Replica Selection HDFS tries to use the replica closest to the program to meet user requests, reducing total bandwidth consumption and read latency. The HDFS architecture supports data balancing strategies. Heartbeat Detection The NameNode periodically receives heartbeats and block reports from each DataNode in the cluster, indicating that the DataNode is working properly. The NameNode marks DataNodes that have not sent heartbeats recently as down and does not send them any new I&#x2F;O requests. The NameNode continuously checks these data blocks that need to be replicated and re-replicates them when necessary. Data Integrity Check For various reasons, the data block obtained from the DataNode may be corrupted. Classic HDFS ArchitectureThe NameNode is responsible for managing the metadata of the file system, while the DataNode is responsible for storing the actual data of the file blocks. This division of labor enables HDFS to efficiently store and manage large-scale data. Specifically, when a client needs to read or write a file, it sends a request to the NameNode. The NameNode returns the metadata information of the file and the location information of the file blocks. The client communicates with the DataNode based on this information to read or write the actual data of the file blocks. Therefore, the NameNode and DataNode play different roles in the HDFS architecture. What is the difference in function? HDFS is an abbreviation for Hadoop Distributed File System, an important component of the Hadoop ecosystem. The HDFS architecture includes one NameNode and multiple DataNodes. The NameNode is the master node of HDFS, responsible for managing the namespace of the file system, the metadata information of the file, and the location information of the file blocks. The DataNode is the slave node of HDFS, responsible for storing the actual data of the file blocks. Specifically, when a client needs to read or write a file, it sends a request to the NameNode. The NameNode returns the metadata information of the file and the location information of the file blocks. The client communicates with the DataNode based on this information to read or write the actual data of the file blocks. General TopologyThere is only one NameNode node, and the SecondaryNameNode or BackupNode node is used to obtain NameNode metadata information in real time and back up metadata. Commercial TopologyThere are two NameNode nodes, and ZooKeeper is used to implement hot standby between NameNode nodes. Command Line InterfaceHDFS provides various interaction methods, such as Java API, HTTP, and shell command line. Command line interaction is mainly operated through hadoop fs. For example: hadoop fs -copyFromLocal &#x2F;&#x2F; Copy files from local to HDFS hadoop fs mkdir &#x2F;&#x2F; Create a directory hadoop fs -ls &#x2F;&#x2F; List file list In Hadoop, the permissions of files and directories are similar to the POSIX model, including three permissions: read, write, and execute. Read permission (r): Used to read files or list the contents of a directoryWrite permission (w): For files, it is the write permission of the file. The write permission of the directory refers to the permission to create or delete files (directories) under the directory.Execute permission (x): Files do not have so-called execute permissions and are ignored. For directories, execute permission is used to access the contents of the directory. Each file or directory has three attributes: owner, group, and mode: Owner: Refers to the owner of the fileGroup: For permission groupsMode: Consists of the owner’s permissions, the permissions of the members of the file’s group, and the permissions of non-owners and non-group members. Data Flow (Read and Write Process)Read FileThe rough process of reading a file is as follows: The client passes a file Path to the FileSystem’s open method. DFS uses RPC to remotely obtain the datanode addresses of the first few blocks of the file. The NameNode determines which nodes to return based on the network topology structure (provided that the node has a block replica). If the client itself is a DataNode and there is a block replica on the node, it is read directly from the local node. The client uses the FSDataInputStream object returned by the open method to read data (call the read method). The DFSInputStream (FSDataInputStream implements this class) connects to the node that holds the first block and repeatedly calls the read method to read data. After the first block is read, find the best datanode for the next block and read the data. If necessary, DFSInputStream will contact the NameNode to obtain the node information of the next batch of Blocks (stored in memory, not persistent), and these addressing processes are invisible to the client. After the data is read, the client calls the close method to close the stream object. During the data reading process, if communication with the DataNode fails, the DFSInputStream object will try to read data from the next best node and remember the failed node, and subsequent block reads will not connect to the node. After reading a Block, DFSInputStram performs checksum verification. If the Block is damaged, it tries to read data from other nodes and reports the damaged block to the NameNode. Which DataNode does the client connect to get the data block is guided by the NameNode, which can support a large number of concurrent client requests, and the NameNode evenly distributes traffic to the entire cluster as much as possible. The location information of the Block is stored in the memory of the NameNode, so the corresponding location request is very efficient and will not become a bottleneck. Write File Step breakdown The client calls the create method of DistributedFileSystem. DistributedFileSystem remotely RPC calls the Namenode to create a new file in the namespace of the file system, which is not associated with any blocks at this time. During this process, the Namenode performs many verification tasks, such as whether there is a file with the same name, whether there are permissions, if the verification passes, it returns an FSDataOutputStream object. If the verification fails, an exception is thrown to the client. When the client writes data, DFSOutputStream is decomposed into packets (data packets) and written to a data queue, which is consumed by DataStreamer. DataStreamer is responsible for requesting the Namenode to allocate new blocks to store data nodes. These nodes store replicas of the same Block and form a pipeline. DataStreamer writes the packet to the first node of the pipeline. After the first node stores the packet, it forwards it to the next node, and the next node continues to pass it down. DFSOutputStream also maintains an ack queue, waiting for confirmation messages from datanodes. After all datanodes on the pipeline confirm, the packet is removed from the ack queue. After the data is written, the client closes the output stream. Flush all packets to the pipeline, and then wait for confirmation messages from datanodes. After all are confirmed, inform the Namenode that the file is complete. At this time, the Namenode already knows all the Block information of the file (because DataStreamer is requesting the Namenode to allocate blocks), and only needs to wait for the minimum replica number requirement to be reached, and then return a successful message to the client. How does the Namenode determine which DataNode the replica is on? The storage strategy of HDFS replicas is a trade-off between reliability, write bandwidth, and read bandwidth. The default strategy is as follows: The first replica is placed on the machine where the client is located. If the machine is outside the cluster, a random one is selected (but it will try to choose a capacity that is not too slow or too busy). The second replica is randomly placed on a rack different from the first replica. The third replica is placed on the same rack as the second replica, but on a different node, and a random selection is made from the nodes that meet the conditions. More replicas are randomly selected throughout the cluster, although too many replicas are avoided on the same rack as much as possible. After the location of the replica is determined, when establishing the write pipeline, the network topology structure is considered. The following is a possible storage strategy: This selection balances reliability, read and write performance well. Reliability: Blocks are distributed on two racks. Write bandwidth: The write pipeline process only needs to cross one switch. Read bandwidth: You can choose one of the two racks to read from. Internal Features of HDFSData Redundancy HDFS stores each file as a series of data blocks, with a default block size of 64MB (configurable). For fault tolerance, all data blocks of a file have replicas (the replication factor is configurable). HDFS files are written once and strictly limited to only one writing user at any time. Replica Placement HDFS clusters usually run on multiple racks, and communication between machines on different racks requires switches. HDFS uses a rack-aware strategy to improve data reliability, availability, and network bandwidth utilization. Rack failures are much less common than node failures, and this strategy can prevent data loss when an entire rack fails, improving data reliability and availability while ensuring performance. Replica Selection HDFS tries to use the replica closest to the program to satisfy user requests, reducing total bandwidth consumption and read latency. HDFS architecture supports data balancing strategies. Heartbeat Detection The NameNode periodically receives heartbeats and block reports from each DataNode in the cluster. Receiving a heartbeat indicates that the DataNode is working properly. The NameNode marks DataNodes that have not sent a heartbeat recently as dead and does not send them any new I&#x2F;O requests. The NameNode continuously checks for data blocks that need to be replicated and replicates them when necessary. Data Integrity Check Various reasons may cause the data block obtained from the DataNode to be corrupted. HDFS client software implements checksum verification of HDFS file content. If the checksum of the data block obtained by the DataNode is different from that in the hidden file corresponding to the data block, the client judges that the data block is corrupted and obtains a replica of the data block from another DataNode. Simple Consistency Model, Stream Data Access HDFS applications generally access files in a write-once, read-many mode. Once a file is created, written, and closed, it does not need to be changed again. This simplifies data consistency issues and makes high-throughput data access possible. Applications running on HDFS mainly focus on stream reading and batch processing, emphasizing high-throughput data access. Client Cache The request for the client to create a file does not immediately reach the NameNode. The HDFS client first caches the data to a local temporary file, and the write operation of the program is transparently redirected to this temporary file. When the accumulated data in this temporary file exceeds the size of a block (64MB), the client contacts the NameNode. If the NameNode crashes before the file is closed, the file will be lost. If client caching is not used, network speed and congestion will have a significant impact on output.","link":"/2023/11/09/en/hadoop-3/"},{"title":"How Does JIT (Just-In-Time) Compiler Work","text":"IntroductionBefore delving into JIT, it’s essential to have a basic understanding of the compilation process. In compiler theory, translating source code into machine instructions generally involves several crucial steps: JIT OverviewJIT stands for Just-In-Time compiler. Through JIT technology, it’s possible to accelerate the execution speed of Java programs. But how is this achieved? Java is an interpreted language (or semi-compiled, semi-interpreted language). Java compiles the source code into platform-independent Java bytecode files (.class) using the javac compiler. These bytecode files are then interpreted and executed by the Java Virtual Machine (JVM), ensuring platform independence. However, interpreting bytecode involves translating it into corresponding machine instructions, which inevitably slows down the execution speed compared to directly executing binary bytecode files. To enhance execution speed, JIT technology is introduced. When the JVM identifies a method or code block that is executed frequently, it recognizes it as a “Hot Spot Code.” JIT compiles these “Hot Spot Codes” into native machine-specific machine code, optimizes it, and caches the compiled machine code for future use. Hot Spot CompilationWhen the JVM executes code, it doesn’t immediately start compiling it. There are two main reasons for this: Firstly, if a piece of code is expected to be executed only once in the future, compiling it immediately is essentially a waste of resources. Compiling code into Java bytecode is much faster than both compiling and executing the code. However, if a piece of code, such as a method call or a loop, is executed multiple times, compiling it becomes worthwhile. The compiler has the ability to discern which methods are frequently called to ensure efficient compilation. Hot Spot VM employs JIT compilation technology to directly compile high-frequency bytecode into machine instructions (with the method as the compilation unit). These compiled machine instructions are executed directly when bytecode is JIT-compiled, providing a performance boost. The second reason involves optimization. As a method or loop is executed more frequently, the JVM gains a better understanding of the code structure. Therefore, the JVM can make corresponding optimizations during the compilation process. How JavaScript is Compiled - How JIT (Just-In-Time) Compiler WorksIn general, there are two ways to translate programs into machine-executable instructions: using a Compiler or an Interpreter. InterpreterAn interpreter translates and executes code line by line as it encounters it. Pros: Fast execution, no compilation delay.Cons: Same code might be translated multiple times, especially within loops. CompilerA compiler translates the code in advance and generates an executable program. Pros: No need for repeated compilation; can optimize code during compilation.Cons: Requires upfront compilation. JITWhen JavaScript first emerged, it was a typical interpreted language, resulting in slow execution speeds. Later, browsers introduced JIT compilers, significantly improving JavaScript’s execution speed. Principle: They added a new component to the JavaScript engine, known as a monitor (or profiler). This monitor observes the running code, noting how many times it runs and the types used. In essence, browsers added a monitor to the JavaScript engine to observe the running code, recording how many times each code segment is executed and the variable types used. Now, why does this approach speed up the execution? Let’s consider a function for illustration: function arraySum(arr) &#123; var sum = 0; for (var i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; &#125; &#125; 1st Step - Interpreter Initially, the code is executed using an interpreter. When a line of code is executed several times, it is marked as “Warm,” and if executed frequently, it is labeled as “Hot.” 2nd Step - Baseline Compiler Warm-labeled code is passed to the Baseline Compiler, which compiles and stores it. The compiled code is indexed based on line numbers and variable types (why variable types are important will be explained shortly). When the index matches, the corresponding compiled code is directly executed without recompilation, eliminating the need to recompile already compiled code. 3rd Step - Optimizing Compiler Hot-labeled code is sent to the Optimizing Compiler, where further optimizations are applied. How are these optimizations performed? This is the key: due to JavaScript’s dynamic typing, a single line of code can have multiple possible compilations, exposing the drawback of dynamic typing. For instance: sum is Int, arr is Array, i is Int; the + operation is simple addition, corresponding to one compilation result. sum is string, arr is Array, i is Int; the + operation is string concatenation, requiring the conversion of i to a string type.… As illustrated in the diagram below, such a simple line of code has 16 possible compilation results. The Baseline Compiler handles this complexity, and thus, the compiled code needs to be indexed using both line numbers and variable types. Different variable types lead to different compilation results. If the code is “Warm,” the JIT’s job ends here. Each subsequent execution involves type checks and uses the corresponding compiled result. However, when the code becomes “Hot,” more optimizations are performed. Here, optimization means JIT makes a specific assumption. For example, assuming sum and i are both Integers and arr is an Array, only one compilation result is needed. In practice, type checks are performed before execution. If the assumptions are incorrect, the execution is “deoptimized,” reverting to the interpreter or baseline compiler versions. This process is called “deoptimization.” As evident, the speed of execution relies on the accuracy of these assumptions. If the assumption success rate is high, the code executes faster. Conversely, low success rates lead to slower execution than without any optimization (due to the optimize =&gt; deoptimize process). ConclusionIn summary, this is what JIT does at runtime. It monitors running code, identifies hot code paths for optimization, making JavaScript run faster. This significantly improves the performance of most JavaScript applications. However, JavaScript performance remains unpredictable. To make it faster, JIT adds some overhead at runtime, including: Optimization and Deoptimization Memory for monitoring and recovering lost information Memory for storing baseline and optimized versions of functions There’s room for improvement, notably in eliminating overhead to make performance more predictable.","link":"/2023/11/04/en/just-in-time-compilers/"},{"title":"pack-tool-preview","text":"I tried migrating a real project from Vite to Rspack. The build time reduced from 125 seconds to 17 seconds, and the page refresh speed during development increased by 64%. However, the HMR (Hot Module Replacement) in Rspack is much slower compared to Vite. If you frequently trigger HMR during development and refresh the page less often, Vite still offers a better development experience. For complex projects where refreshing the page is more common, Rspack provides a better development experience. There are so many frontend build tools out there: RollDown, Rollup, Rspack, Vite… Just a sneak peek; stay tuned for the detailed comparison. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/pack-tool-preview/"},{"title":"Vue Proxy and Reflect","text":"IntroductionSince Vue.js 3’s reactive data is based on Proxy, it’s essential to understand Proxy and its associated concept, Reflect. What is Proxy? In simple terms, Proxy allows you to create a proxy object. It can proxy other objects, emphasizing that Proxy can only proxy objects and not non-object values like strings, booleans, etc. So, what does proxying mean? Proxying refers to the act of creating a basic semantic representation of an object. It allows us to intercept and redefine the basic operations on an object. Create object proxies with Proxy Built-in object Reflect When we talk about “basic semantics” in programming languages, we mean the fundamental operations for reading and modifying data. In JavaScript, these operations typically include reading property values and setting property values. For example, given an object obj, the following operations are considered basic semantics: Read property value: obj.foo (reads the value of property foo) Set property value: obj.foo = newValue (sets the value of property foo) In the above code, Proxy objects allow us to intercept (or redefine) these basic semantic operations. The Proxy constructor takes two parameters: the object being proxied and an object containing interceptors (also known as traps). In the interceptor object, we can define the get method to intercept property read operations and the set method to intercept property set operations. This way, we can execute custom logic when these operations occur. Understanding these basic semantic operations and how to use Proxy and Reflect to intercept and handle them is crucial for implementing reactive data in JavaScript. In reactive data, we can use Proxy and Reflect to track reads and modifications of object properties, enabling reactive updates of data. Basic Usage of ProxyWhen we talk about basic semantics, we refer to fundamental operations in JavaScript, such as reading object property values and setting object property values. Consider the following object obj: const obj = &#123; foo: 1 &#125;; Here, obj.foo is a basic semantic operation for reading property values, and obj.foo = newValue is a basic semantic operation for setting property values. Now, we can use Proxy to intercept these basic semantic operations. const handler = &#123; get(target, key) &#123; console.log(`Reading property $&#123;key&#125;`); return target[key]; &#125;, set(target, key, value) &#123; console.log(`Setting property $&#123;key&#125; to $&#123;value&#125;`); target[key] = value; &#125; &#125;; const proxyObj = new Proxy(obj, handler); proxyObj.foo; // Outputs: Reading property foo proxyObj.foo = 2; // Outputs: Setting property foo to 2 In the above code, we created a handler object that defines get and set methods to intercept property reads and sets. Then, we created a proxy object proxyObj using the Proxy constructor, which intercepts read and set operations on the obj object. When we access proxyObj.foo, the get method is triggered, outputting the corresponding message. When we set the value of proxyObj.foo, the set method is triggered, again outputting the corresponding message. This way, Proxy allows us to execute custom logic when basic semantic operations occur, without directly manipulating the original object. In practical applications, this capability can be used to implement reactive data, data validation, logging, and more. When intercepting object property reads with Proxy, special attention is required for accessor properties because accessor properties are defined using getter functions. The this keyword inside these getter functions changes based on the method of invocation. To solve this issue, we use Reflect.get(target, key, receiver) instead of target[key] when accessing property values. This ensures that the receiver parameter correctly points to the proxy object, not the original object. Consequently, within the getter function of accessor properties, the this keyword refers to the proxy object, establishing the correct reactive relationship. Here is the corrected code using Reflect.get: const handler = &#123; get(target, key, receiver) &#123; track(target, key); // Reactive data dependency tracking return Reflect.get(target, key, receiver); // Use Reflect.get to get property value &#125;, // Other interceptor methods... &#125;; const proxyObj = new Proxy(obj, handler); effect(() => &#123; console.log(proxyObj.bar); // Access the bar property inside the side effect function &#125;); proxyObj.foo++; // Triggers re-execution of the side effect function In this code, we use Reflect.get with the receiver parameter to ensure that this points to the proxy object within the get interceptor function. This establishes the correct reactive relationship, allowing proper dependency tracking when accessing object properties. Usage of Reflect in ReactivityIn interceptor functions, we aim to establish a connection between side-effect functions and reactive data. This ensures that when properties are accessed, the correct dependencies are tracked, enabling re-execution of side-effect functions when properties change. However, if we directly use target[key] to access property values, the this keyword inside the getter function of accessor properties points to the original object, not the proxy object. This prevents the establishment of the correct reactive relationship. To address this issue, we use Reflect.get(target, key, receiver) instead of target[key]. By doing so, the receiver parameter correctly points to the proxy object, allowing the this keyword inside the getter function to refer to the proxy object. This establishes the proper reactive relationship. Here is an example demonstrating the use of the receiver parameter and comparing it with the scenario where the receiver parameter is not used: 1. Using the receiver parameter:const data = &#123; foo: 1 &#125;; const proxy = new Proxy(data, &#123; get(target, key, receiver) &#123; // Use Reflect.get to ensure `this` points to the proxy object const result = Reflect.get(target, key, receiver); // Additional processing, such as triggering update operations, can be performed in practical applications console.log(`Accessed $&#123;key&#125; property with value $&#123;result&#125;`); return result; &#125; &#125;); console.log(proxy.foo); // Outputs: Accessed foo property with value 1 In this example, we use the receiver parameter passed to Reflect.get to ensure that this inside the get interceptor function refers to the proxy object proxy. When you access proxy.foo, the get interceptor function is triggered, and this points to the proxy object. 2. Not using the receiver parameter:const data = &#123; foo: 1 &#125;; const proxy = new Proxy(data, &#123; get(target, key) &#123; // Without using the receiver parameter, 'this' refers to the original object 'data' const result = target[key]; // In practical applications, additional processing might be required, such as triggering update operations console.log(`Accessed $&#123;key&#125; property with value $&#123;result&#125;`); return result; &#125; &#125;); console.log(proxy.foo); // Output: Accessed foo property with value 1 In this example, we did not use the receiver parameter. Since the receiver parameter was not passed, this inside the get interceptor function points to the original object data. Although the proxy object proxy is used, the this inside the get interceptor function does not refer to proxy but instead refers to the original object data. Therefore, in this scenario, the reactive relationship is not established. While the output of the two functions is the same, it’s evident that without using the receiver parameter, the reactive relationship is not established. This means that within the effect function, the object will not receive the correct reactivity. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-Proxy-and-Reflect/"},{"title":"Nested Effects and Effect Stack","text":"IntroductionEffect functions can be nested. But why is this design choice made? Nested Effectseffect(function effectFn1() &#123; effect(function effectFn2() &#123; /* ... */ &#125;) /* ... */ &#125;) In the code above, effectFn1 is nested within effectFn2. The execution of effectFn1 will trigger the execution of effectFn2. So, when do we encounter nested effects? Take Vue.js, for example; Vue.js’s rendering function is executed within an effect. When components are nested, for instance, when the Foo component renders the Bar component: // Bar component const Bar = &#123; render() &#123;/* ... */ &#125;, &#125; // Foo component renders the Bar component const Foo = &#123; render() &#123; return &lt;Bar /> &#125;// JSX syntax &#125; Nested effects occur in this scenario. It can be visualized as follows: effect(() => &#123; Foo.render() // Nested effect(() => &#123; Bar.render()&#125; )&#125; ) The effect function can be nested, meaning an effect function can contain another effect function. When an outer effect function depends on reactive data created inside an inner effect function, the inner effect function is automatically tracked, ensuring that the outer effect function is executed when the inner effect function changes. This nested structure of effect functions establishes a dependency chain, ensuring that when reactive data changes, all the effect functions dependent on it are triggered, thereby maintaining the responsiveness of the application. The “effect stack” in Vue 3’s internal implementation is crucial. Vue uses an effect stack to track the currently executing effect functions, similar to a function call stack. This stack manages the execution order and dependency relationships of effect functions. Now, let’s consider an example of a nested effect function without using a stack structure. However, it cannot achieve the desired nesting functionality. Let’s assume we have two reactive data, count1 and count2, where the value of count2 depends on the value of count1. We can use nested effect functions to establish this dependency relationship. // Original data const data = &#123; foo: true, bar: true &#125;; // Proxy object const obj = new Proxy(data, &#123; get(target, key) &#123; console.log(`Reading property: $&#123;key&#125;`); return target[key]; &#125; &#125;); // Global variables let temp1, temp2; // effectFn1 is nested within effectFn2 effect(function effectFn1() &#123; console.log('effectFn1 executed'); effect(function effectFn2() &#123; console.log('effectFn2 executed'); // Access obj.bar property within effectFn2 temp2 = obj.bar; &#125;); // Access obj.foo property within effectFn1 temp1 = obj.foo; &#125;); effectFn1 is the outer effect function, which depends on the value of obj.foo. It contains an innerEffect within it. The inner effect function, effectFn2, depends on the value of obj.bar. When we modify obj.foo, the outer effect function should be triggered and output the value of obj.foo. When we modify obj.bar, the inner effect function should be triggered and output the value of obj.bar. We use the global variable activeEffect to store the effect functions registered through the effect function. This means that at any given time, activeEffect stores only one effect function. // Use a global variable to store the currently active effect function let activeEffect; function effect(fn) &#123; // Define the effect function const effectFn = () => &#123; // Call the cleanup function; specific implementation needs to be provided based on requirements cleanup(effectFn); // Assign the effect function to activeEffect activeEffect = effectFn; // Execute the effect function fn(); // Store the dependencies of the current effect function in effectFn.deps (Needs to be implemented based on the actual logic) effectFn.deps = []; // Set dependencies collection based on the actual logic &#125;; // Execute the effect function effectFn(); &#125; However, by only using a single variable for storage without employing a stack structure, when nested effects occur, the execution of the inner effect function will overwrite the value of activeEffect. It will never be restored to its original value. If there is a reactive data performing dependency collection, even if it is read within the outer effect function, the collected effect functions will all be from the inner effect function. In other words, when I read obj.foo, activeEffect still refers to the value of innerEffect, and only the innerEffect is triggered. To solve this issue, we need an effect function stack called effectStack. When an effect function is executed, the current effect function is pushed onto the stack. After the execution of the effect function is completed, it is popped from the stack. The activeEffect is always set to the top effect function on the stack. This ensures that a reactive data will only collect the effect function that directly reads its value, preventing mutual interference: // Use a global variable to store the currently active effect function let activeEffect; // Effect stack const effectStack = []; function effect(fn) &#123; const effectFn = () => &#123; cleanup(effectFn); // Call the cleanup function; specific implementation needs to be provided based on requirements activeEffect = effectFn; // Push the current effect function onto the stack effectStack.push(effectFn); fn(); // After the execution of the current effect function is completed, pop it from the stack, and restore activeEffect to its previous value effectStack.pop(); activeEffect = effectStack[effectStack.length - 1]; &#125;; // Initialize the dependencies collection of the effect function effectFn.deps = []; // Execute the effect function effectFn(); &#125; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.","link":"/2023/10/31/en/vue-effect/"},{"title":"vue-expired-side-effects","text":"IntroductionWhen we talk about race conditions, it typically refers to a concurrency problem in multi-process or multi-threaded programming. However, in frontend development, we might not directly encounter multi-threaded programming frequently, but we often face similar situations related to race conditions. A common example is in asynchronous programming, especially when dealing with asynchronous events, callback functions, or Promises. For instance, consider the following asynchronous code: let data; function fetchData() &#123; setTimeout(() => &#123; data = 'Fetched data'; &#125;, 1000); &#125; fetchData(); console.log(data); // Outputs undefined In this example, the fetchData function is asynchronous, and it assigns the data to the data variable after 1 second. However, due to JavaScript’s single-threaded nature, the fetchData function waits in the main thread’s event queue for 1 second. Within this 1 second, the console.log(data) statement executes immediately, and at that point, the value of data is undefined because the fetchData function has not completed yet. In asynchronous programming, due to the non-blocking nature of the code, similar race condition issues can arise. When dealing with asynchronous operations, it’s crucial to ensure data consistency and correctness, avoiding accessing or modifying related data before the asynchronous operation is completed. Race Conditions and ReactivitySo, how are race conditions related to reactivity? Consider the following example: let finalData; watch(obj, async () => &#123; // Send and wait for a network request const res = await fetch('/path/to/request'); // Assign the request result to data finalData = res; &#125;); In this code snippet, we use the watch function to observe changes to the obj object. Every time the obj object changes, a network request, such as an API call, is sent. After the data request is successful, the result is assigned to the finalData variable. At first glance, this code might seem fine. However, upon closer inspection, you’ll realize that this code can lead to race condition problems. Let’s assume we modify a field of the obj object for the first time, triggering the callback function and sending the first request A. As time passes, before the result of request A returns, we modify a field of the obj object again, triggering the second request B. Now, both request A and request B are in progress. Which request will return its result first? We don’t know. If request B completes before request A, the finalData variable will store the result of request B, making request A’s result outdated. However, because request B was sent later, we consider its data as the “latest.” Request A is deemed “expired,” and its result should be invalidated. By ensuring that request B’s result is considered the latest, we can prevent errors caused by race conditions. Essentially, what we need is a way to expire side effects. To illustrate this concept further, let’s replicate the scenario using the watch function in Vue.js to see how Vue.js helps developers address this problem. Later, we’ll attempt to implement this functionality ourselves. watch(obj, async (newValue, oldValue, onInvalidate) => &#123; // Define a flag to indicate whether the current side effect has expired, initially set to false (not expired) let expired = false; // Call the onInvalidate() function to register an expiration callback onInvalidate(() => &#123; // When expired, set the expired flag to true expired = true; &#125;); // Send a network request const res = await fetch('/path/to/request'); // Perform subsequent operations only if the side effect has not expired if (!expired) &#123; finalData = res; // Subsequent operations... &#125; &#125;); As shown in the code above, before sending the request, we define an expired flag variable to indicate whether the current side effect has expired. We then call the onInvalidate function to register an expiration callback. When the side effect expires, the expired flag is set to true. Finally, we use the request result only if the side effect has not expired, effectively avoiding the issue described earlier. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-expired-side-effects/"},{"title":"The Role and Implementation of Vue.js Reactive System","text":"IntroductionThe concept of reactivity is not difficult to understand. It refers to triggering certain events when JavaScript operates on an object or a value. This is achieved by implementing a reactive system, where operations elicit specific responses. Reactive Data and Side Effect FunctionsSide effect functions refer to functions that produce side effects. Consider the following code snippet: function effect() &#123; document.body.innerText = 'hello vue3'; &#125; When the effect function is executed, it sets the text content of the body. Any function other than effect can read or modify the body’s text content. In other words, the execution of the effect function directly or indirectly affects the execution of other functions, indicating that the effect function has side effects. Side effect functions are common and can impact various aspects, such as the effectiveness of “tree shaking” in webpack, a topic we discussed earlier. I won’t delve into this here. Side effects can easily occur; for example, if a function modifies a global variable, it creates a side effect, as shown in the following code: // Global variable let val = 1; function effect() &#123; val = 2; // Modifying a global variable creates a side effect &#125; Understanding side effect functions, let’s now discuss reactive data. Suppose a property of an object is read within a side effect function: const obj = &#123; text: 'hello world' &#125;; function effect() &#123; // The execution of the effect function reads obj.text document.body.innerText = obj.text; &#125; The side effect function effect sets the innerText property of the body element to obj.text. When the value of obj.text changes, we want the side effect function effect to re-execute. Our approach is as follows: we want to store the effect function in a bucket when reading the obj.text value, and when setting the obj.text value, we want to retrieve the effect function from the bucket and execute it. Basic Implementation of Reactive DataHow can we make obj reactive data? By observing, we find that: When the side effect function effect is executed, it triggers the read operation of the obj.text property. When the obj.text value is modified, it triggers the write operation of the obj.text property. We need to intercept the read and write operations of an object property. Before ES2015, this could only be done using the Object.defineProperty function, which was also the approach used in Vue.js 2. In ES2015 and later, we can use the Proxy object to achieve this, and this is the approach adopted in Vue.js 3. function createReactiveObject(target, proxyMap, baseHandlers) &#123; // The core is the proxy // The goal is to detect user get or set actions const existingProxy = proxyMap.get(target); if (existingProxy) &#123; return existingProxy; &#125; const proxy = new Proxy(target, baseHandlers); // Store the created proxy, proxyMap.set(target, proxy); return proxy; &#125; Here, we create a reactive object based on the Proxy object. We have a proxyMap, which is a container for storing various types of proxies. We define get and set intercept functions to intercept read and write operations. Designing a Comprehensive Reactive SystemFrom the above examples, it’s clear that the workflow of a reactive system is as follows: When a read operation occurs, collect the side effect functions into a “bucket”. When a write operation occurs, retrieve the side effect functions from the “bucket” and execute them. Next, I’ll explain the principles through a simple implementation of a reactive system. We know that the Proxy object can accept an object with getters and setters for handling get or set operations. Therefore, we can create a baseHandlers to manage getters and setters. // baseHandlers function createGetter(isReadonly = false, shallow = false) &#123; return function get(target, key, receiver) &#123; const res = Reflect.get(target, key, receiver); if (!isReadonly) &#123; // Collect dependencies when triggering get track(target, \"get\", key); &#125; return res; &#125;; &#125; function createSetter() &#123; return function set(target, key, value, receiver) &#123; const result = Reflect.set(target, key, value, receiver); // Trigger dependencies when setting values trigger(target, \"set\", key); return result; &#125;; &#125; We also need to establish a clear link between side effect functions and the target fields being operated upon. For instance, when reading a property, it doesn’t matter which property is being read; all side effect functions should be collected into the “bucket.” Similarly, when setting a property, regardless of which property is being set, the side effect functions from the “bucket” should be retrieved and executed. There is no explicit connection between side effect functions and the operated fields. The solution is simple: we need to establish a connection between side effect functions and the operated fields. This requires redesigning the data structure of the “bucket” and cannot be as simple as using a Set type for the “bucket.” We use WeakMap to implement the bucket for storing effects as discussed earlier. If you are not familiar with the characteristics of the WeakMap class, you can learn more about it here. // Map to store different types of proxies export const reactiveMap = new WeakMap(); export const readonlyMap = new WeakMap(); export const shallowReadonlyMap = new WeakMap(); Once we have defined the buckets as above, we can proceed to implement the core part of the reactive system, which is the proxy: function createReactiveObject(target, proxyMap, baseHandlers) &#123; // The core is the proxy // The goal is to detect user get or set actions const existingProxy = proxyMap.get(target); if (existingProxy) &#123; return existingProxy; &#125; const proxy = new Proxy(target, baseHandlers); // Store the created proxy proxyMap.set(target, proxy); return proxy; &#125; For the previous track and trigger functions: export function track(target, type, key) &#123; if (!isTracking()) &#123; return; &#125; console.log(`Trigger track -> target: $&#123;target&#125; type:$&#123;type&#125; key:$&#123;key&#125;`); // 1. Find the corresponding dep based on the target // Initialize depsMap if it's the first time let depsMap = targetMap.get(target); if (!depsMap) &#123; // Initialize depsMap logic depsMap = new Map(); targetMap.set(target, depsMap); &#125; let dep = depsMap.get(key); if (!dep) &#123; dep = createDep(); depsMap.set(key, dep); &#125; trackEffects(dep); &#125; export function trigger(target, type, key) &#123; // 1. Collect all deps and put them into deps array, // which will be processed later let deps: Array&lt;any> = []; const depsMap = targetMap.get(target); if (!depsMap) return; // Currently only GET type is implemented // for get type, just retrieve it const dep = depsMap.get(key); // Collect into deps array deps.push(dep); const effects: Array&lt;any> = []; deps.forEach((dep) => &#123; // Destructure dep to get the stored effects effects.push(...dep); &#125;); // The purpose here is to have only one dep that contains all effects // Currently, it should be reused for the triggerEffects function triggerEffects(createDep(effects)); &#125; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/30/en/vue-reactive-1/"},{"title":"Vue Shallow Reactivity vs Deep Reactivity","text":"IntroductionExplore the differences between reactive and shallowReactive, delving into the concepts of deep reactivity and shallow reactivity in Vue.js. Shallow Reactivity vs Deep Reactivityconst obj = reactive(&#123; foo: &#123; bar: 1 &#125; &#125;) effect(() =>&#123; console.log(obj.foo.bar) &#125;) // Modifying obj.foo.bar value does not trigger reactivity obj.foo.bar = 2 Initially, an object obj is created with a property foo containing another object &#123; bar: 1 &#125;. When accessing obj.foo.bar inside an effect function, it is noticed that modifying obj.foo.bar does not trigger the effect function again. Why does this happen? Let’s take a look at the current implementation: function reactive(obj) &#123; return new Proxy(obj ,&#123; get(target, key, receiver) &#123; if (key === 'raw') return target; track(target, key); // When reading the property value, return it directly return Reflect.get(target, key, receiver) &#125; // Other trapping functions are omitted &#125;) &#125; In the given code, when accessing obj.foo.bar, it first reads the value of obj.foo. Here, Reflect.get is used to directly return the result of obj.foo. Since the result obtained through Reflect.get is a plain object, namely &#123; bar: 1 &#125;, it is not a reactive object. Therefore, when accessing obj.foo.bar inside the effect function, no reactivity is established. To address this, the result returned by Reflect.get needs to be wrapped: function reactive(obj) &#123; return new Proxy(obj, &#123; get(target, key, receiver) &#123; const result = Reflect.get(target, key, receiver); // If the result is an object, make it reactive if (typeof result === 'object') &#123; return reactive(result); &#125; return result; &#125;, // Other traps... &#125;); &#125; In this code snippet, the reactive function is defined. It takes an object as a parameter and returns a proxy of that object. The proxy uses a get trap function that triggers when accessing a property of the object. In the get trap, Reflect.get is used to retrieve the property value. If the result is an object, it is made reactive by calling the reactive function recursively. This ensures that nested objects also possess reactive properties, allowing modifications to trigger the reactivity system. Shallow ReactivityHowever, there are scenarios where deep reactivity is not desired, leading to the concept of shallowReactive or shallow reactivity. Shallow reactivity means that only the top-level properties of an object are reactive. For example: Suppose we have an object with a nested object as its property: let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; If we apply deep reactivity to obj: let reactiveObj = reactive(obj); Any modifications to obj or innerObj properties will trigger the reactivity system: reactiveObj.innerObj.key = 'new value'; // Triggers reactivity However, if we want only the top-level properties of obj to be reactive, meaning modifications to obj trigger reactivity but modifications to innerObj do not, we use the shallowReactive function: let shallowReactiveObj = shallowReactive(obj); With shallowReactive, only modifications to obj will trigger reactivity: shallowReactiveObj.innerObj = &#123;&#125;; // Triggers reactivity shallowReactiveObj.innerObj.key = 'new value'; // Does not trigger reactivity Vue.js and reactive vs shallowReactiveIn Vue.js, both reactive and shallowReactive functions are used to create reactive objects. Let’s explore their differences. The reactive function creates deeply reactive objects. This means that both the object itself and all its nested objects become reactive. Any modifications to the object or its nested objects’ properties will trigger the reactivity system. On the other hand, the shallowReactive function creates shallowly reactive objects. This means that only the top-level properties of the object are reactive. If the object contains nested objects, modifications to those nested objects’ properties will not trigger the reactivity system. let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; let reactiveObj &#x3D; Vue.reactive(obj);reactiveObj.innerObj.key &#x3D; ‘new value’; &#x2F;&#x2F; Triggers reactivity let shallowReactiveObj &#x3D; Vue.shallowReactive(obj);shallowReactiveObj.innerObj.key &#x3D; ‘new value’; &#x2F;&#x2F; Does not trigger reactivity Readonly and Shallow ReadonlyAfter discussing reactivity and shallow reactivity, let’s talk about readonly and shallow readonly: Vue.js provides readonly and shallowReadonly functions to create readonly reactive objects. The readonly function creates deeply readonly reactive objects. This means that both the object itself and all its nested objects are readonly. Any attempts to modify the object or its nested objects’ properties will fail. The shallowReadonly function creates shallow readonly reactive objects. This means that only the top-level properties of the object are readonly. If the object contains nested objects, properties of these nested objects can be modified. let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; let readonlyObj = Vue.readonly(obj); readonlyObj.innerObj.key = 'new value'; // This will fail because the object is readonly let shallowReadonlyObj = Vue.shallowReadonly(obj); shallowReadonlyObj.innerObj.key = 'new value'; // This will succeed because only top-level properties are readonly Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-reactive-shallowReactive/"},{"title":"Vue Render Mounting and Updating","text":"IntroductionVue.js templates are powerful and can meet most of our application needs. However, in certain scenarios, such as creating dynamic components based on input or slot values, the render function can be a more flexible solution. Developers familiar with the React ecosystem might already be acquainted with render functions, commonly used in JSX to construct React components. While Vue render functions can also be written in JSX, this discussion focuses on using plain JavaScript. This approach simplifies understanding the fundamental concepts of the Vue component system. Every Vue component includes a render function. Most of the time, this function is created by the Vue compiler. When a template is specified for a component, the Vue compiler processes the template’s content, ultimately generating a render function. This render function produces a virtual DOM node, which Vue renders in the browser DOM. This brings us to the concept of the virtual DOM. But what exactly is the virtual DOM? The virtual Document Object Model (or “DOM”) enables Vue to render components in its memory before updating the browser. This approach enhances speed and avoids the high cost associated with re-rendering the DOM. Since each DOM node object contains numerous properties and methods, pre-rendering them in memory using the virtual DOM eliminates the overhead of creating DOM nodes directly in the browser. When Vue updates the browser DOM, it compares the updated virtual DOM with the previous virtual DOM. Only the modified parts of the virtual DOM are used to update the actual DOM, reducing the number of element changes and enhancing performance. The render function returns virtual DOM nodes, often referred to as VNodes in the Vue ecosystem. These objects enable Vue to write these nodes into the browser DOM. They contain all the necessary information Vue needs. Mounting Child Nodes and Element AttributesWhen vnode.children is a string, it sets the element’s text content. An element can have multiple child elements besides text nodes. To describe an element’s child nodes, vnode.children needs to be defined as an array: const vnode = &#123; type: 'div', children: [ &#123; type: 'p', children: 'hello' &#125; ] &#125;; In the above code, we describe “a div tag with a child node, which is a p tag.” As seen, vnode.children is an array, and each element of the array is an independent virtual node object. This creates a tree-like structure, or a virtual DOM tree. To render child nodes, we need to modify the mountElement function, as shown in the following code: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); if (typeof vnode.children === 'string') &#123; setElementText(el, vnode.children); &#125; else if (Array.isArray(vnode.children)) &#123; // If `children` is an array, iterate through each child node and call the `patch` function to mount them vnode.children.forEach(child => &#123; patch(null, child, el); &#125;); &#125; insert(el, container); &#125; In this code, we have added a new conditional branch. We use the Array.isArray function to check if vnode.children is an array. If it is, we loop through each child node and call the patch function to mount the virtual nodes in the array. During mounting of child nodes, we need to pay attention to two points: The first argument passed to the patch function is null. Since this is the mounting phase and there is no old vnode, we only need to pass null. This way, when the patch function is executed, it will recursively call the mountElement function to mount the child nodes. The third argument passed to the patch function is the mounting point. Since the child elements being mounted are child nodes of the div element, the div element created earlier serves as the mounting point to ensure that these child nodes are mounted in the correct position. After mounting the child nodes, let’s look at how to describe the attributes of an element using vnode and how to render these attributes. We know that HTML elements have various attributes, some of which are common, such as id and class, while others are specific to certain elements, such as the action attribute for form elements. In this discussion, we will focus on the basic attribute handling. To describe the attributes of an element, we need to define a new field in the virtual DOM called vnode.props, as shown in the following code: const vnode = &#123; type: 'div', props: &#123; id: 'foo' &#125;, children: [ &#123; type: 'p', children: 'hello' &#125; ] &#125;; vnode.props is an object where the keys represent the attribute names of the element, and the values represent the corresponding attribute values. This way, we can iterate through the props object and render these attributes onto the element, as shown in the following code: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); // Skip children handling for now // Only handle `vnode.props` if it exists if (vnode.props) &#123; // Iterate through `vnode.props` object for (const key in vnode.props) &#123; // Use `setAttribute```javascript // Use `setAttribute` to set attributes on the element el.setAttribute(key, vnode.props[key]); &#125; &#125; insert(el, container); &#125; In this code snippet, we first check if vnode.props exists. If it does, we iterate through the vnode.props object and use the setAttribute function to set attributes on the element. This approach ensures that the attributes are rendered onto the element during the mounting process. When dealing with attributes, it’s essential to understand the distinction between HTML Attributes and DOM Properties. HTML Attributes are the attributes defined in the HTML tags, such as id=&quot;my-input&quot;, type=&quot;text&quot;, and value=&quot;foo&quot;. When the browser parses this HTML code, it creates a corresponding DOM element object, which we can access using JavaScript code: const el = document.querySelector('#my-input'); Now, let’s talk about DOM Properties. Many HTML Attributes have corresponding DOM Properties on the DOM element object, such as id=&quot;my-input&quot; corresponding to el.id, type=&quot;text&quot; corresponding to el.type, and value=&quot;foo&quot; corresponding to el.value. However, the names of DOM Properties don’t always exactly match HTML Attributes: &lt;div class=\"foo\">&lt;/div> In this case, class=&quot;foo&quot; corresponds to the DOM Property el.className. Additionally, not all HTML Attributes have corresponding DOM Properties: &lt;div aria-valuenow=\"75\">&lt;/div> Attributes with the aria-* prefix do not have corresponding DOM Properties. Similarly, not all DOM Properties have corresponding HTML Attributes. For example, you can use el.textContent to set the element’s text content, but there is no equivalent HTML Attribute for this operation. The values of HTML Attributes and DOM Properties are related. For example, consider the following HTML snippet: &lt;div id=\"foo\">&lt;/div> This snippet defines a div element with an id attribute. The corresponding DOM Property is el.id, and its value is the string &#39;foo&#39;. We consider this situation as a direct mapping, where the HTML Attribute and DOM Property have the same name (id in this case). However, not all HTML Attributes and DOM Properties have a direct mapping relationship. For example: &lt;input value=\"foo\" /> Here, the input element has a value attribute set to &#39;foo&#39;. If the user does not modify the input field, accessing el.value would return the string &#39;foo&#39;. If the user changes the input value to &#39;bar&#39;, accessing el.value would return &#39;bar&#39;. But if you run the following code: console.log(el.getAttribute('value')); // Still 'foo' console.log(el.value); // 'bar' You’ll notice that modifying the input value does not affect the return value of el.getAttribute(&#39;value&#39;). This behavior indicates the meaning behind HTML Attributes. Essentially, HTML Attributes are used to set the initial value of corresponding DOM Properties. Once the value changes, the DOM Properties always store the current value, while getAttribute retrieves the initial value. However, you can still access the initial value using el.defaultValue, as shown below: el.getAttribute('value'); // Still 'foo' el.value; // 'bar' el.defaultValue; // 'foo' This example illustrates that an HTML Attribute can be associated with multiple DOM Properties. In this case, value=&quot;foo&quot; is related to both el.value and el.defaultValue. Although HTML Attributes are considered as setting the initial values of corresponding DOM Properties, some values are restricted. It’s as if the browser internally checks for default value validity. If the value provided through HTML Attributes is invalid, the browser uses a built-in valid value for the corresponding DOM Properties. For example: &lt;input type=\"foo\" /> We know that specifying the string &#39;foo&#39; for the type attribute of the &lt;input/&gt; tag is invalid. Therefore, the browser corrects this invalid value. When you try to read el.type, you actually get the corrected value, which is &#39;text&#39;, not &#39;foo&#39;: console.log(el.type); // 'text' From the analysis above, we can see that the relationship between HTML Attributes and DOM Properties is complex. However, the core principle to remember is this: HTML Attributes are used to set the initial values of corresponding DOM Properties. How to Properly Set Element AttributesIn the previous discussion, we explored how HTML Attributes and DOM Properties are handled in Vue.js single-file components’ templates. In regular HTML files, the browser automatically parses HTML Attributes and sets the corresponding DOM Properties. However, in Vue.js templates, the framework needs to handle the setting of these attributes manually. Firstly, let’s consider a disabled button as an example in plain HTML: &lt;button disabled>Button&lt;/button> The browser automatically disables this button and sets its corresponding DOM Property el.disabled to true. However, if the same code appears in a Vue.js template, the behavior would be different. In Vue.js templates, the HTML template is compiled into virtual nodes (vnode). The value of props.disabled in the virtual node is an empty string. If you use the setAttribute function directly to set the attribute, unexpected behavior occurs, and the button becomes disabled. For example, in the following template: &lt;button disabled=\"false\">Button&lt;/button> The corresponding virtual node is: const button = &#123; type: 'button', props: &#123; disabled: false &#125; &#125;; If you use the setAttribute function to set the attribute value to an empty string, it is equivalent to: el.setAttribute('disabled', ''); However, the el.disabled property is of boolean type and does not care about the specific value of HTML Attributes; it only checks for the existence of the disabled attribute. So, the button becomes disabled. Therefore, renderers should not always use the setAttribute function to set attributes from the vnode.props object. To solve this issue, a better approach is to prioritize setting the element’s DOM Properties. However, if the value is an empty string, manually correct it to true. Here is an implementation example: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); if (vnode.props) &#123; for (const key in vnode.props) &#123; if (key in el) &#123; const type = typeof el[key]; const value = vnode.props[key]; if (type === 'boolean' &amp;&amp; value === '') &#123; el[key] = true; &#125; else &#123; el[key] = value; &#125; &#125; else &#123; el.setAttribute(key, vnode.props[key]); &#125; &#125; &#125; insert(el, container); &#125; In this code, we first check if the property exists on the DOM element. If it does, we determine the type of the property and the value from vnode.props. If the property is of boolean type and the value is an empty string, we correct it to true. If the property does notexist on the DOM element, we use the setAttribute function to set the attribute. However, there are still issues with this implementation. Some DOM Properties are read-only, such as el.form. To address this problem, we can create a helper function, shouldSetAsProps, to determine whether an attribute should be set as DOM Properties. If the property is read-only or requires special handling, we should use the setAttribute function to set the attribute. Finally, to make the attribute setting operation platform-agnostic, we can extract the attribute-related operations into the renderer options. Here is the updated code: const renderer = createRenderer(&#123; createElement(tag) &#123; return document.createElement(tag); &#125;, setElementText(el, text) &#123; el.textContent = text; &#125;, insert(el, parent, anchor = null) &#123; parent.insertBefore(el, anchor); &#125;, patchProps(el, key, prevValue, nextValue) &#123; if (shouldSetAsProps(el, key, nextValue)) &#123; const type = typeof el[key]; if (type === 'boolean' &amp;&amp; nextValue === '') &#123; el[key] = true; &#125; else &#123; el[key] = nextValue; &#125; &#125; else &#123; el.setAttribute(key, nextValue); &#125; &#125; &#125;); In the mountElement function, we only need to call the patchProps function and pass the appropriate parameters. This way, we’ve extracted the attribute-related rendering logic from the core renderer, making it more maintainable and flexible. Please note that the shouldSetAsProps function should be implemented according to your specific requirements and the DOM properties you want to handle differently.","link":"/2023/11/05/en/vue-renderer-2/"},{"title":"vue-renderer-1","text":"PrefaceIn Vue.js, many functionalities rely on renderers to be implemented, such as Transition components, Teleport components, Suspense components, as well as template refs and custom directives. Moreover, the renderer is the core of the framework’s performance, as its implementation directly affects the framework’s performance. Vue.js 3’s renderer not only includes the traditional Diff algorithm but also introduces a fast path update method, leveraging the information provided by the compiler, significantly improving update performance. In Vue.js, the renderer is responsible for executing rendering tasks. On the browser platform, it renders the virtual DOM into real DOM elements. The renderer can render not only real DOM elements but also plays a key role in the framework’s cross-platform capabilities. When designing a renderer, its customizable capabilities need to be considered. Basic Concepts and Meanings of RendererBefore implementing a basic renderer, we need to understand a few fundamental concepts: In Vue.js, a renderer is a component responsible for rendering virtual DOM (or virtual nodes) into real elements on a specific platform. On the browser platform, the renderer renders virtual DOM into real DOM elements. RendererThe renderer is responsible for rendering virtual DOM (or virtual nodes) into real elements on a specific platform. On the browser platform, the renderer renders virtual DOM into real DOM elements. Virtual DOM (vnode)The virtual DOM (also known as virtual nodes, abbreviated as vnode) is a tree-like structure, similar to real DOM, consisting of various nodes. The renderer’s task is to render the virtual DOM into real DOM elements. MountingMounting refers to rendering the virtual DOM into real DOM elements and adding them to a specified mounting point. In Vue.js, the mounted lifecycle hook of a component is triggered when the mounting is completed, and it can access the real DOM element at this point. ContainerThe container specifies the mounting position’s DOM element. The renderer renders the virtual DOM into real DOM elements and adds them to the specified container. In the renderer’s render function, a container parameter is usually passed in, indicating which DOM element the virtual DOM is mounted to. Creation and Usage of RendererThe renderer is usually created using the createRenderer function, which returns an object containing rendering and hydration functions. The hydration function is used in server-side rendering (SSR) to hydrate virtual DOM into existing real DOM elements. Here’s an example of creating and using a renderer: function createRenderer() &#123; function render(vnode, container) &#123; // Render logic &#125; function hydrate(vnode, container) &#123; // Hydration logic &#125; return &#123; render, hydrate &#125;; &#125; const &#123; render, hydrate &#125; = createRenderer(); // Initial rendering render(vnode, document.querySelector('#app')); // Server-side rendering hydrate(vnode, document.querySelector('#app')); In the above code, the createRenderer function creates a renderer object that contains the render and hydrate functions. The render function is used to render the virtual DOM into real DOM elements, while the hydrate function is used to hydrate the virtual DOM into existing real DOM elements. Now that we have a basic understanding of the renderer, let’s dive deeper step by step. The implementation of the renderer can be represented by the following function, where domString is the HTML string to be rendered, and container is the DOM element to mount to: function renderer(domString, container) &#123; container.innerHTML = domString; &#125; Example usage of the renderer: renderer('&lt;h1>Hello&lt;/h1>', document.getElementById('app')); In the above code, &lt;h1&gt;Hello&lt;/h1&gt; is inserted into the DOM element with the id app. The renderer can not only render static strings but also dynamic HTML content: let count = 1; renderer(`&lt;h1>$&#123;count&#125;&lt;/h1>`, document.getElementById('app')); If count is a reactive data, the reactivity system can automate the entire rendering process. First, define a reactive data count and then call the renderer function inside the side effect function to render: const count = ref(1); effect(() => &#123; renderer(`&lt;h1>$&#123;count.value&#125;&lt;/h1>`, document.getElementById('app')); &#125;); count.value++; In the above code, count is a ref reactive data. When modifying the value of count.value, the side effect function will be re-executed, triggering re-rendering. The final content rendered to the page is &lt;h1&gt;2&lt;/h1&gt;. Here, the reactive API provided by Vue 3’s @vue/reactivity package is used. It can be included in the HTML file using the &lt;script&gt; tag: &lt;script src=\"https://unpkg.com/@vue/reactivity@3.0.5/dist/reactivity.global.js\">&lt;/script> The basic implementation of the render function is given in the above code. Let’s analyze its execution flow in detail. Suppose we call the renderer.render function three times consecutively for rendering: const renderer = createRenderer(); // Initial rendering renderer.render(vnode1, document.querySelector('#app')); // Second rendering renderer.render(vnode2, document.querySelector('#app')); // Third rendering renderer.render(null, document.querySelector('#app')); During the initial rendering, the renderer renders vnode1 into real DOM and stores vnode1 in the container element’s container.vnode property as the old vnode. During the second rendering, the old vnode exists (container.vnode has a value), and the renderer takes vnode2 as the new vnode, passing both the new and old vnodes to the patch function to perform patching. During the third rendering, the new vnode’s value is null, indicating that no content should be rendered. However, at this point, the container already contains the content described by vnode2, so the renderer needs to clear the container. In the code above, container.innerHTML = &#39;&#39; is used to clear the container. It’s important to note that clearing the container this way is not the best practice but is used here for demonstration purposes. Regarding the patch function, it serves as the core entry point of the renderer. It takes three parameters: the old vnode n1, the new vnode n2, and the container container. During the initial rendering, the old vnode n1 is undefined, indicating a mounting action. The patch function not only serves for patching but can also handle mounting actions. Custom RendererThe implementation of a custom renderer involves abstracting the core rendering logic, making it independent of specific platform APIs. The following example demonstrates the implementation of a custom renderer using configuration options to achieve platform-independent rendering: // Create a renderer function, accepting options as parameters function createRenderer(options) &#123; // Retrieve DOM manipulation APIs from options const &#123; createElement, insert, setElementText &#125; = options; // Define the function to mount elements function mountElement(vnode, container) &#123; // Call createElement function to create an element const el = createElement(vnode.type); // If children are a string, use setElementText to set text content if (typeof vnode.children === 'string') &#123; setElementText(el, vnode.children); &#125; // Call insert function to insert the element into the container insert(el, container); &#125; // Define the function to patch elements function patch(n1, n2, container) &#123; // Implement patching logic, this part is omitted in the example &#125; // Define the render function, accepting virtual nodes and a container as parameters function render(vnode, container) &#123; // If the old virtual node exists, execute patching logic; otherwise, execute mounting logic if (container.vnode) &#123; patch(container.vnode, vnode, container); &#125; else &#123; mountElement(vnode, container); &#125; // Store the current virtual node in the container's vnode property container.vnode = vnode; &#125; // Return the render function return render; &#125; // Create configuration options for the custom renderer const customRendererOptions = &#123; // Function for creating elements createElement(tag) &#123; console.log(`Creating element $&#123;tag&#125;`); // In a real application, you can return a custom object to simulate a DOM element return &#123; type: tag &#125;; &#125;, // Function for setting an element's text content setElementText(el, text) &#123; console.log(`Setting text content of $&#123;JSON.stringify(el)&#125;: $&#123;text&#125;`); // In a real application, set the object's text content el.textContent = text; &#125;, // Function for inserting an element under a given parent insert(el, parent, anchor = null) &#123; console.log(`Adding $&#123;JSON.stringify(el)&#125; to $&#123;JSON.stringify(parent)&#125;`); // In a real application, insert el into parent parent.children = el; &#125;, &#125;; // Create a render function using the custom renderer's configuration options const customRenderer = createRenderer(customRendererOptions); // Create a virtual node describing &lt;h1>hello&lt;/h1> const vnode = &#123; type: 'h1', children: 'hello', &#125;; // Use an object to simulate a mounting point const container = &#123; type: 'root' &#125;; // Render the virtual node to the mounting point using the custom renderer customRenderer(vnode, container); In the above code, we create a custom renderer using the createRenderer function, which takes configuration options as parameters. The configuration options include functions for creating elements, setting text content, and inserting elements into a parent. The customRenderer function takes a virtual node and a container as parameters, and it can handle both mounting and patching logic based on the existence of the old virtual node (container.vnode). This custom renderer allows platform-independent rendering by abstracting the core logic and making it adaptable to different platforms through configuration options. Please note that the code above demonstrates the concept of a custom renderer and focuses on its implementation logic. In a real-world scenario, you might need additional error handling, optimizations, and proper DOM manipulations based on the specific platform requirements.","link":"/2023/11/05/en/vue-renderer-1/"},{"title":"Handling Event Rendering in Vue","text":"IntroductionIn this section, we will discuss how to handle events in Vue, including how to describe events in virtual nodes, how to add events to DOM elements, and how to update events. Let’s start by addressing the first question, which is how to describe events in virtual nodes. Events can be considered as special attributes, so we can agree that any attribute starting with the string “on” in the vnode.props object should be treated as an event. For example: const vnode = &#123; type: 'p', props: &#123; // Describe events using onXxx onClick: () => &#123; alert('clicked'); &#125; &#125;, children: 'text' &#125;; Once we have resolved how events are described in virtual nodes, let’s see how to add events to DOM elements. This is very simple, just call the addEventListener function in the patchProps method to bind the event, as shown in the following code: function patchProps(el, key, prevValue, nextValue) &#123; // Match attributes starting with on as events if (/^on/.test(key)) &#123; // Get the corresponding event name based on the attribute name, e.g., onClick ---> click const name = key.slice(2).toLowerCase(); // Remove the previously bound event handler prevValue &amp;&amp; el.removeEventListener(name, prevValue); // Bind the new event handler el.addEventListener(name, nextValue); &#125; else if (key === 'class') &#123; // Omitted code (handling class attribute logic) &#125; else if (shouldSetAsProps(el, key, nextValue)) &#123; // Omitted code (handling other attribute logic) &#125; else &#123; // Omitted code (handling other attribute logic) &#125; &#125; In fact, the event update mechanism can be further optimized to avoid multiple calls to removeEventListener and addEventListener. function patchProps(el, key, prevValue, nextValue) &#123; if (/^on/.test(key)) &#123; const name = key.slice(2).toLowerCase(); let invoker = el.__vei || (el.__vei = &#123;&#125;); if (nextValue) &#123; if (!invoker[name]) &#123; // If there is no invoker, create a fake invoker function invoker[name] = (e) => &#123; invoker[name].value(e); &#125;; &#125; // Assign the actual event handler to the value property of the invoker function invoker[name].value = nextValue; // Bind the invoker function as the event handler el.addEventListener(name, invoker[name]); &#125; else if (invoker[name]) &#123; // If the new event handler does not exist and the previously bound invoker exists, remove the binding el.removeEventListener(name, invoker[name]); invoker[name] = null; &#125; &#125; else if (key === 'class') &#123; // Omitted code (handling class attribute logic) &#125; else if (shouldSetAsProps(el, key, nextValue)) &#123; // Omitted code (handling other attribute logic) &#125; else &#123; // Omitted code (handling other attribute logic) &#125; &#125; Looking at the above code, event binding is divided into two steps. First, read the corresponding invoker from el._vei. If invoker does not exist, create a fake invoker function and cache it in el._vei. Assign the actual event handler to the invoker.value property, and then bind the fake invoker function as the event handler to the element. When the event is triggered, the fake event handler is executed, indirectly invoking the actual event handler invoker.value(e). When updating events, since el._vei already exists, we only need to modify the value of invoker.value to the new event handler. This way, updating events can avoid a call to removeEventListener, improving performance. However, the current implementation still has issues. The problem is that el._vei currently caches only one event handler at a time. This means that if an element binds multiple events simultaneously, event override will occur. const vnode = &#123; type: 'p', props: &#123; // Describe events using onXxx onClick: () => &#123; alert('clicked'); &#125;, onContextmenu: () => &#123; alert('contextmenu'); &#125; &#125;, children: 'text' &#125;; // Assume renderer is your renderer object renderer.render(vnode, document.querySelector('#app')); When the renderer tries to render the vnode provided in the above code, it first binds the click event and then binds the contextmenu event. The contextmenu event handler bound later will override the click event handler. To solve the event override problem, we need to redesign the data structure of el._vei. We should design el._vei as an object, where the keys are event names and the values are corresponding event handler functions. This way, event override issues will be resolved. Based on the code snippet you provided, this code is mainly used for handling attribute updates on DOM elements, including the logic for event binding and unbinding. In this code, it uses an el._vei object to cache event handler functions.","link":"/2023/11/05/en/vue-renderer-3/"},{"title":"vue-watch-computed","text":"IntroductionPreviously, we discussed the effect function, which is used to register side-effect functions. It allows specifying options parameters, such as the scheduler to control the timing and manner of side-effect function execution. We also explored the track function for dependency tracking and the trigger function to re-execute side-effect functions. Combining these concepts, we can implement a fundamental and distinctive feature of Vue.js – computed properties. Computed Properties and the lazy OptionIn Vue.js, the effect function is used to create reactive side-effect functions. By default, side-effect functions passed to effect are executed immediately. For example, in the code below, the side-effect function passed to the effect function is executed immediately: effect(() => &#123; console.log(obj.foo); &#125;); However, in certain cases, we want the side-effect function to execute only when needed, not immediately. A typical scenario is with computed properties. To achieve this delayed execution, we can add a lazy property to the options object and set it to true. When lazy is true, the side-effect function is not executed during initialization but only when necessary. The modified code looks like this: effect( // This function will not execute immediately () => &#123; console.log(obj.foo); &#125;, // options &#123; lazy: true &#125; ); In the implementation, the side-effect function effectFn is returned as the result of the effect function. This means that when we call the effect function, we get the corresponding side-effect function and can manually execute it when needed. This mechanism gives us more control, allowing us to decide when to trigger the execution of the side-effect function rather than executing it immediately. This design pattern is particularly suitable for specific scenarios like computed properties. In computed properties, we might want to trigger the side-effect function’s execution at a specific moment rather than immediately during initialization. By returning the side-effect function from the effect function, we can flexibly control when the side-effect function is executed to meet different requirements in various scenarios. function effect(fn, options = &#123;&#125;) &#123; const effectFn = () => &#123; cleanup(effectFn); activeEffect = effectFn; effectStack.push(effectFn); fn(); effectStack.pop(); activeEffect = effectStack[effectStack.length - 1]; &#125;; // Set options and dependencies for the side-effect function effectFn.options = options; effectFn.deps = []; // Execute the side-effect function only if it's not lazy if (!options.lazy) &#123; effectFn(); &#125; // Return the side-effect function as the result return effectFn; &#125; In this code, the effect function’s second parameter is an options object, where the lazy property is set to true. This means that the side-effect function passed to effect will be executed only when necessary, such as when accessing a computed property. The lazy property allows us to control the immediate execution of the side-effect function. Now that we have achieved lazy computation through computed properties, how do we implement data caching? function computed(getter) &#123; // value is used to cache the last computed value let value; // dirty flag indicates whether a recalculation is needed; if true, it means \"dirty\" and needs computation let dirty = true; const effectFn = effect(getter, &#123; lazy: true &#125;); const obj = &#123; get value() &#123; // Compute the value only if it's \"dirty,\" and cache the computed value in the value variable if (dirty) &#123; value = effectFn(); // Set dirty to false, so the cached value can be used directly next time dirty = false; &#125; return value; &#125; &#125;; return obj; &#125; With lazy computation resolved, the value is calculated only when it is truly needed, executing effectFn only when necessary. Additionally, a dirty flag is introduced to indicate whether the current computation needs to be recalculated. If dirty is true, the value is recalculated, and the dirty flag is set to false so that the cached value can be used directly next time. Implementation Principle of watchThe concept of watch essentially involves observing a reactive data and executing the corresponding callback function when the data changes. For example: watch(obj, () => &#123; console.log('Data changed'); &#125;) // Modifying the reactive data triggers the execution of the callback function obj.foo++ Suppose obj is a reactive data, watched using the watch function with a provided callback function. When modifying the reactive data’s value, the callback function is triggered. In fact, the implementation of watch essentially utilizes the effect and the options.scheduler option, as shown in the following code: effect(() => &#123; console.log(obj.foo) &#125;, &#123; scheduler() &#123; // The scheduler function is executed when obj.foo's value changes &#125; &#125;) In a side-effect function accessing the reactive data obj.foo, based on the previous discussion, we know that this establishes a connection between the side-effect function and the reactive data. When the reactive data changes, the side-effect function is re-executed. However, there is an exception: if the side-effect function has a scheduler option, when the reactive data changes, the scheduler function is executed instead of directly triggering the side-effect function. From this perspective, the scheduler function acts as a callback function, and the implementation of watch utilizes this characteristic. Below is the simplest implementation of the watch function: // The watch function receives two parameters: source (reactive data) and cb (callback function) function watch(source, cb) &#123; effect( // Trigger a read operation to establish a connection () => source.foo, &#123; scheduler: scheduler(), // Call the callback function cb when the data changes fn: () => &#123; cb(); &#125;, &#125; ); &#125; In this code, we first define an original data object named data, which contains a property foo with an initial value of 1. Next, we create a proxy object obj using Proxy, intercepting operations on the data. When obj.foo++ is executed, the set interceptor of Proxy is triggered. In the set interceptor, we first set the property value on the target object and then call the watch function, passing obj and a callback function. In the watch function, we use a hypothetical effect function (which might be provided by a framework) to listen for data changes. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/31/en/vue-watch-computed/"},{"title":"A Brief Discussion on WebAssembly","text":"IntroductionAfter reading numerous articles on WebAssembly and conducting some performance tests, I’d like to share my insights on this technology. Is WASM Equivalent to Assembly-Level Performance?Certainly not. The assembly in WASM does not mean actual assembly code; it is a new bytecode with its own conventions that need an interpreter to run. This interpreter is much faster than a JavaScript interpreter but still falls short of native machine code performance. As a reference point, when JavaScript is optimized with Just-In-Time (JIT) compilation, its overall performance is roughly 1&#x2F;20th of machine code. In comparison, WASM can achieve about 1&#x2F;3rd of machine code performance (these figures vary depending on the context and are for reference purposes only). Even if you write code in languages like C++ and Rust, the performance you get is comparable to Java and C#, not native machine code. This explains why WASM does not demonstrate overwhelming performance advantages in all application scenarios: if you know how to optimize JS to run efficiently, it can compete with Rust in the browser environment. A classic case of performance comparison between WASM and JS occurred in a debate between Mozilla developers and V8 developers. Mozilla Hacks published an article titled “Optimizing Source Maps Performance with Rust and WebAssembly”, optimizing the performance of the source-map JavaScript package by five times. V8 core developer Vyacheslav Egorov responded with an article titled “You Might Not Need Rust and WebAssembly to Speed Up Your JS”, achieving astonishing optimizations in pure JS that outperformed Rust. The debate was intense, and the performance comparison chart after three rounds clearly showed Rust’s superiority, although JS managed to outperform in one round: Additionally, Milo Yip conducted performance tests on different languages for ray tracing (a highly intensive computation task), supporting the conclusion about performance comparisons between languages and machine code. C++, Java, and JS, without specific optimizations, can represent three typical performance levels: Language Rendering Comparison (C++&#x2F;C#&#x2F;F#&#x2F;Java&#x2F;JS&#x2F;Lua&#x2F;Python&#x2F;Ruby) Is WASM Faster Than JS, So It Should Be Used for Compute-Intensive Applications?This assumption is a bit biased. WASM is still processed on the CPU. For tasks that can be highly parallelized, using WebGL for GPU acceleration is often much faster. For instance, algorithms for image processing, as I discussed in my article [“Practical WebGL Image Processing Introduction”](link to the article), can easily be several tens of times faster by using WebGL than by looping through canvas pixels in JS. Rewriting such a nested loop in WASM to achieve a few times improvement over JS is already considered quite good. Regarding AI computations in the browser, community evaluations show that WebGL and WebMetal offer the highest performance levels, followed by WASM. Refer to this article: “Browser-Based AI Evaluations” However, WebGL acceleration has precision issues. For example, the core of the frontend image resizing library Pica uses the Lanczos sampling algorithm. I implemented this algorithm with WebGL shaders; it is not complicated. The early version of Pica once included optional WebGL optimizations, but now it has shifted to WASM. The reason is that WASM can ensure consistent computation results with JS for the same parameters, whereas WebGL cannot. For related discussions, see Issue #114 · nodeca&#x2F;pica Moreover, there are not many compute-intensive scenarios in frontend development. Tasks like encryption, compression, and mining are not high-frequency requirements. As for potentially essential AI applications in the future, I personally have confidence in WebGPU, the next-generation standard that can fully unleash GPU potential. However, WASM is already a good alternative. Does Embedding a WASM Function in JS Automatically Improve Performance?Not necessarily. Modern JS engines have powerful tools for performance optimization, namely JIT (Just-In-Time compilation). Simply put, if a function like the add function in the code const add = (a, b) =&gt; a + b consistently performs integer addition, the JS engine will automatically compile machine code to compute int a + int b, replacing the original JS function. This optimization significantly enhances the performance of frequent calls to this function. This is the magic of JIT (Just-In-Time) compilation. So, don’t assume that JS is slow and think of manually replacing such JS functions with C compiled to WASM to improve performance. Modern JS engines automatically “translate JS to C” like this for you! If you can rewrite a JS function into equivalent C code, it’s highly likely that this function, when inlined, will achieve similar performance through JIT compilation. This is probably why V8 developers confidently challenged Rust with JS in the debate I mentioned earlier. In the article “Calls between JavaScript and WebAssembly are Finally Fast 🎉”, Lin Clark eloquently discusses the optimization process. In the end, function calls between JS and WASM are faster than non-inlined JS function calls. However, the comparison between these calls and JS functions that are inlined by JIT is not mentioned in the article. It’s worth mentioning that Mozilla often promotes their massively optimized work, much of which might have stemmed from apparent design issues (let’s be honest; we all have our moments). For instance, the significant power-saving optimization in Firefox 70 for Mac was rooted in what exactly? A rough understanding is that the previous version of Firefox on Mac updated the window pixels for every frame! Of course, these articles contain substantial information, and I highly recommend reading the original texts after building a good foundation. It opens up a broader world and often inspires insights into software architecture design. If WASM supports garbage collection (GC) in the future, managing object lifecycles between JS and WASM may become more complicated. For example, I recently attempted to synchronize large objects between Dart in Flutter and Java in Android, hoping to “embed some Android platform capabilities into the Flutter ecosystem.” However, this approach led to a lot of verbose and low-performance glue code. Objects had to be deep-copied asynchronously through messages, with very low controllability. Although WASM currently does not have GC, once it’s added, I have reasons to suspect that managing object lifecycles between WASM and JS will face similar challenges. However, this problem mainly concerns Mozilla and Google developers; it’s not something we need to worry about. Is WASM Just Like Calling C from Python in Terms of Simplicity?This question can only be answered by practical experience. For example, I have recently attempted the following: Calling C++ from Java classes in Android Calling C from Dart in Flutter Calling C&#x2F;C++ from QuickJS, an embedded JS engine All of these cases involve creating native objects in the engine and passing them to C&#x2F;C++ functions by reference. This method is generally referred to as FFI (Foreign Function Interface), allowing native code to be embedded in language runtimes. However, if you are dealing with two different runtimes, things are not that simple. For instance, in the Quack project, which aims to bind QuickJS with Java, marshaling (similar to serialization and deserialization like JSON) has to be done between JS and Java objects; you cannot simply pass references. So, how does it work with WASM? Essentially, WASM’s linear memory can be freely read and written by JS, without the hassle of deep copying. However, WASM does present some challenges in terms of data flow. It only supports basic data types such as integers and floats; there is no support for complex data structures like strings. Thus, for slightly more complex objects, it’s challenging to manually define corresponding structures on both the JS and WASM sides. This difficulty makes it complicated to directly perform complex object transformations using WASM. Currently, this dirty work is left to tools like wasm-bindgen, which handles complex object transformations between languages. wasm-pack uses another tool called wasm-bindgen to bridge JavaScript and Rust, among other types. However, this process is not the same as directly embedding C&#x2F;C++ functions in JS runtime, as with traditional FFI compiled to machine code. For example, if you frequently manipulate JS objects with WASM, it can almost certainly impact performance. A typical pitfall in this regard is porting OpenGL applications to WASM. For example, a function like glTexImage2D in C++ now needs to go through two layers: first, it goes from WASM to JS in the glue layer, and then from JS to WebGL API like gl.texImage2D through C++ binding. This adds an extra layer of complexity compared to directly writing the equivalent JS code. Can this approach match the performance of writing JS directly instead of two layers of glue code? Of course, Mozilla is aware of this issue. Hence, they are exploring how to better expose Web IDL (the bindings of browser-native APIs) to WASM. In this process, they introduced the concept of WASM Interface Types: since WASM is already an intermediate bytecode, why not establish a universal Intermediate Representation (IR) specification that can unify all types across programming language runtimes? However, this specification hopes to solve problems mainly through protocolization and structured deep copying, with only the anyref type allowing passing by reference. anyref behaves somewhat like file descriptors in Unix; I won’t delve into this here. Is WASM Part of the Frontend Ecosystem?I do not agree with this statement. It’s essential to note that the toolchains for compiling WASM applications and the libraries they depend on have little to do with JS. A toolchain that supports cross-compilation typically comes with libraries supporting the target platform. For example, after including &lt;GLES2/gl2.h&gt;, the glTexImage2D API you call is provided by the dynamic library. This API can run consistently on x86, ARM, MIPS, WASM, etc., platforms (similar to .so files in Android). Emscripten provides a set of dynamic libraries specifically for the WASM platform, compiling them into JS format. However, it only guarantees that these APIs are available; performance is a different story. Emscripten also provides many optimization suggestions for porting WebGL applications. So, I’d like to reiterate that the dependencies and toolchains required to compile WASM applications are almost entirely unrelated to JS. JS is just a format produced by these toolchains, similar to machine code. From the perspective of JS developers, these toolchains may seem quite unfamiliar. Still, from the perspective of native application developers, everything is quite standard. ConclusionWebAssembly is undoubtedly a revolutionary technology, representing a new cross-platform direction, especially valuable for native application developers. However, for frontend developers, it’s just a bytecode virtual machine embedded in the browser. I hope this article clarifies some misconceptions and provides a better understanding of WebAssembly’s capabilities and limitations. While it’s a powerful tool, it’s essential to use it judiciously and consider its advantages and trade-offs within the context of your specific use case. Remember, WebAssembly is not a magic solution that automatically improves performance in all scenarios. It’s another option in the toolkit, providing a balance between performance, development cost, and effectiveness. As the technology evolves, it will be interesting to see how it integrates further into the broader web ecosystem. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/03/en/wasm-1/"},{"title":"Webpack Custom Loader/Plugin","text":"IntroductionA loader is a node module exported as a function. This function is called when transforming resources in the loader. The given function will utilize the Loader API and can be accessed through the this context. Here is an official link on loader usage and examples, including local development and testing of custom loaders. Simple Usage of Webpack LoaderWhen a loader is used in a resource, it can only take one parameter - a string containing the content of the resource file. Synchronous loaders can return a single value representing the transformed module. Loaders can return one or two values. The first value is a string or buffer containing JavaScript code. The optional second value is a SourceMap, which is a JavaScript object. Here’s a simple example of using a loader. It matches all JavaScript files and processes them using loader.js: // webpack.config.js const path = require('path'); module.exports = &#123; //... module: &#123; rules: [ &#123; test: /\\.js$/, use: [ &#123; loader: path.resolve('path/to/loader.js'), options: &#123; /* ... */ &#125;, &#125;, ], &#125;, ], &#125;, &#125;; From the above, we can understand how loaders are used. But this only scratches the surface. What does a specific loader look like? For example, a simple loader could be like this: module.exports = function (content) &#123; // content is the source content string passed in return content &#125; A loader is just a node module exposing a function that can only receive one parameter: a string containing the content of the resource file. The function’s return value is the processed content. Creating a Custom Webpack LoaderGuidelines for Using Custom LoadersWhen writing loaders, you should follow these guidelines. They are listed in order of importance, and some apply only to specific scenarios. Please read the detailed sections below for more information. Keep it simple. Use chaining. Output should be modular. Ensure it’s stateless. Use loader utilities. Record loader dependencies. Resolve module dependencies. Extract common code. Avoid absolute paths. Use peer dependencies. Step 1: Create Project Directory and FilesFirst, create the following files in a folder within your webpack project directory: src/loader/custom-loader.js: The source file for your custom loader. src/index.js: JavaScript entry file for testing the custom loader. Step 2: Write the Custom LoaderIn the custom-loader.js file, write your custom loader code. This loader adds a comment at the top of each loaded JavaScript file. // src/loader/custom-loader.js module.exports = function(source) &#123; // Add a custom comment at the top of the source code const updatedSource = `/** Custom Comment added by Custom Loader */\\n$&#123;source&#125;`; return updatedSource; &#125;; Step 3: Configure WebpackCreate a Webpack configuration file webpack.config.js in the project root directory. Use the custom loader you just created in the configuration file. // webpack.config.js const path = require('path'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: path.resolve(__dirname, 'dist'), &#125;, module: &#123; rules: [ &#123; test: /\\.js$/, use: ['custom-loader'], // Use the custom loader to process .js files exclude: /node_modules/, &#125;, ], &#125;, &#125;; This configuration achieves a simple functionality. Now let’s discuss how to test the local loader. There are two ways to do this: one is through Npm link for testing, a convenient method where you can create a symbolic link for local testing. Here is a link to npm-link. Another way is to configure the path directly in the project: Single Loader Configuration// webpack.config.js &#123; test: /\\.js$/ use: [ &#123; loader: path.resolve('path/to/custom-loader.js'), options: &#123;/* ... */&#125; &#125; ] &#125; Multiple Loader ConfigurationYou can also configure it using an array: // webpack.config.js resolveLoader: &#123; // Look for loaders first in the node_modules directory; if not found, search in the loaders directory modules: [ 'node_modules', path.resolve(__dirname, 'custom-loader') ] &#125; Step 4: Test the Custom LoaderIn the index.js file, write some JavaScript code, for example: // src/index.js console.log('Hello, Webpack Loader!'); Step 5: Run Webpack BuildRun the following command to build your project: npx webpack --config webpack.config.js After the build is complete, you will find the generated bundle.js file in the dist folder. In this file, you can see JavaScript code with a custom comment added at the top. ## Simple Usage of Webpack Plugin Plugins provide complete control over the webpack engine for third-party developers. By introducing custom behaviors into the webpack build process through stage-based build callbacks, developers can customize webpack&#39;s behavior. Here&#39;s the simplest example: &#96;&#96;&#96;javascript &#x2F;&#x2F; webpack.config.js const HtmlWebpackPlugin &#x3D; require(&#39;html-webpack-plugin&#39;); module.exports &#x3D; &#123; entry: &#39;.&#x2F;src&#x2F;index.js&#39;, output: &#123; filename: &#39;bundle.js&#39;, path: __dirname + &#39;&#x2F;dist&#39;, &#125;, plugins: [ new HtmlWebpackPlugin(&#123; template: &#39;.&#x2F;src&#x2F;index.html&#39;, &#x2F;&#x2F; Specify the HTML template file filename: &#39;index.html&#39;, &#x2F;&#x2F; Generated HTML file name &#125;), &#x2F;&#x2F; You can add more plugins here ], &#125;; In this example, the HtmlWebpackPlugin is used. It generates a new HTML file based on the specified HTML template and automatically adds the bundled JavaScript file to the generated HTML file. A basic webpack plugin consists of the following components: A JavaScript named function or JavaScript class. Define an apply method on the plugin function’s prototype. The apply method is called when webpack loads the plugin and is passed the compiler object. Specify an event hook bound to webpack itself. Process specific data from webpack’s internal instances. Call the callback provided by webpack after the functionality is completed. A plugin structure looks like this: class HelloWorldPlugin &#123; apply(compiler) &#123; compiler.hooks.done.tap( 'Hello World Plugin', ( stats /* After binding the done hook, stats is passed as a parameter. */ ) => &#123; console.log('Hello World!'); &#125; ); &#125; &#125; module.exports = HelloWorldPlugin; Compiler and CompilationThe two most important resources in plugin development are the compiler and compilation objects. Plugin development revolves around hooks on these objects. The compiler object is essentially bound to the entire webpack environment. It contains all the environment configurations, including options, loaders, and plugins. When webpack starts, this object is instantiated and it is globally unique. The parameters passed into the apply method are properties of this object. The compilation object is created each time resources are built. It represents the current module resources, compiled generated resources, changed files, and tracked dependency status. It also provides many hooks. Creating a Custom Webpack PluginStep 1: Create Project Directory and FilesFirst, create the following file in a folder within your webpack project directory: src/plugins/CustomPlugin.js: Source file for your custom plugin. Step 2: Write the Custom PluginIn the CustomPlugin.js file, write a plugin that outputs a message when the webpack build process is completed. // src/plugins/CustomPlugin.js class CustomPlugin &#123; apply(compiler) &#123; compiler.hooks.done.tap('CustomPlugin', () => &#123; console.log('CustomPlugin: Webpack build process is done!'); &#125;); &#125; &#125; module.exports = CustomPlugin; Step 3: Configure WebpackIn the configuration file, use the custom plugin you just created. // webpack.config.js const CustomPlugin = require('./src/plugins/CustomPlugin'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: __dirname + '/dist', &#125;, plugins: [ new CustomPlugin(), // You can add more plugins here ], &#125;; Step 4: Run Webpack BuildNow, run the webpack build: npx webpack --config webpack.config.js Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/29/en/webpack-plugin-design/"}],"tags":[{"name":"http","slug":"http","link":"/en/tags/http/"},{"name":"Cloud-Computing","slug":"Cloud-Computing","link":"/en/tags/Cloud-Computing/"},{"name":"Webpack,Front-end","slug":"Webpack-Front-end","link":"/en/tags/Webpack-Front-end/"},{"name":"Ajax","slug":"Ajax","link":"/en/tags/Ajax/"},{"name":"JavaScript","slug":"JavaScript","link":"/en/tags/JavaScript/"},{"name":"Front-end","slug":"Front-end","link":"/en/tags/Front-end/"},{"name":"Hadoop,Cloud-Computing","slug":"Hadoop-Cloud-Computing","link":"/en/tags/Hadoop-Cloud-Computing/"},{"name":"Vue","slug":"Vue","link":"/en/tags/Vue/"},{"name":"Wasm","slug":"Wasm","link":"/en/tags/Wasm/"},{"name":"JIT compiler,JavaScript,WebAssembly","slug":"JIT-compiler-JavaScript-WebAssembly","link":"/en/tags/JIT-compiler-JavaScript-WebAssembly/"},{"name":"Build","slug":"Build","link":"/en/tags/Build/"}],"categories":[{"name":"http","slug":"http","link":"/en/categories/http/"},{"name":"Cloud-Computing","slug":"Cloud-Computing","link":"/en/categories/Cloud-Computing/"},{"name":"Webpack","slug":"Webpack","link":"/en/categories/Webpack/"},{"name":"Front-end","slug":"Front-end","link":"/en/categories/Front-end/"},{"name":"Hadoop","slug":"Hadoop","link":"/en/categories/Hadoop/"},{"name":"Build","slug":"Build","link":"/en/categories/Build/"},{"name":"Vue","slug":"Vue","link":"/en/categories/Vue/"},{"name":"Cloud-Computing","slug":"Hadoop/Cloud-Computing","link":"/en/categories/Hadoop/Cloud-Computing/"},{"name":"Wasm","slug":"Wasm","link":"/en/categories/Wasm/"},{"name":"JIT compiler","slug":"JIT-compiler","link":"/en/categories/JIT-compiler/"}]}