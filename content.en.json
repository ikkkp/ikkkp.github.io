{"pages":[{"title":"About Me","text":"Writing: A Soliloquy of the Soul“Writing is a soliloquy of the soul, a flow of the spirit.” - Wang Xiaobo I am ikkkp. In fact, the idea of creating my own personal website and sharing things I love on it has been in my mind for quite some time. The reason I write might not be very complicated. Words are my bridge to the world, a tool with which I think, feel, and touch life. Writing, for me, is a way to find companionship in solitude, a beacon of direction in moments of confusion. Words annotate life, and writing reveals the inner self. In this noisy world, writing has found me a sanctuary, a moment of tranquility. Writing allows me to think more clearly, to understand myself more profoundly. Every time I transform my thoughts into words, it’s like giving my soul a baptism, a process that helps me comprehend the depths of my inner world. Writing is also my way of connecting with others. Through words, I can convey my thoughts, share my experiences, and build profound emotional bonds with others. In the ocean of words, we can sail without constraints, exploring endless possibilities.","link":"/en/abouts/index.html"}],"posts":[{"title":"2023-11-22-Essay","text":"“When you don’t love me, I may still love you, and at the same time love the past self.He is cute, stubborn, and irreplaceable. Actually, thank you for your consideration. These days, the sky is blue and clear.” “Spending Long Years with Anthony”","link":"/2023/11/22/en/2023-11-22-Writing/"},{"title":"2023-11-24-随笔","text":"“All human suffering is essentially anger at one’s own powerlessness.”","link":"/2023/11/24/en/2023-11-24-Writing/"},{"title":"301/302 Redirection","text":"IntroductionWhen configuring redirects on a website, especially in the following scenarios, issues can arise: Redirecting from HTTP to HTTPS: Suppose you’ve set up an SSL certificate to upgrade your website from HTTP to HTTPS. If problems occur during this process, rendering the site inaccessible, you might consider reverting to the HTTP version. However, the challenge arises once a permanent 301 redirection is in place. Even if you remove the redirection on the server, browsers retain this information. Consequently, users’ browsers continue to enforce the HTTPS redirection, preventing them from accessing the HTTP version. Changing the Website Domain: When migrating a website from one domain (such as old-domain.com) to another (such as new-domain.com), a permanent 301 redirection is often employed. This informs search engines and browsers that the site has permanently moved to a new domain. However, if complications arise during this process, you may wish to undo the redirection, allowing users to access the old domain again. Unfortunately, due to browser hard caching of 301 redirections, users become permanently redirected to the new domain, unable to revisit the old one. To avoid such situations, it is advisable to use a temporary 302 redirection initially, ensuring everything functions correctly. Unlike a 301 redirection, a 302 redirection is not permanently cached by browsers. This means that if necessary, you can revert the redirection without users being permanently locked into the new URL. This approach eliminates the need for users to manually clear their browser caches, enhancing the overall user experience. 301 Redirection: Indicates that a resource (page) has permanently moved to a new location. The client&#x2F;browser should not attempt to request the original location but use the new location from now on. 302 Redirection: Indicates that the resource is temporarily located elsewhere, and the client&#x2F;browser should continue requesting the original URL. A 301 redirection is permanent, meaning that even if removed from the server, browsers will perpetually redirect resources to the new domain or HTTPS due to hard caching. On the other hand, a 302 redirection is not hard-cached by browsers. If you remove the redirection from your server (website), you can still access the old version. Clearing 301&#x2F;302 redirection cache typically involves clearing browser cache or the operating system’s DNS cache. Here’s how to do it on different platforms: Clearing Browser Cache (Applicable to Windows, macOS, Linux)Google Chrome: Open the Chrome browser. Click the three vertical dots in the upper right corner and select “More tools.” Choose “Clear browsing data.” In the popup window, select the “Advanced” tab. Set the time range to “All time.” Check the “Cached images and files” option. Click the “Clear data” button. Mozilla Firefox: Open the Firefox browser. Click the three horizontal lines in the upper right corner and select “Privacy &amp; Security.” In the “Cookies and Site Data” section, click “Clear Data.” Ensure the “Cache” option is checked. Click “Clear.” Microsoft Edge: Open the Edge browser. Click the three horizontal dots in the upper right corner and select “Settings.” Scroll down and click “View advanced settings.” In the “Privacy and services” section, click “Clear browsing data.” Check the “Cached images and files” option. Click the “Clear” button. Clearing Operating System’s DNS Cache (Applicable to Windows, macOS)Windows: Open Command Prompt (search for “cmd” in the Start menu and open it). Enter the following command and press Enter:ipconfig &#x2F;flushdns macOS: Open Terminal (find it in Applications &gt; Utilities folder). Enter the following command and press Enter:sudo dscacheutil -flushcache Then enter your administrator password and press Enter again. Please note that clearing browser cache might lead to loss of login sessions on websites. Ensure you have backed up essential information in case you need to log in again. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/28/en/301-302-Redirection/"},{"title":"Browser Default Behaviors","text":"PrefaceIn fact, browsers come with many built-in events, and many events automatically trigger certain behaviors in the browser. For example: Clicking a link triggers navigation to the URL. Clicking the submit button in a form triggers submission to the server. Pressing and dragging the mouse button on text selects the text. When handling an event with JavaScript, we usually don’t want the corresponding browser behavior to occur. Instead, we want to implement alternative behaviors. Preventing Browser BehaviorThere are two ways to tell the browser that we don’t want it to execute default behavior: The common way is to use the event object, which has a event.preventDefault() method. If the handler is assigned using on&lt;event&gt; (rather than addEventListener), returning false is also effective. In the example below, clicking the link will not trigger navigation, and the browser will not perform any action: &lt;a href=\"/\" onclick=\"return false\">Click here&lt;/a> or &lt;a href=\"/\" onclick=\"event.preventDefault()\">here&lt;/a> Click hereorhere It’s important to note that using on&lt;event&gt; and returning false is not a good practice. Returning false from the handler is an exception. The return value of event handlers is usually ignored. The only exception is returning false from a handler assigned using on&lt;event&gt;. Handler Option “passive”The optional passive: true option in addEventListener signals to the browser that the handler will not call preventDefault(). Why is this necessary? On mobile devices, some events like touchmove (when the user moves their finger on the screen) can lead to scrolling by default, which can be prevented using preventDefault() in the handler. So, when the browser detects such events, it must first process all handlers. If preventDefault is not called anywhere, the page can continue scrolling. However, this may cause unnecessary delays and “jitter” in the UI. The passive: true option informs the browser that the handler will not cancel scrolling. The browser then immediately scrolls the page to provide a smoother experience and somehow processes the event. For certain browsers (such as Firefox and Chrome), touchstart and touchmove events have passive set to true by default. event.defaultPreventedIf the default behavior is prevented, the event.defaultPrevented property is true; otherwise, it is false. Here’s an interesting use case. Do you remember our discussion on event.stopPropagation() in the Bubbling and Capturing chapter and why stopping propagation is not good? Sometimes, we can use event.defaultPrevented as an alternative to notify other event handlers that the event has been handled. Let’s look at a practical example. By default, the browser displays a context menu with standard options on the contextmenu event (right-click). We can prevent it and display our custom menu like this: &lt;button>Right-click shows browser context menu&lt;/button> &lt;button oncontextmenu=\"alert('Draw our menu'); return false\"> Right-click shows our context menu &lt;/button> Right-click shows browser context menu Right-click shows our context menu Now, in addition to this context menu, let’s implement a document-wide context menu. When right-clicking, it should display the nearest context menu: &lt;p>Right-click here for the document context menu&lt;/p> &lt;button id=\"elem\">Right-click here for the button context menu&lt;/button> &lt;script> elem.oncontextmenu = function(event) &#123; event.preventDefault(); alert(\"Button context menu\"); &#125;; document.oncontextmenu = function(event) &#123; event.preventDefault(); alert(\"Document context menu\"); &#125;; &lt;/script> Right-click here for the document context menu Right-click here for the button context menu elem.oncontextmenu = function(event) { event.preventDefault(); alert(\"Button context menu\"); }; document.oncontextmenu = function(event) { event.preventDefault(); alert(\"Document context menu\"); }; Issue and SolutionsThe problem arises when clicking on elem, and we get two menus: the button-level menu and the document-level menu due to event bubbling. One solution is to prevent the event from bubbling up when handling the right-click event in the button. We can achieve this using event.stopPropagation(): &lt;p>Right-click for the document menu&lt;/p> &lt;button id=\"elem\">Right-click for the button menu (fixed with event.stopPropagation)&lt;/button> &lt;script> elem.oncontextmenu = function(event) &#123; event.preventDefault(); event.stopPropagation(); alert(\"Button context menu\"); &#125;; document.oncontextmenu = function(event) &#123; event.preventDefault(); alert(\"Document context menu\"); &#125;; &lt;/script> Now, the button-level menu works as expected. However, this comes at a cost — we deny any external code access to right-click information, including counters for collecting statistics. This is not advisable. Another alternative is to check whether the document handler has prevented the browser’s default behavior. If it has, the event has already been handled, and we don’t need to react to it: &lt;p>Right-click for the document menu (added a check for event.defaultPrevented)&lt;/p> &lt;button id=\"elem\">Right-click for the button menu&lt;/button> &lt;script> elem.oncontextmenu = function(event) &#123; event.preventDefault(); alert(\"Button context menu\"); &#125;; document.oncontextmenu = function(event) &#123; if (event.defaultPrevented) return; event.preventDefault(); alert(\"Document context menu\"); &#125;; &lt;/script> Now everything works as expected. If we have nested elements, each with its own context menu, this approach will work. Just make sure to check event.defaultPrevented in each contextmenu handler. event.stopPropagation() and event.preventDefault()As we’ve seen, event.stopPropagation() and event.preventDefault() (also considered as return false) are two different things. They are unrelated to each other. Nested Context Menu StructureThere are other ways to implement nested context menus. One approach is to have a global object with a document.oncontextmenu handler and a method to store other handlers. This object would capture any right-click, browse stored handlers, and run the appropriate one. However, every piece of code needing a context menu should be aware of this object and use its assistance rather than having its own contextmenu handler. SummaryThere are many default browser behaviors: mousedown — Starts selection (dragging the mouse for selection). click on &lt;input type=&quot;checkbox&quot;&gt; — Selects&#x2F;deselects the input. submit — Clicking &lt;input type=&quot;submit&quot;&gt; or pressing Enter in form fields triggers this event, leading the browser to submit the form. keydown — Pressing a key adds a character to a field or triggers other actions. contextmenu — Event occurs on right-click, triggering the default behavior of showing the browser context menu. …and many more… If we want to handle events using JavaScript only, all default behaviors can be prevented. To prevent default behavior, you can use event.preventDefault() or return false. The second method is only applicable to handlers assigned through on&lt;event&gt;. The passive: true option of addEventListener informs the browser that the behavior will not be prevented. This is useful for certain mobile events (like touchstart and touchmove) to let the browser scroll without waiting for all handlers to finish. If the default behavior is prevented, the value of event.defaultPrevented becomes true; otherwise, it is false.","link":"/2023/12/06/en/Browser-Default-Behavior/"},{"title":"Cloud Computing and Its Applications","text":"Definition of Cloud ComputingCloud computing is a type of service related to information technology, software, and the internet that provides on-demand, dynamically scalable, and inexpensive computing services through a network. Cloud computing is a pay-per-use model that provides available, convenient, and on-demand network access to a shared pool of configurable computing resources (including networks, servers, storage, application software, and services) that can be rapidly provisioned. History of Cloud ComputingIn March 2006, Amazon launched the Elastic Compute Cloud (EC2) service. On August 9, 2006, Google CEO Eric Schmidt first proposed the concept of “cloud computing” at the Search Engine Strategies conference (SES San Jose 2006). Google’s “cloud computing” originated from the “Google 101” project by Google engineer Christopher Beshlia. In October 2007, Google and IBM began promoting cloud computing on American university campuses. On February 1, 2008, IBM (NYSE: IBM) announced the establishment of the world’s first cloud computing center for Chinese software companies in the Wuxi Taihu New City Science and Education Industrial Park. On July 29, 2008, Yahoo, HP, and Intel announced a joint research project to launch a cloud computing research test bed to promote cloud computing. On August 3, 2008, the US Patent and Trademark Office website showed that Dell was applying for the “cloud computing” trademark to strengthen its control over the term that could reshape future technology architecture. In March 2010, Novell and the Cloud Security Alliance (CSA) jointly announced a vendor-neutral plan called the “Trusted Cloud Initiative.” In July 2010, the US National Aeronautics and Space Administration and supporting vendors such as Rackspace, AMD, Intel, and Dell announced the “OpenStack” open source code plan. Microsoft announced its support for the integration of OpenStack and Windows Server 2008 R2 in October 2010, while Ubuntu added OpenStack to version 11.04. In February 2011, Cisco Systems officially joined OpenStack and focused on developing OpenStack’s network services. Technical Background of Cloud ComputingCloud computing is the commercial implementation of concepts in computer science such as parallel computing, distributed computing, and grid computing. Cloud computing is the result of the mixed evolution and improvement of technologies such as virtualization, utility computing, Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).","link":"/2023/11/08/en/Cloud-Computing/"},{"title":"Vue Components and Web Components Custom Elements","text":"PrefaceWeb Components is a collective term for a set of native web APIs that allow developers to create reusable custom elements. Vue and Web Components are primarily complementary technologies. Whether integrating custom elements into existing Vue applications or building and distributing custom elements using Vue, Vue provides excellent support for both using and creating custom elements. What are Custom ElementsA key feature of Web Components is the ability to create custom elements: HTML elements whose behavior is defined by web developers, extending the set of elements available in browsers. There are two types of custom elements in Web Components: Customized Built-in Elements: Inherit from standard HTML elements, such as HTMLImageElement or HTMLParagraphElement. Their implementation defines the behavior of standard elements. Autonomous Custom Elements: Inherit from the HTML element base class HTMLElement. You have to implement their behavior from scratch. Custom Element Lifecycle CallbacksCustom Elements also have lifecycle callbacks. Once your custom element is registered, the browser calls certain methods of your class when your custom element interacts with the page in specific ways. By providing implementations for these methods, known as lifecycle callbacks, you can run code to respond to these events. The custom element lifecycle callbacks include: connectedCallback(): Called whenever the element is added to the document. The specification recommends developers to set up custom element properties in this callback rather than in the constructor. disconnectedCallback(): Called whenever the element is removed from the document. adoptedCallback(): Called whenever the element is moved to a new document. attributeChangedCallback(): Called when attributes are changed, added, removed, or replaced. For more detailed information about this callback, see Responding to attribute changes. Here is a minimal example of a custom element that logs these lifecycle events: // Create a class for this element class MyCustomElement extends HTMLElement &#123; static observedAttributes = [\"color\", \"size\"]; constructor() &#123; // Must call the super method first super(); &#125; connectedCallback() &#123; console.log(\"Custom element added to the page.\"); &#125; disconnectedCallback() &#123; console.log(\"Custom element removed from the page.\"); &#125; adoptedCallback() &#123; console.log(\"Custom element moved to a new page.\"); &#125; attributeChangedCallback(name, oldValue, newValue) &#123; console.log(`Attribute $&#123;name&#125; has changed.`); &#125; &#125; customElements.define(\"my-custom-element\", MyCustomElement); Using Custom Elements in VueUsing custom elements in a Vue application is largely similar to using native HTML elements, but a few points need to be kept in mind: Skipping Component ResolutionBy default, Vue tries to parse non-native HTML tags as registered Vue components and then render them as custom elements. This leads to Vue issuing a “Unknown custom element” warning during development. To inform Vue that certain elements should be treated as custom elements and skip component resolution, we can specify the compilerOptions.isCustomElement option. If you are using Vue for build setup, this option should be passed through build configuration as it is a compile-time option. Example of in-browser configuration: &#x2F;&#x2F; Only works if using in-browser compilation. &#x2F;&#x2F; If using build tools, see config examples below. app.config.compilerOptions.isCustomElement &#x3D; (tag) &#x3D;&gt; tag.includes(&#39;-&#39;) Vite Configuration Example// vite.config.js import vue from '@vitejs/plugin-vue' export default &#123; plugins: [ vue(&#123; template: &#123; compilerOptions: &#123; // treat all tags with a dash as custom elements isCustomElement: (tag) => tag.includes('-') &#125; &#125; &#125;) ] &#125; Vue CLI Configuration Example// vue.config.js module.exports = &#123; chainWebpack: config => &#123; config.module .rule('vue') .use('vue-loader') .tap(options => (&#123; ...options, compilerOptions: &#123; // treat any tag that starts with ion- as custom elements isCustomElement: tag => tag.startsWith('ion-') &#125; &#125;)) &#125; &#125; Building Custom Elements with VueThe primary advantage of custom elements is their ability to be used with any framework or even without a framework. This makes them suitable for distributing components to end consumers who may not use the same frontend stack, or when you want to isolate the implementation details of the components used in the final application. Defining Custom Elements Vue supports creating custom elements using the same Vue component API with the defineCustomElement method. This method takes the same parameters as defineComponent but returns an extended custom element constructor HTMLElement: Template:0 &lt;my-vue-element&gt;&lt;&#x2F;my-vue-element&gt; import &#123; defineCustomElement &#125; from 'vue' const MyVueElement = defineCustomElement(&#123; // normal Vue component options here props: &#123;&#125;, emits: &#123;&#125;, template: `...`, // defineCustomElement only: CSS to be injected into shadow root styles: [`/* inlined css */`] &#125;) // Register the custom element. // After registration, all `&lt;my-vue-element>` tags // on the page will be upgraded. customElements.define('my-vue-element', MyVueElement) // You can also programmatically instantiate the element: // (can only be done after registration) document.body.appendChild( new MyVueElement(&#123; // initial props (optional) &#125;) ) When discussing custom elements and Vue components, we are essentially talking about two different approaches to building web applications. Custom elements are a web standard, akin to HTML elements, while Vue components are a more advanced building approach provided by the Vue.js framework. Some argue that using only custom elements is a more “future-proof” approach, but this passage points out that such a view is overly simplistic. It lists reasons explaining why the Vue component model is more practical. Some key points include: Vue components offer more features, such as a convenient template system, methods for managing state, and an efficient way to render components on the server. These features are essential for building complex applications. Vue components support powerful composition mechanisms, while custom elements have some limitations in this regard. This means that with Vue, you can build flexible and powerful component structures more easily. Using Vue, you leverage a mature framework and a large community, without having to build and maintain an internal framework. There is indeed some overlap in functionality between custom elements and Vue components: both allow us to define reusable components with data passing, event emitting, and lifecycle management. However, the Web Components API is relatively lower-level and simpler. To build actual applications, we need some additional features not covered by the platform: Declarative and efficient template system; A reactive state management system conducive to extracting and reusing logic across components; A high-performance method for rendering components on the server (SSR) and composing them on the client side, crucial for Web Vitals metrics like SEO and LCP. Native custom elements SSR typically involves simulating the DOM in Node.js and then serializing the mutated DOM, while Vue SSR compiles as much as possible into string concatenation, which is more efficient. defineCustomElement API Vue Component TransformationUsing the defineCustomElement API to transform Vue components into registerable custom element classes has several benefits: Cross-Framework Integration: By transforming Vue components into custom element classes, you can use these components in different frontend frameworks and libraries. This makes your components more versatile and integrable with other technology stacks. Independent Usage: After registering a Vue component as a custom element, it can be used independently of a Vue application. This means you can use the component without an entire Vue application, and introduce it into different build systems and module systems. Progressive Migration: If your application is gradually transitioning to Vue, you can achieve a progressive migration by transforming certain components into custom elements. This allows you to gradually introduce Vue components into an existing project without the need for a complete rewrite. Web Components Standard Compatibility: Registering Vue components as custom elements makes them compatible with the Web Components standard. This means you can leverage other tools and libraries in the Web Components ecosystem, enhancing the interoperability of your components. In other words, the defineCustomElement API’s purpose is to compile Vue components into custom elements that can be used in the browser without relying on the Vue compiler for real-time compilation. When using the defineCustomElement API, Vue components are pre-compiled into native custom elements, allowing them to be used directly in the browser without runtime compilation. Overall, by using the defineCustomElement API, you can combine Vue components with custom elements, making these components usable and shareable in a broader context. This provides increased reusability and flexibility for components, especially in the context of cross-platform component development, where you can develop components as custom elements and use them in different environments. A typical example is the integration implementation mentioned earlier between Vue 2 and Vue 3, meaning you only need to compile Vue 3 components into custom elements and then use them in Vue 2.","link":"/2023/11/29/en/Custom-Elements/"},{"title":"Bubbling and Capturing","text":"PrefaceLet’s start with an example. A handler is assigned to the &lt;div&gt;, but if you click on any nested tags (e.g., &lt;em&gt; or &lt;code&gt;), the handler will also run: &lt;div onclick=\"alert('The handler!')\"> &lt;em>If you click on &lt;code>EM&lt;/code>, the handler on &lt;code>DIV&lt;/code> runs.&lt;/em> &lt;/div> So, if you click on EM, the handler on DIV runs. BubblingThe bubbling principle is straightforward. When an event occurs on an element, it first runs the handler on that element, then runs the handler on its parent element, and continues up to handlers on other ancestors. Suppose we have 3 layers of nesting FORM &gt; DIV &gt; P, each with its own handler: &lt;style> body * &#123; margin: 10px; border: 1px solid blue; &#125; &lt;/style> &lt;form onclick=\"alert('form')\">FORM &lt;div onclick=\"alert('div')\">DIV &lt;p onclick=\"alert('p')\">P&lt;/p> &lt;/div> &lt;/form> Now, what happens if you click on &lt;p&gt;? First, the handler runs on &lt;p&gt;. Then, it runs on &lt;div&gt;. Then, it runs on &lt;form&gt;. Finally, it runs on the document. This behavior is called “event bubbling” because it bubbles up from the element like a bubble. So, if we click on &lt;p&gt;, we will see 3 alerts: p → div → form. event.targetThe handler on a parent element can always access detailed information about where the event actually occurred. The element that triggered the event, the deepest nested one, is called the target element, and it can be accessed through event.target. Note the difference with this (which equals event.currentTarget): this is the “current” element at the time the handler is running, and it remains the same. event.target is the “target” element when the event happens, and it can be any element; it changes during the bubbling process. Let’s illustrate this with an example! For instance, if we have a handler form.onclick, it can “capture” all clicks inside the form. Regardless of where the click happens, it will bubble up to &lt;form&gt; and run the handler. Note: Here, all events are bound to the form instead of individual elements. In the form.onclick handler: this (&#x3D;event.currentTarget) is the &lt;form&gt; element because the handler runs on it. event.target is the actual clicked element inside the form. &lt;!DOCTYPE HTML> &lt;html> &lt;head> &lt;meta charset=\"utf-8\"> &lt;/head> &lt;body> A click shows both &lt;code>event.target&lt;/code> and &lt;code>this&lt;/code> to compare: &lt;form style=\"background-color: green; position: relative; width: 150px; height: 150px; text-align: center; cursor: pointer;\">FORM &lt;div style=\"background-color: blue; position: absolute; top: 25px; left: 25px; width: 100px; height: 100px;\">DIV &lt;p style=\"background-color: red; position: absolute; top: 25px; left: 25px; width: 50px; height: 50px; line-height: 50px; margin: 0;\">P&lt;/p> &lt;/div> &lt;/form> &lt;script> form=document.querySelector('form'); form.onclick = function(event) &#123; // Output the event target and the current element // (this=the current element=form, event.target=the clicked element) alert(\"target = \" + event.target.tagName + \", this=\" + this.tagName); &#125;; &lt;/script> &lt;/body> &lt;/html> ## Stopping Bubbling Bubbling events ascend from the target element. Usually, they rise all the way up to `&lt;html>`, then to the document object, and some events even reach the window, calling all handlers on their way. However, any handler can decide that the event has been fully processed and stop the bubbling. The method used to stop bubbling is `event.stopPropagation()`. For example, if you click `&lt;button>`, the `body.onclick` here won't work: ```html &lt;body onclick=\"alert(`the bubbling doesn't reach here`)\"> &lt;button onclick=\"event.stopPropagation()\">Click me&lt;/button> &lt;/body> Click me CapturingAnother phase of event handling is called “capturing.” The DOM event standard describes the three phases of event propagation: Capturing phase — The event travels down the hierarchy of elements (from the window). Target phase — The event reaches the target element. Bubbling phase — The event starts to bubble up from the element. Below is an illustration of clicking on &lt;td&gt; in a table, taken from the specification: In other words: Clicking on &lt;td&gt;, the event first descends through the ancestor chain to the element (capturing phase), then reaches the target (target phase), and finally ascends (bubbling phase), calling handlers along the way. So far, we have only discussed bubbling because the capturing phase is rarely used. Typically, we don’t see it. Handlers added with on&lt;event&gt; attributes or using HTML attributes or with addEventListener(event, handler) with two arguments have no knowledge of capturing; they run only in the second and third phases. SummaryWhen an event occurs — the element nested deepest where the event happened is marked as the “target element” (event.target). Then, the event moves down from the document root to the event.target, invoking handlers assigned with addEventListener(..., true) (where true is a shorthand for &#123;capture: true&#125;). Next, the handler on the target element itself is called. Finally, the event bubbles up from event.target to the root, calling handlers assigned with on&lt;event&gt;, HTML attributes, and addEventListener without a third parameter or with false&#x2F;&#123;capture: false&#125;. Each handler has access to properties of the event object: event.target — The element deepest in the hierarchy that triggered the event. event.currentTarget (&#x3D;this) — The current element handling the event (the one with the handler). event.eventPhase — The current phase (capturing&#x3D;1, target&#x3D;2, bubbling&#x3D;3). Any event handler can stop the event by calling event.stopPropagation(), but it’s not recommended because we might not be sure if we genuinely don’t need the event to bubble up for other purposes. The capturing phase is rarely used, and usually, events are handled during the bubbling phase. There’s a reason for this logic. Event handlers work the same way. Code that sets a handler on a specific element gains the most detailed information about that element. A handler specific to a &lt;td&gt; might be perfectly tailored for that &lt;td&gt;, knowing all about it. So, it should get the chance first. Then, its immediate parent also knows context-specific details but fewer, and so on, until handling general concepts and running the last handler on the topmost element.","link":"/2023/12/05/en/Event-Bubbling/"},{"title":"Browser Event Delegation","text":"PrefaceCapturing and bubbling allow us to implement one of the most powerful event handling patterns, namely the Event Delegation pattern. The idea is that if we have many elements being handled in a similar way, we don’t need to assign a handler for each element — instead, we place a single handler on their common ancestor. In the handler, we access event.target to see where the event actually occurred and handle it accordingly. Introduction to Event Delegation&lt;!DOCTYPE HTML> &lt;html> &lt;body> &lt;link type=\"text/css\" rel=\"stylesheet\" href=\"bagua.css\"> &lt;table id=\"bagua-table\"> &lt;tr> &lt;th colspan=\"3\">&lt;em>Bagua&lt;/em> Chart: Direction, Element, Color, Meaning&lt;/th> &lt;/tr> &lt;tr> &lt;td class=\"nw\">&lt;strong>Northwest&lt;/strong> &lt;br>Metal &lt;br>Silver &lt;br>Elders &lt;/td> &lt;td class=\"n\">&lt;strong>North&lt;/strong> &lt;br>Water &lt;br>Blue &lt;br>Change &lt;/td> &lt;td class=\"ne\">&lt;strong>Northeast&lt;/strong> &lt;br>Earth &lt;br>Yellow &lt;br>Direction &lt;/td> &lt;/tr> &lt;tr> &lt;td class=\"w\">&lt;strong>West&lt;/strong> &lt;br>Metal &lt;br>Gold &lt;br>Youth &lt;/td> &lt;td class=\"c\">&lt;strong>Center&lt;/strong> &lt;br>All &lt;br>Purple &lt;br>Harmony &lt;/td> &lt;td class=\"e\">&lt;strong>East&lt;/strong> &lt;br>Wood &lt;br>Blue &lt;br>Future &lt;/td> &lt;/tr> &lt;tr> &lt;td class=\"sw\">&lt;strong>Southwest&lt;/strong> &lt;br>Earth &lt;br>Brown &lt;br>Tranquility &lt;/td> &lt;td class=\"s\">&lt;strong>South&lt;/strong> &lt;br>Fire &lt;br>Orange &lt;br>Fame &lt;/td> &lt;td class=\"se\">&lt;strong>Southeast&lt;/strong> &lt;br>Wood &lt;br>Green &lt;br>Romance &lt;/td> &lt;/tr> &lt;/table> &lt;script> let table = document.getElementById('bagua-table'); let selectedTd; table.onclick = function(event) &#123; let target = event.target; while (target != this) &#123; if (target.tagName == 'TD') &#123; highlight(target); return; &#125; target = target.parentNode; &#125; &#125; function highlight(node) &#123; if (selectedTd) &#123; selectedTd.classList.remove('highlight'); &#125; selectedTd = node; selectedTd.classList.add('highlight'); &#125; &lt;/script> &lt;style> #bagua-table th &#123; text-align: center; font-weight: bold; &#125; #bagua-table td &#123; width: 150px; white-space: nowrap; text-align: center; vertical-align: bottom; padding-top: 5px; padding-bottom: 12px; &#125; #bagua-table .nw &#123; background: #999; &#125; #bagua-table .n &#123; background: #03f; color: #fff; &#125; #bagua-table .ne &#123; background: #ff6; &#125; #bagua-table .w &#123; background: #ff0; &#125; #bagua-table .c &#123; background: #60c; color: #fff; &#125; #bagua-table .e &#123; background: #09f; color: #fff; &#125; #bagua-table .sw &#123; background: #963; color: #fff; &#125; #bagua-table .s &#123; background: #f60; color: #fff; &#125; #bagua-table .se &#123; background: #0c3; color: #fff; &#125; #bagua-table .highlight &#123; background: red; &#125; &lt;/style> &lt;/body> &lt;/html> Dynamic Event Handling with Event DelegationThis table has 9 cells, but it could have 99 or 9999 cells, and it wouldn’t matter. Our task is to highlight the clicked &lt;td&gt; when the user clicks on it. Instead of assigning an onclick handler to each &lt;td&gt; (potentially many), we can set a “catch-all” handler on the &lt;table&gt; element. It uses event.target to determine the clicked element and highlights it. The JavaScript code is as follows: let selectedTd; table.onclick = function(event) &#123; let target = event.target; // Where was the click? if (target.tagName != 'TD') return; // Not on TD? We don't care then highlight(target); // Highlight it &#125;; function highlight(td) &#123; if (selectedTd) &#123; selectedTd.classList.remove('highlight'); &#125; selectedTd = td; selectedTd.classList.add('highlight'); // Highlight the new td &#125; This code doesn’t care how many cells are in the table. We can dynamically add&#x2F;remove &lt;td&gt;, and highlighting still works. However, there is a flaw. Clicks might not happen on the &lt;td&gt; itself but inside it. In our example, if we inspect the HTML internals, we can see nested tags within &lt;td&gt;, such as &lt;strong&gt;: In the table.onclick handler, we should accept such event.target and determine if the click occurred inside a &lt;td&gt;. Here is the improved code: table.onclick = function(event) &#123; let td = event.target.closest('td'); // (1) if (!td) return; // (2) if (!table.contains(td)) return; // (3) highlight(td); // (4) &#125;; Explanation: elem.closest(selector) method returns the closest ancestor that matches the selector. In our case, we look for &lt;td&gt; starting from the target element. If event.target is not within any &lt;td&gt;, the call returns immediately, as there’s nothing to do here. For nested tables, event.target might be a &lt;td&gt; but outside the current table. So, we check if it’s a &lt;td&gt; within our table. If yes, we highlight it. Delegation Example: Actions in MarkupEvent delegation has other uses, like handling actions in markup. For instance, let’s say we want to create a menu with buttons for “Save,” “Load,” and “Search.” There’s an object with methods like save, load, and search. How do we match them? The initial idea might be to assign a separate handler for each button. But there’s a more elegant solution. We can add a handler to the entire menu and add a data-action attribute to buttons with method calls: &lt;button data-action=\"save\">Click to Save&lt;/button> The handler reads the attribute and executes the corresponding method. The working example is as follows: &lt;div id=\"menu\"> &lt;button data-action=\"save\">Save&lt;/button> &lt;button data-action=\"load\">Load&lt;/button> &lt;button data-action=\"search\">Search&lt;/button> &lt;/div> &lt;script> class Menu &#123; constructor(elem) &#123; this._elem = elem; elem.onclick = this.onClick.bind(this); // (*) &#125; save() &#123; alert('saving'); &#125; load() &#123; alert('loading'); &#125; search() &#123; alert('searching'); &#125; onClick(event) &#123; let action = event.target.dataset.action; if (action) &#123; this[action](); &#125; &#125;; &#125; new Menu(menu); &lt;/script> &lt;body> &lt;div id=\"menu\"> &lt;button data-action=\"save\">Save&lt;/button> &lt;button data-action=\"load\">Load&lt;/button> &lt;button data-action=\"search\">Search&lt;/button> &lt;/div> &lt;script> class Menu &#123; constructor(elem) &#123; this._elem = elem; elem.onclick = this.onClick.bind(this); // (*) &#125; save() &#123; alert('saving'); &#125; load() &#123; alert('loading'); &#125; search() &#123; alert('searching'); &#125; onClick(event) &#123; let action = event.target.dataset.action; if (action) &#123; this[action](); &#125; &#125;; &#125; new Menu(menu); &lt;/script> &lt;/body> Please note that this.onClick is bound to this in (*) line. This is crucial because otherwise, the inner this would refer to the DOM element (elem) instead of the Menu object, and this[action] wouldn’t be what we need. So, what benefits does delegation bring us here? We don’t need to write code to assign a handler for each button. Just create a method and place it in the markup. The HTML structure is very flexible, and we can add&#x2F;remove buttons at any time. We could also use classes like .action-save, .action-load, but the data-action attribute is more semantically meaningful. We can also use it in CSS rules. “Behavior” PatternWe can also use event delegation to add “behavior” in a declarative way to elements with specific attributes and classes. The behavior pattern consists of two parts: We add custom attributes to elements describing their behavior. A document-wide handler tracks events and executes the behavior if the event occurs on an element with a specific attribute. Behavior: CounterFor example, the data-counter attribute here adds a “click to increase” behavior to buttons. &lt;script> document.addEventListener('click', function(event) &#123; if (event.target.dataset.counter != undefined) &#123; // If this attribute exists... event.target.value++; &#125; &#125;); &lt;/script> Counter: One more counter: document.addEventListener('click', function(event) { if (event.target.dataset.counter != undefined) { // If this attribute exists... event.target.value++; } }); SummaryEvent delegation is really cool! It’s one of the most useful patterns for DOM events. It’s commonly used to add the same handling for many similar elements, but not limited to that. Algorithm Place a handler on the container. In the handler — check the source element event.target. If the event happens within an element of interest, handle it. Benefits Simplifies initialization and saves memory: No need to add many handlers. Less code: When adding or removing elements, no need to add&#x2F;remove handlers. DOM modifications: We can use innerHTML, etc., to add&#x2F;remove elements in batches. Event Delegation LimitationsFirstly, events must bubble. Some events do not bubble. Also, low-level handlers should not use event.stopPropagation().Secondly, delegation might increase CPU load as the container-level handler responds to events anywhere in the container, regardless of whether we are interested in that event. However, the load is usually negligible, so we don’t consider it.","link":"/2023/12/05/en/Event-Delegation/"},{"title":"ModernWeb - A Simple Record on Chrome Local Code Debugging","text":"IntroductionSince attending a frontend conference last week, I have gained some new insights into new technologies in frontend development. One of them is about Chrome local code debugging. So, I want to make a simple record here. First, let me share a link: youtube-ChromeDevs I would like to express my gratitude to the Chrome DevTools team for their excellent work. However, before the frontend conference by jecfish, I was not aware of so many friendly web debugging tools and performance testing methods. So, I will make a simple record here. If you are interested, you can check out the link above, which contains a lot of useful information. Feel free to show your support to their team. Below is the outline of this chapter, which will be further elaborated. Local Code DebuggingThis section will cover local code debugging, mainly focusing on the topic of source maps. I will provide some brief introductions and examples. The Need for Source Code Mapping Here is a link: source-map This link contains some content about source maps written by jecfish. If you are interested, you can check it out. If the article is too long for you, you can directly read the content below. Today, we are going to discuss source code mapping, an important tool in modern web development that greatly simplifies debugging work. In this article, we will explore the basics of source code mapping, how source code mapping is generated, and how it improves the debugging experience. Before diving into the study of source maps, we need to have a basic understanding of Modern Web. So, what should we talk about when I mention Modern Web? We can see that the console prints some content related to BaseOn CSS styles. Although we are not going to focus on the CSS styles of the console today, you can still define some styles to make your console output more beautiful! After 15 years, we can see that the debugging tools in the Chrome DevTools have evolved from a few options to a rich and diverse set of tools. The changes have been significant. So, today, let’s talk about the specific applications of these 32 debugging tools in Chrome. We know that browsers only understand a few languages: HTML &lt;&#x2F;&gt; CSS {;} JavaScript (,) Wasm (of course, modern browser engines now have built-in support for Wasm modules by default). However, our frontend frameworks are extremely rich, involving languages such as TypeScript, Less, Sass, and frameworks such as Vue, React, and meta frameworks such as Nust.js, Next.js. These frameworks are used in our frontend code, but browsers do not understand these languages. Therefore, we need to convert these languages into languages that browsers can understand. We are building more complex web applications, and your development workflow may involve the use of various tools. For example: Template languages and HTML preprocessors: Pug, Nunjucks, Markdown. CSS preprocessors: SCSS, LESS, PostCSS. JavaScript frameworks: Angular, React, Vue, Svelte. JavaScript meta frameworks: Next.js, Nuxt, Astro. Advanced programming languages: TypeScript, Dart, CoffeeScript.And more. The list keeps growing! These tools require a build process to transpile the code into standard HTML, JavaScript, and CSS that browsers can understand. Additionally, for performance optimization, it is common practice to minify (e.g., using Terser to reduce and obfuscate JavaScript) and concatenate these files to reduce their size and improve web efficiency. During the process of converting these various template languages, preprocessors, and meta frameworks into HTML, JavaScript, and CSS that browsers can understand, there is a compilation process that generates some intermediate code. This intermediate code is our source code mapping, which is what we are going to talk about today. For example, using a build tool, we can transpile and minify the following TypeScript file into a single line of JavaScript. You can find this demo on GitHub: parcel-demo /* A TypeScript demo: example.ts */ document.querySelector('button')?.addEventListener('click', () => &#123; const num: number = Math.floor(Math.random() * 101); const greet: string = 'Hello'; (document.querySelector('p') as HTMLParagraphElement).innerText = `$&#123;greet&#125;, you are no. $&#123;num&#125;!`; console.log(num); &#125;); Certainly! Below is the English translation of the provided content: /* A compressed JavaScript version of the TypeScript demo: example.min.js */ document.querySelector(\"button\")?.addEventListener(\"click\", (() => &#123; const e = Math.floor(101 * Math.random()); document.querySelector(\"p\").innerText = `Hello, you are no. $&#123;e&#125;!`; console.log(e); &#125;)); However, this optimization increases the difficulty of debugging. If compressed code puts everything in a single line and uses short variable names, it becomes challenging to trace the root of the problem. This is where source maps come into play—they map the compiled code back to the original code. Now let’s look at a specific example. This example is a simple click event triggering an XHR written in TypeScript, where a 404 error is reported. Oh? What’s happening here? Pay attention to the red arrow; it seems like our browser understands TypeScript code? Actually, it’s not the case. Looking at this image, it appears that TypeScript is parsed from main.js. Understanding Source Code Mapping (source-map) These source map files contain basic information about how the compiled code maps back to the original code, allowing developers to easily debug. Here’s an example of a source map: &#123; \"mappings\": \"AAAAA,SAASC,cAAc,WAAWC, ...\", \"sources\": [\"src/script.ts\"], \"sourcesContent\": [\"document.querySelector('button')...\"], \"names\": [\"document\",\"querySelector\", ...], \"version\": 3, \"file\": \"example.min.js.map\" &#125; To understand each field, you can read the Source Map Specification or this classic article on Anatomy of a Source Map. The most crucial aspect of source maps is the mappings field. It uses VLQ base 64-encoded strings to map lines and positions in the compiled file to the corresponding original file. Source map visualization tools like source-map-visualization and Source Map Visualization can intuitively display this mapping. The left column represents the generated code, while the original column shows the original source. Visualization tools color code each line in the original column and the corresponding code in the generated column. The mapping section shows the decoded code mappings. For example, the entry 65 -&gt; 2:2 means: Generated code: The word const in the compressed content starts at position 65. Original code: The word const starts at line 2, column 2 in the original content. This way, developers can quickly identify the relationship between the minified code and the original code, making the debugging process smoother. Browser developer tools apply these source code mappings, helping you pinpoint debugging issues directly in the browser. How DevTools Know What to Hide? Source Maps Practical Chrome Debugging TipsRequests You can view detailed information about requests in the Network panel, including request headers, response headers, request body, response body, Cookies, Timing, and more. Additionally, DevTools overrides allow you to simulate remote resources by overriding HTTP response headers and web content (including XHR and fetch requests) through local overrides. This enables you to prototype changes without waiting for backend support. Local overrides also let you retain changes made in DevTools during page load. This is particularly useful in situations where frontend requests to the backend return results that haven’t undergone cross-origin handling (cross-origin handling is typically done on the backend). In such cases, even though the frontend receives correct data, the browser may mark the file as untrusted due to security policies. Local overrides allow you to simulate the backend’s response, facilitating frontend debugging. Or, if some data on the backend hasn’t been modified yet, and the frontend receives outdated data, do we have to wait for backend engineers to fix the data before we can work on it? That seems a bit inefficient. Instead, we can use content rewriting through local overrides to simulate the modified data, allowing frontend debugging. DevTools&#x2F;overrides indeed is powerful. How does it work? When you make changes in DevTools, DevTools saves a copy of the modified file to a folder you specify. When you reload the page, DevTools provides the locally modified file instead of the network resource. Overriding Web ContentSet Up a Folder Set up local overrides. Make changes to files and save them in DevTools. For example, you can edit files in “Sources” or edit CSS in “Elements” &gt; “Styles” unless the CSS is in an HTML file. DevTools saves the modified files, listing them in Sources &gt; Overrides, and displays them in related panels and panes, indicated by icons next to overridden files in Elements &gt; Styles, Network, and Sources &gt; Overrides. Override XHR or Fetch Requests to Simulate Remote ResourcesWith local overrides, you don’t need access to the backend, and you don’t have to wait for it to support your changes. Simulate and experiment instantly: Set up local overrides. In Network, filter XHR&#x2F;fetch requests, find the desired request, right-click it, and choose “Override content.” Make changes to the fetched data and save the file. Refresh. Reload the page and observe the applied changes. To understand this workflow better, watch the video here. Override HTTP Response HeadersIn the “Network” panel, you can override HTTP response headers without accessing the web server. With response header overrides, you can prototype fixes for various headers, including but not limited to: Cross-Origin Resource Sharing (CORS) headers Permissions-Policy headers Cross-Origin Isolation headers To override response headers: Set up local overrides and check. Go to Network, find the request, right-click it, and choose “Override headers.” DevTools will guide you to the **Headers Response Headers Editor**. Recorder, Beneficial for Debugging and TestingCustomize and automate user flows based on the Chrome DevTools Recorder for enhanced debugging and testing. Writing automated tests might not be the most exciting part of a developer’s life. As developers, our focus is on functionality, fixing bugs, and improving the world! However, having automated tests in our workflow is crucial in the long run. So, we also recognize the importance of writing automated tests. With the Chrome DevTools Recorder panel, you can record and replay user flows. You can export these flows in various formats (such as test scripts) using different third-party extensions and libraries. You can also customize user flows using the Puppeteer Replay library and integrate them into your existing workflow. In this blog post, we’ll discuss: How to programmatically export and replay user flows. How to customize user flows with Puppeteer Replay. How to integrate with your CI&#x2F;CD workflow. Watch the video for a practical demonstration. Programmatically Exporting User Flows and ReplayingBy default, the Recorder allows you to export recordings as Puppeteer or Puppeteer Replay scripts or as pure JSON files. Replaying with Puppeteer ReplayAfter exporting the user flow as a JSON file, you have the option to import it back into the Recorder panel and replay it or use external libraries for replay. One such available library is Puppeteer Replay. Puppeteer Replay is a library that helps you replay user flows. It’s based on Puppeteer and allows you to replay user flows in the browser without writing any code. You can use Puppeteer Replay to replay your user flows to ensure that your application performs well in different environments. Integration with CI&#x2F;CD PipelineThere are various ways to achieve this, and several tools can be used. Here’s an example of automating this process using GitHub Actions: # .github/node.js.yml name: Replay recordings on: push: branches: [ \"main\" ] schedule: - cron: '30 12 * * *' # daily 12:30pm jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Use Node.js uses: actions/setup-node@v3 with: node-version: 18.x cache: 'npm' - run: npm install puppeteer - run: npm run replay-all - run: npm run start In this example, we replay the user flow under the following conditions: New changes are pushed to the main branch. Daily at 12:30 PM. In addition to GitHub Actions, you can also integrate this with your preferred cloud provider.","link":"/2023/11/20/en/ModernWeb-Debugging1/"},{"title":"Webpack Performance Optimization-2","text":"Performance Optimization - JS-CSS Code Minification Terser is a toolset for JavaScript parsing, mangling, and compressing. In the early days, we used uglify-js to minify and uglify our JavaScript code. However, it is no longer maintained and does not support ES6+ syntax. Terser is a fork of uglify-es and retains most of its original APIs, compatible with uglify-es and uglify-js@3, etc. webpack-terser JavaScript Code MinificationWebpack provides the terser-webpack-plugin plugin for code optimization and minification. In production mode, TerserPlugin is used by default for code processing. const TerserPlugin = require('terser-webpack-plugin'); module.exports = &#123; // Configure other Webpack options... optimization: &#123; minimizer: [new TerserPlugin()], &#125;, &#125;; CSS Code MinificationApart from JavaScript code, CSS code can also be minified using Webpack. Use css-minimizer-webpack-plugin to compress CSS code. const CssMinimizerPlugin = require('css-minimizer-webpack-plugin'); module.exports = &#123; // Configure other Webpack options... optimization: &#123; minimizer: [ new CssMinimizerPlugin(), // You can continue adding other compression plugins... ], &#125;, &#125;; Tree Shaking Implementation in WebpackTree shaking is a term commonly used to describe the removal of dead code in JavaScript context. Tree Shaking in WebpackIn modern front-end development, optimizing code size is a crucial topic. Tree shaking is an optimization technique used to eliminate unused JavaScript modules in a project, reducing the size of the bundled files. Webpack provides built-in support, making it easy to implement tree shaking in projects. Enable ES Module SyntaxFirst, ensure your JavaScript code follows ES module syntax, as Webpack’s tree shaking feature only works with ES modules. Use import and export syntax to define modules in your project. // math.js export function square(x) &#123; return x * x; &#125; export function cube(x) &#123; return x * x * x; &#125; Webpack ConfigurationIn the Webpack configuration file, ensure the following settings to enable tree shaking: Set mode to &#39;production&#39;. Webpack will automatically enable related optimizations, including tree shaking. Implementing Tree Shaking for JavaScriptWebpack implements tree shaking using two different approaches: usedExports: Marks certain functions as used, and later optimizes them with Terser. sideEffects: Skips entire modules&#x2F;files and checks if the file has side effects. Using usedExports to Implement Tree ShakingSet the mode to production: module.exports = &#123; mode: 'production', // ...other configurations &#125;; Configure usedExports in the optimization section: const path = require('path'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: path.resolve(__dirname, 'dist'), &#125;, mode: 'development', optimization: &#123; usedExports: true, &#125;, &#125;; Using sideEffects to Implement Tree ShakingSet the sideEffects field in package.json: Set it to false to inform Webpack that it can safely remove unused exports. If there are specific files you want to keep, set it as an array. &#123; \"name\": \"your-project\", \"sideEffects\": [\"./src/some-side-effectful-file.js\"] &#125; Webpack Side Effects Understanding Tree Shaking and sideEffectssideEffects and usedExports (more commonly considered tree shaking) are two different optimization techniques. sideEffects is more efficient as it allows skipping entire modules&#x2F;files and their entire subtree. usedExports depends on terser to detect side effects in statements. It’s a more complex JavaScript task and is not as straightforward as sideEffects. Also, it cannot skip subtrees&#x2F;dependencies because side effects need to be evaluated. While exported functions work as usual, higher-order functions (HOC) in the React framework can have issues in this scenario. CSS Tree Shaking ImplementationFor CSS tree shaking, additional plugins are required. In the past, PurifyCss plugin was used for CSS tree shaking, but it’s no longer maintained (last update was 4 years ago). A different library, PurgeCSS, can now be used for CSS tree shaking, helping remove unused CSS. File Compression in WebpackWhat is HTTP CompressionHTTP compression is a technique used between servers and clients to improve transmission speed and bandwidth utilization.The process of HTTP compression is as follows: Data is compressed on the server before being sent. (Can be done in Webpack) Compatible browsers inform the server about supported compression formats during requests. The server returns the corresponding compressed file to the browser, indicating it in the response headers. Popular Compression FormatsThere are several popular compression formats: compress: Method used by UNIX’s “compress” program (historical reasons, not recommended for most applications, use gzip or deflate instead). deflate: Compression based on the deflate algorithm (defined in RFC 1951) and encapsulated in zlib data format. gzip: GNU zip format (defined in RFC 1952), widely used compression algorithm. br: A new open-source compression algorithm designed specifically for HTTP content encoding. Webpack Configuration for File CompressionWebpack essentially performs the first step of HTTP compression. You can use the CompressionPlugin for this purpose. Step 1: Install CompressionPlugin: npm install compression-webpack-plugin -D Step 2: Use CompressionPlugin in your Webpack configuration: module.exports = &#123; plugins: [ new CompressionPlugin(&#123; test: /\\.js(\\?.*)?$/i, &#125;), ], &#125;; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/Webpack-optimization-2/"},{"title":"Webpack Performance Optimization-1","text":"IntroductionLet’s talk about why optimization is necessary. If your project is small and builds quickly, you might not need to worry too much about performance optimization. However, as your project grows with more pages, features, and business logic, the build time of webpack, the underlying build tool, also increases. At this point, optimizing performance becomes crucial. Webpack offers various avenues for performance optimization, which can be broadly categorized into two areas: Optimization One: Optimizing the built result for production, focusing on performance during deployment (e.g., code splitting, reducing bundle size, using CDN servers, etc.).Optimization Two: Optimizing build speed for development or production build, enhancing the speed of the build process (e.g., using exclusion, cache loaders, etc.).The performance during production directly affects user experience, whereas build time is closely related to developers’ daily workflow. If the local development server or production build takes too long, it significantly hampers productivity. Performance Optimization - Code SplittingCode splitting is a critical feature in webpack: Its primary purpose is to separate code into different bundles, which can be loaded on demand or in parallel.By default, all JavaScript code (business logic, third-party dependencies, and modules not immediately used) is loaded on the initial page load, impacting the loading speed.Code splitting allows creating smaller bundles and controlling resource loading priorities, thereby enhancing code loading performance. Webpack provides three common approaches to code splitting: Entry Points: Manually splitting code using entry configuration Preventing Duplication: Avoiding duplicate code using Entry Dependencies or SplitChunksPlugin Dynamic Imports: Splitting code using inline functions in modules Optimizing Entry Points - Entry DependenciesWhen a project has multiple entry points, there might be issues with duplicate dependencies. Some modules might be referenced in multiple entry points, causing redundancy in the final output, increasing the output file size. module.exports = &#123; entry: &#123; page1: &#123; import: './src/page1.js', dependOn: 'shared', &#125;, page2: &#123; import: './src/page2.js', dependOn: 'shared', &#125;, shared: './src/shared.js', &#125;, output: &#123; filename: '[name].bundle.js', path: __dirname + '/dist', &#125;, &#125;; Dynamic ImportsDynamic imports are a technique in webpack for lazy loading, allowing modules to load asynchronously at runtime instead of bundling all modules into a large initial file. This approach improves the initial loading speed and reduces the initial bundle size. const path = require('path'); module.exports = &#123; entry: &#123; main: './src/index.js', &#125;, output: &#123; filename: '[name].bundle.js', path: path.resolve(__dirname, 'dist'), publicPath: '/', &#125;, module: &#123; rules: [ // Add your loader rules here ], &#125;, optimization: &#123; splitChunks: &#123; chunks: 'all', &#125;, &#125;, &#125;; In the above configuration, code splitting is achieved using optimization.splitChunks with the option chunks: &#39;all&#39;. Then, dynamic imports can be used in the code like this: // Dynamically import modules where needed const loadModule = () => import('./Module'); loadModule().then(module => &#123; // Use the loaded module &#125;); Webpack will split the modules imported using import() into separate files. These files will be loaded asynchronously when needed during runtime. Custom Bundle Splitting - SplitChunksBundle splitting is an optimization strategy that allows breaking down code into smaller pieces, enabling faster content display during loading. Webpack provides various strategies for bundle splitting, one of which involves using the SplitChunksPlugin plugin. This strategy is known as splitChunks. module.exports = &#123; // ...other configurations optimization: &#123; splitChunks: &#123; chunks: 'all', minSize: 30000, // Minimum size of the module before splitting minChunks: 1, // Minimum number of times a module should be duplicated before splitting maxAsyncRequests: 5, // Maximum number of parallel requests when loading modules on demand maxInitialRequests: 3, // Maximum number of parallel requests at an entry point automaticNameDelimiter: '~', name: true, cacheGroups: &#123; vendors: &#123; test: /[\\\\/]node_modules[\\\\/]/, priority: -10, reuseExistingChunk: true, &#125;, default: &#123; minChunks: 2, priority: -20, reuseExistingChunk: true, &#125;, &#125;, &#125;, &#125;, &#125;; For more information, refer to the webpack-split-chunks-plugin documentation. Performance Optimization - CDNCDN, or Content Delivery Network, refers to a network of interconnected servers strategically placed to deliver content to users efficiently. It ensures faster and more reliable delivery of resources such as music, images, videos, applications, and other files by utilizing servers closest to each user, providing high performance, scalability, and low-cost content delivery. In development, CDN is typically used in two ways: All static resources are bundled and stored on a CDN server, and users load resources exclusively through the CDN. Some third-party resources are hosted on CDN servers. Utilizing a Content Delivery Network (CDN) is a highly effective performance optimization strategy, especially within Webpack. CDN accelerates website loading speed, reduces server load, and enhances user experience. Here’s how you can configure and use CDN in Webpack: Using CDN for Third-Party LibrariesIntegrate third-party libraries used in your project (such as React, Vue, jQuery, etc.) through CDN links directly in the HTML file: &lt;script src=\"https://cdn.jsdelivr.net/npm/react@version/dist/react.min.js\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/react-dom@version/dist/react-dom.min.js\">&lt;/script> Configuring Externals in WebpackIn your Webpack configuration, utilize the externals field to inform Webpack about externally referenced modules that shouldn’t be bundled: module.exports = &#123; // ...other configurations externals: &#123; react: 'React', 'react-dom': 'ReactDOM', &#125;, &#125;; Then, include the CDN links in the HTML file using script tags: &lt;script src=\"https://cdn.jsdelivr.net/npm/react@version/dist/react.min.js\">&lt;/script> &lt;script src=\"https://cdn.jsdelivr.net/npm/react-dom@version/dist/react-dom.min.js\">&lt;/script> Configuring CDN’s publicPathIn the Webpack output field, set the publicPath to specify the URL prefix for resource imports, typically set to the CDN’s address: module.exports = &#123; // ...other configurations output: &#123; // ...other output configurations publicPath: 'https://cdn.example.com/', &#125;, &#125;; This way, during Webpack build, all resource paths will be prefixed with the CDN’s address. Performance Optimization - Extracting CSS FilesExtracting CSS files from JavaScript bundles is a common performance optimization strategy. This approach reduces the size of JavaScript files, speeds up page loading, and allows browsers to download CSS and JavaScript files in parallel, enhancing loading performance. In Webpack, you can achieve this using the mini-css-extract-plugin plugin. Webpack ConfigurationIn your Webpack configuration file, include the mini-css-extract-plugin plugin and configure the module.rules to handle CSS files: const MiniCssExtractPlugin = require('mini-css-extract-plugin'); module.exports = &#123; // ...other configurations module: &#123; rules: [ &#123; test: /\\.css$/, use: [ MiniCssExtractPlugin.loader, 'css-loader', // Additional CSS loaders like postcss-loader and sass-loader can be added here ], &#125;, ], &#125;, plugins: [ new MiniCssExtractPlugin(&#123; filename: 'styles.css', // Filename for the extracted CSS file &#125;), ], &#125;; Including CSS FilesIn your JavaScript files or entry point file, import the CSS file: import './styles.css'; Alternatively, in the HTML file, use the link tag to include the extracted CSS file: &lt;link rel=\"stylesheet\" href=\"styles.css\"> Performance Optimization - Bundling File Naming (Hash, ContentHash, ChunkHash)In Webpack, how files are named during bundling is a crucial performance optimization strategy. Proper naming ensures that browsers can cache files correctly, avoiding unnecessary network requests and improving application loading speed. Here are three common bundling file naming techniques: Hash, ContentHash, and ChunkHash. HashHash is generated based on file content. When file content changes, its corresponding hash value also changes. In Webpack, you can use the [hash] placeholder to represent the hash value. output: &#123; filename: 'bundle.[hash].js', &#125; ContentHashContentHash is generated based on file content as well, but unlike Hash, it’s solely influenced by file content and remains unaffected by file name or path changes. In Webpack, you can use the [contenthash] placeholder to represent the ContentHash value. output: &#123; filename: 'bundle.[contenthash].js', &#125; ChunkHashChunkHash is generated based on module content. Different module contents result in different ChunkHash values. In Webpack, you can use the [chunkhash] placeholder to represent the ChunkHash value. output: &#123; filename: '[name].[chunkhash].js', &#125; Performance Optimization - Implementing Tree Shaking in WebpackJavaScript Tree Shaking: Tree Shaking in JavaScript originates from the rollup bundler, a build tool. It relies on the static syntax analysis of ES Modules (no code execution) to determine module dependencies. Webpack 2 introduced native support for ES2015 modules, enhancing tree shaking capabilities. Webpack 4 extended this ability and introduced the sideEffects property in package.json to indicate which files have no side effects, allowing webpack to safely remove unused code. In Webpack 5, partial CommonJS tree shaking support was introduced. CommonJS Tree Shaking Implementing Tree Shaking in JavaScriptWebpack implements Tree Shaking through two methods: usedExports: Marking certain functions as used and optimizing them using Terser. sideEffects: Skipping entire modules&#x2F;files and checking if they have side effects. Tree Shaking for CSSTree Shaking for CSS involves using additional plugins. While PurifyCss was used previously, it’s no longer maintained. An alternative is PurgeCSS, a tool for removing unused CSS. Note: This translation includes placeholder strings like [hash], [contenthash], and [chunkhash] to represent dynamic values. Please replace these placeholders with appropriate values based on your specific use case. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/26/en/Webpack-optimization-1/"},{"title":"How to Use `sideEffects` in Webpack","text":"Webpack v4 introduced a new feature called sideEffects, which allows you to declare in your package.json whether a package&#x2F;module contains side effects or not. This declaration provides more optimization space for tree-shaking. In the conventional understanding of side effects, if we are certain that the modules within our package have no side effects, we can mark the package in npm with &quot;sideEffects&quot;: false in package.json. This allows us to offer a better bundling experience for consumers. The principle behind this is that Webpack can transform imports like import &#123;a&#125; from xx into import &#123;a&#125; from &#39;xx/a&#39; for packages marked as side-effects-free, automatically trimming unnecessary imports, similar to babel-plugin-import. Tree Shaking and Side EffectsTree shaking, first introduced and implemented by Rollup in the frontend community, has been a topic of discussion in various articles about optimizing bundling. Principles of Tree ShakingES6 module imports are statically analyzable, meaning the compiler can accurately determine what code is loaded during compilation. The program flow is analyzed to identify unused or unreferenced variables, which are then removed from the code. The principle sounds perfect, so why do we sometimes find that unnecessary code in our projects isn’t eliminated? The reason is side effects. Side EffectsFor those familiar with functional programming, the term “side effect” is not unfamiliar. It can be broadly understood as any action of a function that might or might not affect variables outside its scope. For example, consider this function: function go(url) &#123; window.location.href = url; &#125; This function modifies the global variable location and even triggers a browser redirect, making it a function with side effects. // components.js export class Person &#123; constructor(&#123; name &#125;) &#123; this.className = 'Person'; this.name = name; &#125; getName() &#123; return this.name; &#125; &#125; export class Apple &#123; constructor(&#123; model &#125;) &#123; this.className = 'Apple'; this.model = model; &#125; getModel() &#123; return this.model; &#125; &#125; // main.js import &#123; Apple &#125; from './components'; const appleModel = new Apple(&#123; model: 'IphoneX' &#125;).getModel(); console.log(appleModel); In this code, the Person class is clearly unused. However, why can other tools like Rollup successfully eliminate unused code, while Webpack cannot? The answer lies in Babel compilation + Webpack bundling. I’ll provide a link here that explains in detail how Babel compilation + Webpack bundling might prevent effective code elimination: Your Tree-Shaking Isn’t Working. If you don’t want to read the article, here’s a brief explanation: Babel compilation wraps the Person class in an IIFE (Immediately Invoked Function Expression) and returns a constructor, introducing a side effect. There’s an issue related to this: Class declarations inside IIFEs are considered side effects. When I declare a class inside an IIFE and don’t use the class, UglifyJS doesn’t remove it because it’s considered a side effect. var V6Engine = (function () &#123; function V6Engine() &#123; &#125; V6Engine.prototype.toString = function () &#123; return 'V6'; &#125;; return V6Engine; &#125;()); During compilation, you might receive this warning: WARN: Side effects in initialization of unused variable V6Engine [./dist/car.bundle.js:74,4]. The reason is that UglifyJS doesn’t perform complete program flow analysis. It doesn’t remove code because you noticed a side effect. If you want a more sophisticated tree shaking, go check out Rollup! Summarizing some key points from the issue: If a function’s parameter is a reference type, any operations on its properties could potentially have side effects. This is because it’s a reference type, and any modification to its properties affects data outside the function. Additionally, accessing or modifying its properties triggers getter or setter, which are opaque and may have side effects. UglifyJS lacks complete program flow analysis. It can simple judge whether a variable is later referenced or modified but cannot determine the complete modification process of a variable. It doesn’t know if it points to an external variable, so many potentially side-effect-causing code cannot be removed. Rollup has the ability to perform program flow analysis, making it better at determining whether code truly has side effects. However, these issues were prevalent in older versions. The current Webpack tree shaking has undergone many optimizations and can perform sufficient program flow analysis for tree shaking. The purpose of Webpack’s tree shaking is to mark unused exported members as unused and not export them in the modules where they are re-exported. It sounds complicated, but looking at the code makes it clearer: // a.js export function a() &#123;&#125; // b.js export function b()&#123;&#125; // package/index.js import a from './a' import b from './b' export &#123; a, b &#125; // app.js import &#123;a&#125; from 'package' console.log(a) When using app.js as the entry point, the code after tree shaking becomes: // a.js export function a() &#123;&#125; // b.js is no longer exported: function b()&#123;&#125; function b() &#123;&#125; // package/index.js does not export module b anymore import a from './a' import './b' export &#123; a &#125; // app.js import &#123;a&#125; from 'package' console.log(a) After combining Webpack’s scope hoisting and uglify, all traces of module b will be completely eliminated. But what if module b contains some side effects, such as a simple log: // b.js export function b(v) &#123; return v &#125; console.log(b(1)) After webpack, the content of module `b` becomes: // b.js console.log(function (v)&#123;return v&#125;(1)) Although the export of module b is ignored, the code with side effects is retained. Due to various strange operations introduced by the transformer after compilation, which may cause side effects, we often find that even with tree shaking, our bundle size doesn’t significantly decrease. Usually, we expect that if module b is not being used, none of its code should be included. This is where the role of sideEffects becomes apparent: if the imported package&#x2F;module is marked as &quot;sideEffects: false&quot;, regardless of whether it truly has side effects, as long as it’s not being referenced, the entire module&#x2F;package will be completely removed. Taking mobx-react-devtools as an example, we often use it like this: import DevTools from 'mobx-react-devtools'; class MyApp extends React.Component &#123; render() &#123; return ( &lt;div> ... &#123; process.env.NODE_ENV === 'production' ? null : &lt;DevTools /> &#125; &lt;/div> ); &#125; &#125; This is a common scenario of importing modules on demand. However, without the sideEffects: false configuration, even if NODE_ENV is set to production, the bundled code will still include the mobx-react-devtools package. Although we haven’t used any of its exported members, mobx-react-devtools will still be imported because it “might” have side effects. But when we add sideEffects: false, tree shaking can safely remove it entirely from the bundle. Use Cases of sideEffectsAs mentioned earlier, it’s often difficult to guarantee whether packages&#x2F;modules published on npm contain side effects (it could be the code’s fault or the transformer’s fault). However, we can usually ensure whether a package&#x2F;module will affect objects outside of it, such as modifying properties on the window object or overwriting native object methods. If we can guarantee this, we can determine whether a package can have &quot;sideEffects: false&quot;. Whether it truly has side effects is not that important for Webpack; it’s acceptable as long as it’s marked. This explains why packages with inherent side effects, like vue, can still have &quot;sideEffects: false&quot; applied. So, in Webpack, &quot;sideEffects: false&quot; doesn’t mean that the module truly has no side effects. It’s just a way to tell Webpack during tree shaking: “I designed this package with the expectation that it has no side effects, even if it ends up having side effects after being bundled.” Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/Webpack-optimization-3/"},{"title":"Webpack HMR Principle Analysis","text":"PrefaceHot Module Replacement (HMR) is a major feature of Webpack. When you modify and save the code, Webpack repackages the code and sends the new module to the browser, which replaces the old module with the new one without refreshing the browser, allowing you to update the application without refreshing the browser. For example, when developing a web page, if you click a button and a pop-up window appears, but the title of the pop-up window is not aligned, you can modify the CSS style and save it. Without refreshing the browser, the title style changes. It feels like directly modifying the element style in Chrome’s developer tools. Hot Module Replacement (HMR)The Hot Module Replacement (HMR) function replaces, adds, or deletes modules during application runtime without reloading the entire page. This significantly speeds up development in the following ways: Preserve application state lost during a full page reload. Only update the changed content to save valuable development time. When CSS&#x2F;JS changes occur in the source code, they are immediately updated in the browser, which is almost equivalent to directly changing the style in the browser devtools. Why do we need HMR?Before the webpack HMR function, there were many live reload tools or libraries, such as live-server. These libraries monitor file changes and notify the browser to refresh the page. So why do we still need HMR? The answer is actually mentioned in the previous text. Live reload tools cannot save the application state (states). When the page is refreshed, the previous state of the application is lost. In the example mentioned earlier, when you click a button to display a pop-up window, the pop-up window disappears when the browser is refreshed. To restore the previous state, you need to click the button again. However, webapck HMR does not refresh the browser, but replaces the module at runtime, ensuring that the application state is not lost and improving development efficiency. In the ancient development process, we may need to manually run commands to package the code and then manually refresh the browser page after packaging. All these repetitive work can be automated through the HMR workflow, allowing more energy to be devoted to business instead of wasting time on repetitive work. HMR is compatible with most front-end frameworks or libraries on the market, such as React Hot Loader, Vue-loader, which can listen to changes in React or Vue components and update the latest components to the browser in real-time. Elm Hot Loader supports the translation and packaging of Elm language code through webpack, and of course, it also implements HMR functionality. HMR Working Principle DiagramWhen I first learned about HMR, I thought it was very magical, and there were always some questions lingering in my mind. Webpack can package different modules into bundle files or several chunk files, but when I develop with webpack HMR, I did not find the webpack packaged files in my dist directory. Where did they go? By looking at the package.json file of webpack-dev-server, we know that it depends on the webpack-dev-middleware library. So what role does webpack-dev-middleware play in the HMR process? During the use of HMR, I know that the browser communicates with webpack-dev-server through websocket, but I did not find new module code in the websocket message. How are the new modules sent to the browser? Why are the new modules not sent to the browser through websocket with the message? After the browser gets the latest module code, how does HMR replace the old module with the new one? How to handle the dependency relationship between modules during the replacement process? During the module hot replacement process, is there any fallback mechanism if the replacement module fails? With these questions in mind, I decided to delve into the webpack source code and find the underlying secrets of HMR. Figure 1: HMR workflow diagram The above figure is a module hot update process diagram for application development using webpack with webpack-dev-server. The red box at the bottom of the figure is the server, and the orange box above is the browser. The green box is the area controlled by the webpack code. The blue box is the area controlled by the webpack-dev-server code. The magenta box is the file system, where file changes occur, and the cyan box is the application itself. The figure shows a cycle from when we modify the code to when the module hot update is completed. The entire process of HMR is marked by Arabic numerals in dark green. In the first step, in webpack’s watch mode, when a file in the file system is modified, webpack detects the file change, recompiles and packages the module according to the configuration file, and saves the packaged code in memory as a simple JavaScript object. The second step is the interface interaction between webpack-dev-server and webpack. In this step, the main interaction is between the dev-server middleware webpack-dev-middleware and webpack. Webpack-dev-middleware calls webpack’s exposed API to monitor code changes and tells webpack to package the code into memory. The third step is the monitoring of file changes by webpack-dev-server. This step is different from the first step, and it does not monitor code changes and repackage them. When we configure devServer.watchContentBase to true in the configuration file, the server will monitor changes in static files in these configured folders, and notify the browser to perform live reload of the corresponding application after the changes. Note that this is a different concept from HMR. The fourth step is also the work of the webpack-dev-server code. In this step, the server establishes a websocket long connection between the browser and the server through sockjs (a dependency of webpack-dev-server), and informs the browser of the status information of various stages of webpack compilation and packaging, including the information of Server listening to static file changes in the third step. The browser performs different operations based on these socket messages. Of course, the most important information transmitted by the server is the hash value of the new module. The subsequent steps perform module hot replacement based on this hash value. The webpack-dev-server/client side cannot request updated code or perform hot module replacement operations, but instead returns these tasks to webpack. The role of webpack/hot/dev-server is to determine whether to refresh the browser or perform module hot updates based on the information passed to it by webpack-dev-server/client and the configuration of dev-server. Of course, if it is only to refresh the browser, there will be no subsequent steps. HotModuleReplacement.runtime is the hub of client HMR. It receives the hash value of the new module passed to it by the previous step, and sends an Ajax request to the server through JsonpMainTemplate.runtime. The server returns a json that contains the hash values of all modules to be updated. After obtaining the update list, the module requests the latest module code again through jsonp. This is steps 7, 8, and 9 in the above figure. The tenth step is the key step that determines the success or failure of HMR. In this step, the HotModulePlugin compares the old and new modules and decides whether to update the module. After deciding to update the module, it checks the dependency relationship between the modules and updates the dependency references between the modules while updating the modules. The last step is to fall back to live reload when HMR fails, that is, to refresh the browser to obtain the latest packaged code. Simple Example of Using HMRIn the previous section, a HMR workflow diagram was presented to briefly explain the process of module hot updates. However, you may still feel confused, and some of the English terms that appear above may be unfamiliar (these English terms represent code repositories or file modules). Don’t worry, in this section, I will use the simplest and purest example to analyze in detail the specific responsibilities of each library in the HMR process through the webpack and webpack-dev-server source code. Here, I will use a simple vue example to demonstrate. Here is a link to the repository github.com&#x2F;ikkkp&#x2F;webpack-vue-demo Before starting this example, let me briefly explain the files in this repository. The files in the repository include: const path = require('path'); const HtmlWebpackPlugin = require('html-webpack-plugin'); const &#123; VueLoaderPlugin &#125; = require('vue-loader'); const webpack = require('webpack'); // 引入 webpack const AutoImport = require('unplugin-auto-import/webpack') const Components = require('unplugin-vue-components/webpack') const &#123; ElementPlusResolver &#125; = require('unplugin-vue-components/resolvers') /** * @description * @version 1.0 * @author Huangzl * @fileName webpack.base.config.js * @date 2023/11/10 11:00:59 */ module.exports = &#123; entry: &#123; main: './src/main', //单页应用开发模式禁用多入口 &#125;, resolveLoader: &#123; modules: [ 'node_modules', path.resolve(__dirname, './src/loader') ] &#125;, output: &#123; filename: '[id].[fullhash].js', // 使用 [fullhash] 替代 [hash]，这是新版本 webpack 的写法 path: path.join(__dirname, 'dist'), publicPath: './' &#125;, module: &#123; rules: [&#123; test: /\\.vue$/, loader: 'vue-loader' &#125;, &#123; test: /\\.css$/, use: [ 'style-loader', &#123; loader: 'css-loader', options: &#123; importLoaders: 1 &#125; &#125;, 'postcss-loader' ] &#125;, &#123; test: /\\.js$/, use: ['babel-loader', &#123; loader: 'company-loader', options: &#123; sign: 'we-doctor@2021', &#125;, &#125;,], exclude: /node_modules/, &#125;, &#123; test: /\\.(ico|png|jpg|gif|svg|eot|woff|woff2|ttf)$/, loader: 'file-loader', options: &#123; name: '[name].[ext]?[hash]' &#125; &#125;, ] &#125;, plugins: [ new HtmlWebpackPlugin(&#123; template: './public/index.html' &#125;), new VueLoaderPlugin(), new webpack.DefinePlugin(&#123; BASE_URL: JSON.stringify('./') // 这里定义了 BASE_URL 为根路径 '/' &#125;), AutoImport(&#123; resolvers: [ElementPlusResolver()], &#125;), Components(&#123; resolvers: [ElementPlusResolver()], &#125;), ], optimization: &#123; splitChunks: &#123; chunks: 'all', // 只处理异步模块 maxSize: 20000000, // 设置最大的chunk大小为2MB &#125;, &#125;, &#125;; It is worth mentioning that HotModuleReplacementPlugin is not configured in the above configuration, because when we set devServer.hot to true and add the following script to package.json: “start”: “webpack-dev-server –hot –open” After adding the –hot configuration item, devServer will tell webpack to automatically introduce the HotModuleReplacementPlugin plugin, without us having to manually introduce it. The above is the content of webpack.base.config.js. We will modify the content of App.vue below: - &lt;div&gt;hello&lt;&#x2F;div&gt; &#x2F;&#x2F; change the hello string to hello world + &lt;div&gt;hello world&lt;&#x2F;div&gt; Step 1: webpack watches the file system and packages it into memory webpack-dev-middleware calls webpack’s api to watch the file system. When the hello.js file changes, webpack recompiles and packages the file, then saves it to memory. // webpack-dev-middleware/lib/Shared.js if(!options.lazy) &#123; var watching = compiler.watch(options.watchOptions, share.handleCompilerCallback); context.watching = watching; &#125; You may wonder why webpack does not directly package files into the output.path directory. Where do the files go? It turns out that webpack packages the bundle.js file into memory. The reason for not generating files is that accessing code in memory is faster than accessing files in the file system, and it also reduces the overhead of writing code to files. All of this is thanks to memory-fs, a dependency of webpack-dev-middleware. Webpack-dev-middleware replaces the original outputFileSystem of webpack with a MemoryFileSystem instance, so the code is output to memory. The relevant source code of webpack-dev-middleware is as follows: // webpack-dev-middleware/lib/Shared.js var isMemoryFs = !compiler.compilers &amp;&amp; compiler.outputFileSystem instanceof MemoryFileSystem; if(isMemoryFs) &#123; fs = compiler.outputFileSystem; &#125; else &#123; fs = compiler.outputFileSystem = new MemoryFileSystem(); &#125; First, check whether the current fileSystem is an instance of MemoryFileSystem. If not, replace the outputFileSystem before the compiler with an instance of MemoryFileSystem. This way, the code of the bundle.js file is saved as a simple JavaScript object in memory. When the browser requests the bundle.js file, devServer directly retrieves the JavaScript object saved above from memory and returns it to the browser. Step 2: devServer notifies the browser that the file has changed In this stage, sockjs is the bridge between the server and the browser. When devServer is started, sockjs establishes a WebSocket long connection between the server and the browser to inform the browser of the various stages of webpack compilation and packaging. The key step is still webpack-dev-server calling the webpack API to listen for the done event of the compile. After the compile is completed, webpack-dev-server sends the hash value of the newly compiled and packaged module to the browser through the _sendStatus method. // webpack-dev-server/lib/Server.js compiler.plugin('done', (stats) => &#123; // stats.hash 是最新打包文件的 hash 值 this._sendStats(this.sockets, stats.toJson(clientStats)); this._stats = stats; &#125;); ... Server.prototype._sendStats = function (sockets, stats, force) &#123; if (!force &amp;&amp; stats &amp;&amp; (!stats.errors || stats.errors.length === 0) &amp;&amp; stats.assets &amp;&amp; stats.assets.every(asset => !asset.emitted) ) &#123; return this.sockWrite(sockets, 'still-ok'); &#125; // 调用 sockWrite 方法将 hash 值通过 websocket 发送到浏览器端 this.sockWrite(sockets, 'hash', stats.hash); if (stats.errors.length > 0) &#123; this.sockWrite(sockets, 'errors', stats.errors); &#125; else if (stats.warnings.length > 0) &#123; this.sockWrite(sockets, 'warnings', stats.warnings); &#125; else &#123; this.sockWrite(sockets, 'ok'); &#125; &#125;; Step 3: webpack-dev-server&#x2F;client responds to server messages You may wonder how the code in bundle.js receives websocket messages since you did not add any code to receive websocket messages in your business code or add a new entry file in the entry property of webpack.config.js. It turns out that webpack-dev-server modifies the entry property in webpack configuration and adds webpack-dev-client code to it. This way, the code in bundle.js will have the code to receive websocket messages. When webpack-dev-server&#x2F;client receives a hash message, it temporarily stores the hash value. When it receives an ok message, it performs a reload operation on the application. The hash message is received before the ok message. In the reload operation, webpack-dev-server&#x2F;client stores the hash value in the currentHash variable. When it receives an ok message, it reloads the App. If module hot updates are configured, it calls webpack&#x2F;hot&#x2F;emitter to send the latest hash value to webpack and then hands over control to the webpack client code. If module hot updates are not configured, it directly calls the location.reload method to refresh the page. Step 4: webpack receives the latest hash value, verifies it, and requests module code In this step, three modules (three files, with the English names corresponding to the file paths) in webpack work together. First, webpack&#x2F;hot&#x2F;dev-server (referred to as dev-server) listens for the webpackHotUpdate message sent by webpack-dev-server&#x2F;client in step 3. It calls the check method in webpack&#x2F;lib&#x2F;HotModuleReplacement.runtime (referred to as HMR runtime) to check for new updates. In the check process, it uses two methods in webpack&#x2F;lib&#x2F;JsonpMainTemplate.runtime (referred to as jsonp runtime): hotDownloadUpdateChunk and hotDownloadManifest. The second method calls AJAX to request whether there are updated files from the server. If there are, it returns the list of updated files to the browser. The first method requests the latest module code through jsonp and returns the code to HMR runtime. HMR runtime further processes the returned new module code, which may involve refreshing the page or hot updating the module. It is worth noting that both requests use the file name concatenated with the previous hash value. The hotDownloadManifest method returns the latest hash value, and the hotDownloadUpdateChunk method returns the code block corresponding to the latest hash value. Then, the new code block is returned to HMR runtime for module hot updating. Step 5: HotModuleReplacement.runtime hot updates the module This step is the key step of the entire module hot updating (HMR), and all module hot updates occur in the hotApply method of HMR runtime. // webpack/lib/HotModuleReplacement.runtime function hotApply() &#123; // ... var idx; var queue = outdatedModules.slice(); while(queue.length > 0) &#123; moduleId = queue.pop(); module = installedModules[moduleId]; // ... // remove module from cache delete installedModules[moduleId]; // when disposing there is no need to call dispose handler delete outdatedDependencies[moduleId]; // remove \"parents\" references from all children for(j = 0; j &lt; module.children.length; j++) &#123; var child = installedModules[module.children[j]]; if(!child) continue; idx = child.parents.indexOf(moduleId); if(idx >= 0) &#123; child.parents.splice(idx, 1); &#125; &#125; &#125; // ... // insert new code for(moduleId in appliedUpdate) &#123; if(Object.prototype.hasOwnProperty.call(appliedUpdate, moduleId)) &#123; modules[moduleId] = appliedUpdate[moduleId]; &#125; &#125; // ... &#125; From the hotApply method above, it can be seen that module hot replacement mainly consists of three stages. The first stage is to find outdatedModules and outdatedDependencies. I did not include this part of the code here, but if you are interested, you can read the source code yourself. The second stage is to delete expired modules and dependencies from the cache, as follows: delete installedModules[moduleId]; delete outdatedDependencies[moduleId]; The third stage is to add the new module to the modules object. The next time the __webpack_require__ method (the require method rewritten by webpack) is called, the new module code will be obtained. For error handling during module hot updates, if an error occurs during the hot update process, the hot update will fall back to refreshing the browser. This part of the code is in the dev-server code, and the brief code is as follows: module.hot.check(true).then(function(updatedModules) &#123; if(!updatedModules) &#123; return window.location.reload(); &#125; // ... &#125;).catch(function(err) &#123; var status = module.hot.status(); if([\"abort\", \"fail\"].indexOf(status) >= 0) &#123; window.location.reload(); &#125; &#125;); dev-server first verifies if there are any updates, and if there are no code updates, it reloads the browser. If an abort or fail error occurs during the hotApply process, the browser is also reloaded.","link":"/2023/11/10/en/Webpack-optimization-4/"},{"title":"Understanding Ajax and Cross-Origin Requests Easily","text":"IntroductionWhen learning to write web pages, you usually start with HTML and CSS, which are responsible for creating and beautifying the layout. Once you have a solid foundation, you start learning JavaScript to create interactive effects. In addition to user and browser interactions, don’t forget about the interaction between the client and server, which means you must learn how to use JavaScript to retrieve data from the backend server. Otherwise, your web page data will be static. The main target audience of this article is beginners in web front-end development. I hope that after reading this article, readers who do not understand how to exchange data with the server or how to connect to APIs can have a better understanding of how to connect to the backend. Let’s start with an exampleBefore we begin, let’s consider a question: Why does the front-end need to exchange data with the backend? Actually, this depends on the type of web page you are creating. If you are creating an official website, the entire website is likely to be static, and only HTML and CSS are required, without the need to retrieve data from the backend server. Let’s assume that today we want to create a web page that can browse the current Twitch live stream list, as shown below. If this web page does not retrieve data from the backend, it means that the content of the web page is fixed and will remain the same no matter when it is viewed. However, this is not correct because the goal of this web page is to display “channels that are currently live,” so the content will change accordingly. Since the content will change, we must continuously update the data, retrieve data from the server, and then display it after processing it on the front-end. After confirming the need to retrieve data, we can ask ourselves two questions: Who do we retrieve data from? How do we retrieve data? The answer to the first question is obviously Twitch because Twitch has the data we need! As for the second question, we must use the Twitch API. APIWhat is an API? You may have heard this term many times, but still don’t know what it means. Let’s start with its full name, which is “Application Programming Interface.” You may wonder what this is and why I can’t understand it in both Chinese and English. But actually, the most important thing in these few words is the word “interface.” What is an interface? An interface is used for connection. I’ll give you an example. Isn’t there a USB slot on your computer? As long as you see USB flash drives on the market, you can buy them and plug them into the USB slot, and your computer can read them. Have you ever wondered why? Although they are made by different manufacturers, they can all be read and plugged into the USB slot. This is because there is a standard called the USB interface. After this standard was established, as long as all manufacturers develop according to this standard, they can ensure that they can connect to the computer and USB flash drives. API is the same, but it becomes a connection between programs. For example, if I need to read a file in my program, how do I read it? Reading files is a function provided by the operating system, so I can connect to the “read file API” and use this function in my program. I’ll give you a few more examples. Suppose I want to allow my web page to log in with Facebook. What should I do? I need to connect to the “Facebook API,” which is a set of standards provided by Facebook to everyone who wants to access Facebook services. Any developer who wants to access Facebook services can follow these standards to obtain the data they want. This thing is called an API. Or maybe you are a developer of a hotel management system today, and your company has developed an ERP for hotels, which can manage the booking status of hotels and so on, and can know which rooms are empty now. If you only use this data yourself, it would be a pity. Therefore, the company decided to provide this data to large booking websites, which can display the room status of this hotel in real-time on those websites. Therefore, data exchange is necessary, and you need to provide a “query room status API” to other websites so that they can connect to it and obtain this information. By now, you should have some sense of what an API is. Let me give you a few more examples: I want to retrieve photos from Flickr, so I need to connect to the Flickr API. Google wants to allow other apps to log in and authenticate with Google, so Google needs to provide the “Google login API.” I want to retrieve the channels currently available on Twitch, so I need to connect to the Twitch API. API DocumentationNow that we know what an API is and that we need to connect to it, the next question is “how do we connect?” Earlier, we mentioned an example of file access. This is actually more like calling a function provided by the operating system or a programming language library. You can usually find more detailed information about these functions in the official documentation, such as reading files in Node.js: (Source: https://nodejs.org/api/fs.html#fs_fs_readdir_path_options_callback) Above, it is written which function you should call and what parameters you should pass in. API integration is the same. You must have documentation to know how to integrate, otherwise you cannot integrate at all because you don’t even know what parameters to pass. Let’s take a look at how the Twitch API documentation is written. It explains that you must have a Client ID, and the API Root URL is https://api.twitch.tv/kraken, etc. These are basic information related to the API. If you click on any API in the left column, you will see detailed information about each API: Here, it is written what the URL is, what parameters you should pass, etc. There are also reference examples below, which is a very complete API documentation. Usually, when writing web pages, we directly talk about APIs, but actually we are referring to Web APIs, which are APIs transmitted through the network. Are there non-Web APIs? Yes, like the file reading API we mentioned earlier, they are all executed locally on the computer without going through any network. But this doesn’t really matter, everyone is used to talking about APIs, as long as they can understand it. Now that we have the API documentation, we have all the information we need. Using the Twitch example above, as long as we can send a request to https://api.twitch.tv/kraken/games/top?client_id=xxx through JavaScript, Twitch will return the current list of the most popular games. We have narrowed down the scope of the problem step by step. At first, it was “how to get data from Twitch”, and now it is divided into: “how to use JavaScript to send a request”. AjaxTo send a request on the browser, you must use a technology called Ajax, which stands for “Asynchronous JavaScript and XML”, with the emphasis on the word “Asynchronous”. Before talking about what is asynchronous, let’s first mention what is synchronous. Almost all JavaScript you originally wrote is executed synchronously. This means that when it executes to a certain line, it will wait for this line to finish executing before executing the next line, ensuring the execution order. That is to say, the last line of the following code needs to wait for a long time to be executed: var count = 10000000; while(count--) &#123; // Do some time-consuming operations &#125; // Executed after a long time console.log('done') It looks reasonable. Isn’t the program executed line by line? But if it involves network operations, everyone can think about the following example: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // Executed after a long time console.log(result); When JavaScript executes sendRequest, because it is synchronous, it will wait for the response to come back before continuing to do anything. In other words, before the response comes back, the entire JavaScript engine will not execute anything! It’s scary, isn’t it? You click on anything related to JavaScript, and there is no response because JavaScript is still waiting for the response. Therefore, for operations that are expected to be very time-consuming and unstable, synchronous execution cannot be used, but asynchronous execution must be used. What does asynchronous mean? It means that after it is executed, it will not be taken care of, and it will continue to execute the next line without waiting for the result to come back: // Assuming there is a function called sendRequest to send a request var result = sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx'); // The above request is executed, and then it executes to this line, so result will not have anything // because the response has not returned yet console.log(result); Please note that “asynchronous functions cannot directly return results through return”. Why? Because, as in the example above, after sending a request, the next line will be executed, and at this time, there is no response yet. What should be returned? So what should we do? Let me give you a very common example! When I was eating in a food court in Singapore, there was a table number on each table. When you order, just tell the boss which table you are sitting at, and the boss will deliver it to you after the meal is ready. So I don’t need to stand at the door of the store and wait. I just continue to sit on my own things. Anyway, the boss will deliver it to me after the meal is ready. The concept of asynchronous is also like this. After I send a request (after I order), I don’t need to wait for the response to come back (I don’t need to wait for the boss to finish), I can continue to do my own thing. After the response comes back (after the meal is ready), it will help me deliver the result (the boss will deliver it by himself). In the ordering example, the boss can know where to send the data through the table number. What about in JavaScript? Through Function! And this function, we call it a Callback Function, a callback function. When the asynchronous operation is completed, this function can be called and the data can be brought in. // Assuming there is a function called sendRequest to send a request sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', callMe); function callMe (response) &#123; console.log(response); &#125; // Or write it as an anonymous function sendRequest('https://api.twitch.tv/kraken/games/top?client_id=xxx', function (response) &#123; console.log(response); &#125;); Now you know why network operations are asynchronous and what callback functions are. XMLHttpRequestJust mentioned the concepts of Ajax, asynchronous, and callback functions, but didn’t say how to send a request, just wrote a fake sendRequest function as a reference. To send a request, we need to use an object prepared by the browser called XMLHttpRequest. The sample code is as follows: var request = new XMLHttpRequest(); request.open('GET', `https://api.twitch.tv/kraken/games/top?client_id=xxx`, true); request.onload = function() &#123; if (request.status >= 200 &amp;&amp; request.status &lt; 400) &#123; // Success! console.log(request.responseText); &#125; &#125;; request.send(); The request.onload above actually specifies which function to use to handle the data when it comes back. With the above code, you have finally succeeded and can finally connect to the Twitch API and get data from there! It’s really gratifying. From now on, you will live a happy life with the skill of “connecting to the API”… Not really. Same Origin PolicyJust when you thought you were already familiar with connecting to APIs and wanted to try connecting to other APIs, you found that a problem occurred with just one line: XMLHttpRequest cannot load http:&#x2F;&#x2F;odata.tn.edu.tw&#x2F;ebookapi&#x2F;api&#x2F;getOdataJH&#x2F;?level&#x3D;all. No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource. Origin &#39;null&#39; is therefore not allowed access. Huh? Why is there this error? In fact, for security reasons, the browser has something called the Same-origin policy. This means that if the website you are currently on and the API website you want to call are “different sources”, the browser will still help you send the request, but it will block the response, preventing your JavaScript from receiving it and returning an error. What is a different source? Simply put, if the domain is different, it is a different source, or if one uses http and the other uses https, or if the port numbers are different, it is also a different source. So if you are using someone else’s API, in most cases it will be a different source. I want to emphasize here that “your request is still sent”, and the browser “does receive the response”, but the key point is that “due to the same-origin policy, the browser does not pass the result back to your JavaScript”. If there is no browser, there is actually no such problem. You can send it to whoever you want and get the response no matter what. Okay, since we just said that different sources will be blocked, how did we successfully connect to the Twitch API? CORSAs we all know, it is very common to transfer data between different sources, just like we connect to the Twitch API. How can we be under the same domain as the Twitch API? Therefore, the same-origin policy does regulate that non-same-origin requests will be blocked, but at the same time, there is another regulation that says: “If you want to transfer data between different origins, what should you do?” This regulation is called CORS. CORS, short for Cross-Origin Resource Sharing, is a cross-origin resource sharing protocol. This protocol tells you that if you want to open cross-origin HTTP requests, the server must add Access-Control-Allow-Origin to the response header. You should be familiar with this field. If you feel unfamiliar, you can go back and look at the error message just now, which actually mentioned this header. After the browser receives the response, it will first check the content of Access-Control-Allow-Origin. If it contains the origin of the request that is currently being initiated, it will allow it to pass and allow the program to receive the response smoothly. If you carefully check the request we sent to Twitch in the beginning, you will find that the header of the response is roughly like this: Content-Type: application&#x2F;json Content-Length: 71 Connection: keep-alive Server: nginx Access-Control-Allow-Origin: * Cache-Control: no-cache, no-store, must-revalidate, private Expires: 0 Pragma: no-cache Twitch-Trace-Id: e316ddcf2fa38a659fa95af9012c9358 X-Ctxlog-Logid: 1-5920052c-446a91950e3abed21a360bd5 Timing-Allow-Origin: https:&#x2F;&#x2F;www.twitch.tv The key point is this line: Access-Control-Allow-Origin: *, where the asterisk represents a wildcard character, meaning that any origin is accepted. Therefore, when the browser receives this response, it compares the current origin with the * rule, passes the verification, and allows us to accept the response of the cross-origin request. In addition to this header, there are actually others that can be used, such as Access-Control-Allow-Headers and Access-Control-Allow-Methods, which can define which request headers and methods are accepted. To sum up, if you want to initiate a cross-origin HTTP request and receive a response smoothly, you need to ensure that the server side has added Access-Control-Allow-Origin, otherwise the response will be blocked by the browser and an error message will be displayed. Preflight RequestDo you still remember Twitch’s API documentation? It requires a client-id parameter, and the document says that you can pass it in the GET parameter or in the header. Let’s try passing it in the header! Open Devtool, and you will see a magical phenomenon: Huh? I clearly only sent one request, why did it become two? And the method of the first one is actually OPTIONS. Why did adding one header result in an extra request? In fact, this is also related to CORS mentioned above. CORS divides requests into two types, one is a simple request. What is a simple request? There is actually a long definition, which I think you can read when you need it. But in short, if you don’t add any custom headers, and it’s a GET request, it’s definitely a simple request (isn’t this simple enough?). On the contrary, if you add some custom headers, such as the Client-ID we just added, this request is definitely not a simple request. (Definition reference: MDN: Simple Request) From the above classification, we know that the request we just initiated is not a simple request because it has a custom header. So why is there an extra request? This request is called a Preflight Request, which is used to confirm whether subsequent requests can be sent because non-simple requests may contain some user data. If this Preflight Request fails, the real request will not be sent, which is the purpose of the Preflight Request. Let me give you an example, and you will know why this Preflight Request is needed. Assuming that a server provides an API URL called: https://example.com/data/16, you can get the data with id 16 by sending a GET request to it, and you can delete this data by sending a DELETE request to it. If there is no Preflight Request mechanism, I can send a DELETE request to this API on any web page of any domain. As I emphasized earlier, the browser’s CORS mechanism will still help you send the request, but only the response will be blocked by the browser. Therefore, even though there is no response, the server did receive this request, so it will delete this data. If there is a Preflight Request, when receiving the result of the request, it will know that this API does not provide CORS, so the real DELETE request will not be sent, and it will end here. The purpose of the Preflight Request is to use an OPTIONS request to confirm whether the subsequent request can be sent. JSONPFinally, let’s talk about JSONP, which is another method for cross-origin requests besides CORS, called JSON with Padding. Do you remember the same-origin policy mentioned at the beginning? If you think about it carefully, you will find that some things are not restricted by the same-origin policy, such as the &lt;script&gt; tag. Don’t we often refer to third-party packages such as CDN or Google Analytics on web pages? The URLs are all from other domains, but they can be loaded normally. JSONP uses this feature of &lt;script&gt; to achieve cross-origin requests. Imagine you have an HTML like this: &lt;script> var response = &#123; data: 'test' &#125;; &lt;/script> &lt;script> console.log(response); &lt;/script> It’s a very easy-to-understand piece of code, so I won’t explain it much. What if you replace the above code with a URL? &lt;script src=\"https://another-origin.com/api/games\">&lt;/script> &lt;script> console.log(response); &lt;/script> If the content returned by https://another-origin.com/api/games is the same as before: var response = &#123; data: 'test' &#125;; Then can’t I get the data in the same way? And these data are still controlled by the server, so the server can give me any data. But using global variables like this is not very good. We can use the concept of Callback Function just mentioned and change it to this: &lt;script> receiveData(&#123; data: 'test' &#125;); &lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> So what is JSONP? JSONP actually uses the above format to put data in &lt;script&gt; and bring the data back through the specified function. If you think of the first &lt;script&gt; as the server’s return value, you will understand. In practice, when operating JSONP, the server usually provides a callback parameter for the client to bring over. The Twitch API provides a JSONP version, and we can directly look at the example. URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=aaa&amp;limit=1 aaa(&#123;\"_total\":1069,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65243,\"channels\":373&#125;]&#125;) URL: https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1 receiveData(&#123;\"_total\":1067,\"_links\":&#123;\"self\":\"https://api.twitch.tv/kraken/games/top?limit=1\",\"next\":\"https://api.twitch.tv/kraken/games/top?limit=1\\u0026offset=1\"&#125;,\"top\":[&#123;\"game\":&#123;\"name\":\"Dota 2\",\"popularity\":63361,\"_id\":29595,\"giantbomb_id\":32887,\"box\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-272x380.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-136x190.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-52x72.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-boxart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"logo\":&#123;\"large\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-240x144.jpg\",\"medium\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-120x72.jpg\",\"small\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-60x36.jpg\",\"template\":\"https://static-cdn.jtvnw.net/ttv-logoart/Dota%202-&#123;width&#125;x&#123;height&#125;.jpg\"&#125;,\"_links\":&#123;&#125;,\"localized_name\":\"Dota 2\",\"locale\":\"zh-tw\"&#125;,\"viewers\":65622,\"channels\":376&#125;]&#125;) Have you noticed? It passes the callback parameter you brought over as the function name and passes the entire JavaScript object to the Function, so you can get the data inside the Function. Combined, it would look like this: &lt;script src=\"https://api.twitch.tv/kraken/games/top?client_id=xxx&amp;callback=receiveData&amp;limit=1\">&lt;/script> &lt;script> function receiveData (response) &#123; console.log(response); &#125; &lt;/script> Using JSONP, you can also access cross-origin data. However, the disadvantage of JSONP is that the parameters you need to pass can only be passed through the URL in a GET request, and cannot be passed through a POST request. If CORS can be used, it should be prioritized over JSONP. SummaryThe content of this article starts with the process of fetching data and tells you step by step where to fetch it and how to fetch it. If you want to fetch data using an API, what is an API? How to call Web API in JavaScript? How to access cross-origin data? Generally speaking, I have mentioned everything related to fetching data with the front-end, but there is a regret that I did not mention the Fetch API, which is a newer standard used to fetch data. The introduction on MDN is: The Fetch API provides an interface for fetching resources (including across the network). It will seem familiar to anyone who has used XMLHttpRequest, but the new API provides a more powerful and flexible feature set. Interested readers can check it out for themselves. I hope that after reading this article, you will have a better understanding of how to connect to the back-end API and the difficulties you may encounter when connecting. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2017/08/27/en/ajax-and-cors/"},{"title":"Understanding Hadoop in One Article","text":"What is Hadoop?Hadoop is a distributed system infrastructure developed by the Apache Foundation. It is a software framework that combines a storage system and a computing framework. It mainly solves the problem of storing and computing massive data and is the cornerstone of big data technology. Hadoop processes data in a reliable, efficient, and scalable way. Users can develop distributed programs on Hadoop without understanding the underlying details of the distributed system. Users can easily develop and run applications that process massive data on Hadoop. What problems can Hadoop solve? Massive data storage HDFS has high fault tolerance and is designed to be deployed on low-cost hardware. It provides high throughput for accessing data and is suitable for applications with large data sets. It consists of n machines running DataNode and one machine running NameNode (another standby). Each DataNode manages a portion of the data, and NameNode is responsible for managing the information (metadata) of the entire HDFS cluster. Resource management, scheduling, and allocation Apache Hadoop YARN (Yet Another Resource Negotiator) is a new Hadoop resource manager. It is a general resource management system and scheduling platform that provides unified resource management and scheduling for upper-layer applications. Its introduction has brought huge benefits to the cluster in terms of utilization, unified resource management, and data sharing. The origin of Hadoop The core architecture of HadoopThe core of Hadoop is HDFS and MapReduce. HDFS provides storage for massive data, and MapReduce provides a computing framework for massive data. HDFS The entire HDFS has three important roles: NameNode, DataNode, and Client. Typical master-slave architecture, using TCP&#x2F;IP communication. NameNode: The master node of the distributed file system, responsible for managing the namespace of the file system, cluster configuration information, and storage block replication. The NameNode stores the metadata of the file system in memory, including file information, block information for each file, and information about each block in the DataNode. DataNode: The slave node of the distributed file system, which is the basic unit of file storage. It stores blocks in the local file system and saves the metadata of the blocks. It also periodically sends information about all existing blocks to the NameNode. Client: Splits files, accesses HDFS, interacts with the NameNode to obtain file location information, and interacts with the DataNode to read and write data. There is also the concept of a block: a block is the basic read and write unit in HDFS. Files in HDFS are stored as blocks, which are replicated to multiple DataNodes. The size of a block (usually 64MB) and the number of replicated blocks are determined by the client when the file is created. MapReduceMapReduce is a distributed computing model that divides large data sets (greater than 1TB) into many small data blocks, and then performs parallel processing on various nodes in the cluster, and finally aggregates the results. The MapReduce calculation process can be divided into two stages: the Map stage and the Reduce stage. Map stage: The input data is divided into several small data blocks, and then multiple Map tasks process them in parallel. Each Map task outputs the processing result as several key-value pairs. Reduce stage: The output results of the Map stage are grouped according to the keys in the key-value pairs, and then multiple Reduce tasks process them in parallel. Each Reduce task outputs the processing result as several key-value pairs. SummaryHadoop is a distributed system infrastructure that mainly solves the problem of storing and computing massive data. Its core is HDFS and MapReduce, where HDFS provides storage for massive data, and MapReduce provides a computing framework for massive data. In addition, Hadoop also has an important component-YARN, which is a general resource management system and scheduling platform that provides unified resource management and scheduling for upper-layer applications.","link":"/2023/11/08/en/hadoop-1/"},{"title":"MapReduce Working Principle in Hadoop","text":"Definition of MapReduceMapReduce is a programming framework for distributed computing programs. It is the core framework for developing “Hadoop-based data analysis applications”. Its core function is to integrate the user’s written business logic code and default components into a complete distributed computing program, which runs concurrently on a Hadoop cluster. Reason for the Emergence of MapReduceWhy do we need MapReduce? Massive data cannot be processed on a single machine due to hardware resource limitations. Once the single-machine version of the program is extended to run on a cluster, it will greatly increase the complexity and development difficulty of the program. With the introduction of the MapReduce framework, developers can focus most of their work on the development of business logic, while leaving the complexity of distributed computing to the framework to handle. Consider a word count requirement in a scenario with massive data: Single-machine version: limited memory, limited disk, limited computing power Distributed: file distributed storage (HDFS), computing logic needs to be divided into at least two stages (one stage is independently concurrent, one stage is converged), how to distribute computing programs, how to allocate computing tasks (slicing), how to start the two-stage program? How to coordinate? Monitoring during the entire program running process? Fault tolerance? Retry? It can be seen that when the program is extended from a single-machine version to a distributed version, a large amount of complex work will be introduced. Relationship between MapReduce and YarnYarn is a resource scheduling platform that is responsible for providing server computing resources for computing programs, which is equivalent to a distributed operating system platform. MapReduce and other computing programs are like application programs running on top of the operating system. Important concepts of YARN: Yarn does not know the running mechanism of the program submitted by the user; Yarn only provides scheduling of computing resources (when the user program applies for resources from Yarn, Yarn is responsible for allocating resources); The supervisor role in Yarn is called ResourceManager; The role that specifically provides computing resources in Yarn is called NodeManager; In this way, Yarn is completely decoupled from the running user program, which means that various types of distributed computing programs (MapReduce is just one of them), such as MapReduce, storm programs, spark programs, tez, etc., can run on Yarn; Therefore, computing frameworks such as Spark and Storm can be integrated to run on Yarn, as long as they have resource request mechanisms that comply with Yarn specifications in their respective frameworks; Yarn becomes a universal resource scheduling platform. From then on, various computing clusters that previously existed in enterprises can be integrated on a physical cluster to improve resource utilization and facilitate data sharing. MapReduce Working PrincipleStrictly speaking, MapReduce is not an algorithm, but a computing idea. It consists of two stages: map and reduce. MapReduce ProcessTo improve development efficiency, common functions in distributed programs can be encapsulated into frameworks, allowing developers to focus on business logic. MapReduce is such a general framework for distributed programs, and its overall structure is as follows (there are three types of instance processes during distributed operation): MRAppMaster: responsible for the process scheduling and status coordination of the entire program MapTask: responsible for the entire data processing process of the map phase ReduceTask: responsible for the entire data processing process of the reduce phase MapReduce Mechanism The process is described as follows: When an MR program starts, the MRAppMaster is started first. After the MRAppMaster starts, according to the description information of this job, it calculates the number of MapTask instances required and applies to the cluster to start the corresponding number of MapTask processes. After the MapTask process is started, data processing is performed according to the given data slice range. The main process is: Use the inputformat specified by the customer to obtain the RecordReader to read the data and form input KV pairs; Pass the input KV pairs to the customer-defined map() method for logical operation, and collect the KV pairs output by the map() method to the cache; Sort the KV pairs in the cache according to K partition and continuously overflow to the disk file. After the MRAppMaster monitors that all MapTask process tasks are completed, it will start the corresponding number of ReduceTask processes according to the customer-specified parameters, and inform the ReduceTask process of the data range (data partition) to be processed. After the ReduceTask process is started, according to the location of the data to be processed notified by the MRAppMaster, it obtains several MapTask output result files from several machines where the MapTask is running, and performs re-merging and sorting locally. Then, groups the KV with the same key into one group, calls the customer-defined reduce() method for logical operation, collects the result KV output by the operation, and then calls the customer-specified outputformat to output the result data to external storage. Let’s take an example. The above figure shows a word frequency counting task. Hadoop divides the input data into several slices and assigns each split to a map task for processing. After mapping, each word and its frequency in this task are obtained. Shuffle puts the same words together, sorts them, and divides them into several slices. According to these slices, reduce is performed. The result of the reduce task is counted and output to a file. In MapReduce, two roles are required to complete these processes: JobTracker and TaskTracker. JobTracker is used to schedule and manage other TaskTrackers. JobTracker can run on any computer in the cluster. TaskTracker is responsible for executing tasks and must run on DataNode. Here is a simple MapReduce implementation example: It is used to count the number of occurrences of each word in the input file. Import necessary packages: import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; Define the Mapper class: public static class MyMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable> &#123; protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); // Split each line of text into words and send them to the Reducer String[] words = line.split(\"\\\\s+\"); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125; &#125; The Mapper class is responsible for splitting the input text data into words and outputting a key-value pair (word, 1) for each word. Define the Reducer class: public static class MyReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable> &#123; protected void reduce(Text key, Iterable&lt;IntWritable> values, Context context) throws IOException, InterruptedException &#123; int sum = 0; // Accumulate the number of occurrences of the same word for (IntWritable value : values) &#123; sum += value.get(); &#125; // Output the word and its total number of occurrences context.write(key, new IntWritable(sum)); &#125; &#125; The Reducer class receives key-value pairs from the Mapper, accumulates the values of the same key, and then outputs the word and its total number of occurrences. Main function (main method): public static void main(String[] args) throws InterruptedException, IOException, ClassNotFoundException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(word.class); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(MyReduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // Set the input and output paths FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // Submit the job and wait for it to complete job.waitForCompletion(true); &#125; In the entire Hadoop architecture, the computing framework plays a crucial role, on the one hand, it can operate on the data in HDFS, on the other hand, it can be encapsulated to provide calls from upper-level components such as Hive and Pig. Let’s briefly introduce some of the more important components. HBase: originated from Google’s BigTable; it is a highly reliable, high-performance, column-oriented, and scalable distributed database. Hive: is a data warehouse tool that can map structured data files to a database table, and quickly implement simple MapReduce statistics through SQL-like statements, without the need to develop dedicated MapReduce applications, which is very suitable for statistical analysis of data warehouses. Pig: is a large-scale data analysis tool based on Hadoop. It provides a SQL-LIKE language called Pig Latin. The compiler of this language converts SQL-like data analysis requests into a series of optimized MapReduce operations. ZooKeeper: originated from Google’s Chubby; it is mainly used to solve some data management problems frequently encountered in distributed applications, simplifying the difficulty of coordinating and managing distributed application. Ambari: Hadoop management tool, which can monitor, deploy, and manage clusters quickly. Sqoop: used to transfer data between Hadoop and traditional databases. Mahout: an extensible machine learning and data mining library. Advantages and Applications of Hadoop Overall, Hadoop has the following advantages: High reliability: This is determined by its genes. Its genes come from Google. The best thing Google is good at is “garbage utilization.” When Google started, it was poor and couldn’t afford high-end servers, so it especially likes to deploy this kind of large system on ordinary computers. Although the hardware is unreliable, the system is very reliable. High scalability: Hadoop distributes data and completes computing tasks among available computer clusters, and these clusters can be easily expanded. In other words, it is easy to become larger. High efficiency: Hadoop can dynamically move data between nodes and ensure dynamic balance of each node, so the processing speed is very fast. High fault tolerance: Hadoop can automatically save multiple copies of data and automatically redistribute failed tasks. This is also considered high reliability. Low cost: Hadoop is open source and relies on community services, so the cost of use is relatively low. Based on these advantages, Hadoop is suitable for applications in large data storage and large data analysis, suitable for running on clusters of several thousand to tens of thousands of servers, and supports PB-level storage capacity. Hadoop’s applications are very extensive, including: search, log processing, recommendation systems, data analysis, video and image analysis, data storage, etc., can be deployed using it.","link":"/2023/11/08/en/hadoop-2/"},{"title":"Hadoop 2.0 Architecture - Distributed File System HDFS","text":"HDFS Design PrinciplesDesign GoalsStore very large files: “very large” here means several hundred M, G, or even TB. Adopt a stream-based data access method: HDFS is based on the assumption that the most effective data processing mode is to generate or copy a data set once and then do a lot of analysis work on it. Analysis work often reads most of the data in the data set, even if not all of it. Therefore, the time required to read the entire data set is more important than the delay in reading the first record. Run on commercial hardware: Hadoop does not require special expensive, reliable machines and can run on ordinary commercial machines (which can be purchased from multiple vendors). Commercial machines do not mean low-end machines. In a cluster (especially a large one), the node failure rate is relatively high. HDFS’s goal is to ensure that the cluster does not cause significant interruptions to users when nodes fail. Application Types Not Suitable for HDFSSome scenarios are not suitable for storing data in HDFS. Here are a few examples: Low-latency data accessApplications that require latency in the millisecond range are not suitable for HDFS. HDFS is designed for high-throughput data transmission, so latency may be sacrificed. HBase is more suitable for low-latency data access. A large number of small filesThe metadata of files (such as directory structure, node list of file blocks, and block-node mapping) is stored in the memory of the NameNode. The number of files in the entire file system is limited by the memory size of the NameNode. As a rule of thumb, a file&#x2F;directory&#x2F;file block generally occupies 150 bytes of metadata memory space. If there are one million files, each file occupies one file block, which requires about 300M of memory. Therefore, the number of files in the billions is difficult to support on existing commercial machines. Multiple reads and writes, requiring arbitrary file modificationHDFS writes data in an append-only manner. It does not support arbitrary offset modification of files. It does not support multiple writers. HDFS PositioningTo improve scalability, HDFS uses a master&#x2F;slave architecture to build a distributed storage cluster, which makes it easy to add or remove slaves to the cluster. HDFS is an important component of the Hadoop ecosystem. It is a distributed file system designed to store large amounts of data and provide high-throughput data access. HDFS is designed to store data on inexpensive hardware and provide high fault tolerance. It achieves this goal by distributing data to multiple nodes in the cluster. HDFS is positioned as a batch processing system suitable for offline processing of large-scale data. The main features of HDFS include: High fault tolerance: HDFS distributes data to multiple nodes, so even if a node fails, data can still be accessed through other nodes. High throughput: HDFS is designed to support batch processing of large-scale data, so it provides high-throughput data access. Suitable for large files: HDFS is suitable for storing large files because it divides files into multiple blocks for storage and distributes these blocks to multiple nodes. Stream data access: HDFS supports stream data access, which means it can efficiently process large amounts of data streams. HDFS ArchitectureHDFS uses a master&#x2F;slave architecture to build a distributed storage service, which improves the scalability of HDFS and simplifies the architecture design. HDFS stores files in blocks, optimizing storage granularity. The NameNode manages the storage space of all slave machines, while the DataNode is responsible for actual data storage and read&#x2F;write operations. BlocksThere is a concept of blocks in physical disks. The physical block of a disk is the smallest unit of disk operation for reading and writing, usually 512 bytes. The file system abstracts another layer of concepts on top of the physical block of the disk, and the file system block is an integer multiple of the physical disk block. Generally, it is several KB. The blocks in Hadoop are much larger than those in general single-machine file systems, with a default size of 128M. The file in HDFS is split into block-sized chunks for storage, and these chunks are scattered across multiple nodes. If the size of a file is smaller than the block size, the file will not occupy the entire block, only the actual size. For example, if a file is 1M in size, it will only occupy 1M of space in HDFS, not 128M. Why are HDFS blocks so large?To minimize the seek time and control the ratio of time spent locating and transmitting files. Assuming that the time required to locate a block is 10ms and the disk transmission speed is 100M&#x2F;s. If the proportion of time spent locating a block to the transmission time is controlled to 1%, the block size needs to be about 100M. However, if the block is set too large, in MapReduce tasks, if the number of Map or Reduce tasks is less than the number of cluster machines, the job efficiency will be very low. Benefits of block abstraction The splitting of blocks allows a single file size to be larger than the capacity of the entire disk, and the blocks that make up the file can be distributed across the entire cluster. In theory, a single file can occupy the disk of all machines in the cluster. Block abstraction also simplifies the storage system, without worrying about its permissions, owner, and other content (these contents are controlled at the file level). Blocks are the unit of replication in fault tolerance and high availability mechanisms. Namenode &amp; DatanodeThe entire HDFS cluster consists of a master-slave model of Namenode and Datanode. The Namenode stores the file system tree and metadata of all files and directories. The metadata is persisted in two forms: Namespace image Edit log However, the persistent data does not include the node list where the block is located and which nodes the file blocks are distributed to in the cluster. This information is reconstructed when the system is restarted (through the block information reported by the Datanode). In HDFS, the Namenode may become a single point of failure for the cluster. When the Namenode is unavailable, the entire file system is unavailable. HDFS provides two solutions to single point of failure: Backup persistent metadataWrite the file system metadata to multiple file systems at the same time, such as writing metadata to both the local file system and NFS at the same time. These backup operations are synchronous and atomic. Secondary NamenodeThe Secondary node periodically merges the namespace image and edit log of the main Namenode to avoid the edit log being too large, and merges them by creating a checkpoint. It maintains a merged namespace image replica that can be used to recover data when the Namenode completely crashes. The following figure shows the management interface of the Secondary Namenode: Internal Features of HDFSData Redundancy HDFS stores each file as a series of data blocks, with a default block size of 64MB (configurable). For fault tolerance, all data blocks of a file have replicas (the replication factor is configurable). HDFS files are written once and strictly limited to only one write user at any time. Replica Placement HDFS clusters usually run on multiple racks, and communication between machines on different racks requires switches. HDFS uses a rack-aware strategy to improve data reliability, availability, and network bandwidth utilization. Rack failures are much less common than node failures, and this strategy can prevent data loss when an entire rack fails, improve data reliability and availability, and ensure performance. Replica Selection HDFS tries to use the replica closest to the program to meet user requests, reducing total bandwidth consumption and read latency. The HDFS architecture supports data balancing strategies. Heartbeat Detection The NameNode periodically receives heartbeats and block reports from each DataNode in the cluster, indicating that the DataNode is working properly. The NameNode marks DataNodes that have not sent heartbeats recently as down and does not send them any new I&#x2F;O requests. The NameNode continuously checks these data blocks that need to be replicated and re-replicates them when necessary. Data Integrity Check For various reasons, the data block obtained from the DataNode may be corrupted. Classic HDFS ArchitectureThe NameNode is responsible for managing the metadata of the file system, while the DataNode is responsible for storing the actual data of the file blocks. This division of labor enables HDFS to efficiently store and manage large-scale data. Specifically, when a client needs to read or write a file, it sends a request to the NameNode. The NameNode returns the metadata information of the file and the location information of the file blocks. The client communicates with the DataNode based on this information to read or write the actual data of the file blocks. Therefore, the NameNode and DataNode play different roles in the HDFS architecture. What is the difference in function? HDFS is an abbreviation for Hadoop Distributed File System, an important component of the Hadoop ecosystem. The HDFS architecture includes one NameNode and multiple DataNodes. The NameNode is the master node of HDFS, responsible for managing the namespace of the file system, the metadata information of the file, and the location information of the file blocks. The DataNode is the slave node of HDFS, responsible for storing the actual data of the file blocks. Specifically, when a client needs to read or write a file, it sends a request to the NameNode. The NameNode returns the metadata information of the file and the location information of the file blocks. The client communicates with the DataNode based on this information to read or write the actual data of the file blocks. General TopologyThere is only one NameNode node, and the SecondaryNameNode or BackupNode node is used to obtain NameNode metadata information in real time and back up metadata. Commercial TopologyThere are two NameNode nodes, and ZooKeeper is used to implement hot standby between NameNode nodes. Command Line InterfaceHDFS provides various interaction methods, such as Java API, HTTP, and shell command line. Command line interaction is mainly operated through hadoop fs. For example: hadoop fs -copyFromLocal &#x2F;&#x2F; Copy files from local to HDFS hadoop fs mkdir &#x2F;&#x2F; Create a directory hadoop fs -ls &#x2F;&#x2F; List file list In Hadoop, the permissions of files and directories are similar to the POSIX model, including three permissions: read, write, and execute. Read permission (r): Used to read files or list the contents of a directoryWrite permission (w): For files, it is the write permission of the file. The write permission of the directory refers to the permission to create or delete files (directories) under the directory.Execute permission (x): Files do not have so-called execute permissions and are ignored. For directories, execute permission is used to access the contents of the directory. Each file or directory has three attributes: owner, group, and mode: Owner: Refers to the owner of the fileGroup: For permission groupsMode: Consists of the owner’s permissions, the permissions of the members of the file’s group, and the permissions of non-owners and non-group members. Data Flow (Read and Write Process)Read FileThe rough process of reading a file is as follows: The client passes a file Path to the FileSystem’s open method. DFS uses RPC to remotely obtain the datanode addresses of the first few blocks of the file. The NameNode determines which nodes to return based on the network topology structure (provided that the node has a block replica). If the client itself is a DataNode and there is a block replica on the node, it is read directly from the local node. The client uses the FSDataInputStream object returned by the open method to read data (call the read method). The DFSInputStream (FSDataInputStream implements this class) connects to the node that holds the first block and repeatedly calls the read method to read data. After the first block is read, find the best datanode for the next block and read the data. If necessary, DFSInputStream will contact the NameNode to obtain the node information of the next batch of Blocks (stored in memory, not persistent), and these addressing processes are invisible to the client. After the data is read, the client calls the close method to close the stream object. During the data reading process, if communication with the DataNode fails, the DFSInputStream object will try to read data from the next best node and remember the failed node, and subsequent block reads will not connect to the node. After reading a Block, DFSInputStram performs checksum verification. If the Block is damaged, it tries to read data from other nodes and reports the damaged block to the NameNode. Which DataNode does the client connect to get the data block is guided by the NameNode, which can support a large number of concurrent client requests, and the NameNode evenly distributes traffic to the entire cluster as much as possible. The location information of the Block is stored in the memory of the NameNode, so the corresponding location request is very efficient and will not become a bottleneck. Write File Step breakdown The client calls the create method of DistributedFileSystem. DistributedFileSystem remotely RPC calls the Namenode to create a new file in the namespace of the file system, which is not associated with any blocks at this time. During this process, the Namenode performs many verification tasks, such as whether there is a file with the same name, whether there are permissions, if the verification passes, it returns an FSDataOutputStream object. If the verification fails, an exception is thrown to the client. When the client writes data, DFSOutputStream is decomposed into packets (data packets) and written to a data queue, which is consumed by DataStreamer. DataStreamer is responsible for requesting the Namenode to allocate new blocks to store data nodes. These nodes store replicas of the same Block and form a pipeline. DataStreamer writes the packet to the first node of the pipeline. After the first node stores the packet, it forwards it to the next node, and the next node continues to pass it down. DFSOutputStream also maintains an ack queue, waiting for confirmation messages from datanodes. After all datanodes on the pipeline confirm, the packet is removed from the ack queue. After the data is written, the client closes the output stream. Flush all packets to the pipeline, and then wait for confirmation messages from datanodes. After all are confirmed, inform the Namenode that the file is complete. At this time, the Namenode already knows all the Block information of the file (because DataStreamer is requesting the Namenode to allocate blocks), and only needs to wait for the minimum replica number requirement to be reached, and then return a successful message to the client. How does the Namenode determine which DataNode the replica is on? The storage strategy of HDFS replicas is a trade-off between reliability, write bandwidth, and read bandwidth. The default strategy is as follows: The first replica is placed on the machine where the client is located. If the machine is outside the cluster, a random one is selected (but it will try to choose a capacity that is not too slow or too busy). The second replica is randomly placed on a rack different from the first replica. The third replica is placed on the same rack as the second replica, but on a different node, and a random selection is made from the nodes that meet the conditions. More replicas are randomly selected throughout the cluster, although too many replicas are avoided on the same rack as much as possible. After the location of the replica is determined, when establishing the write pipeline, the network topology structure is considered. The following is a possible storage strategy: This selection balances reliability, read and write performance well. Reliability: Blocks are distributed on two racks. Write bandwidth: The write pipeline process only needs to cross one switch. Read bandwidth: You can choose one of the two racks to read from. Internal Features of HDFSData Redundancy HDFS stores each file as a series of data blocks, with a default block size of 64MB (configurable). For fault tolerance, all data blocks of a file have replicas (the replication factor is configurable). HDFS files are written once and strictly limited to only one writing user at any time. Replica Placement HDFS clusters usually run on multiple racks, and communication between machines on different racks requires switches. HDFS uses a rack-aware strategy to improve data reliability, availability, and network bandwidth utilization. Rack failures are much less common than node failures, and this strategy can prevent data loss when an entire rack fails, improving data reliability and availability while ensuring performance. Replica Selection HDFS tries to use the replica closest to the program to satisfy user requests, reducing total bandwidth consumption and read latency. HDFS architecture supports data balancing strategies. Heartbeat Detection The NameNode periodically receives heartbeats and block reports from each DataNode in the cluster. Receiving a heartbeat indicates that the DataNode is working properly. The NameNode marks DataNodes that have not sent a heartbeat recently as dead and does not send them any new I&#x2F;O requests. The NameNode continuously checks for data blocks that need to be replicated and replicates them when necessary. Data Integrity Check Various reasons may cause the data block obtained from the DataNode to be corrupted. HDFS client software implements checksum verification of HDFS file content. If the checksum of the data block obtained by the DataNode is different from that in the hidden file corresponding to the data block, the client judges that the data block is corrupted and obtains a replica of the data block from another DataNode. Simple Consistency Model, Stream Data Access HDFS applications generally access files in a write-once, read-many mode. Once a file is created, written, and closed, it does not need to be changed again. This simplifies data consistency issues and makes high-throughput data access possible. Applications running on HDFS mainly focus on stream reading and batch processing, emphasizing high-throughput data access. Client Cache The request for the client to create a file does not immediately reach the NameNode. The HDFS client first caches the data to a local temporary file, and the write operation of the program is transparently redirected to this temporary file. When the accumulated data in this temporary file exceeds the size of a block (64MB), the client contacts the NameNode. If the NameNode crashes before the file is closed, the file will be lost. If client caching is not used, network speed and congestion will have a significant impact on output.","link":"/2023/11/09/en/hadoop-3/"},{"title":"How Does JIT (Just-In-Time) Compiler Work","text":"IntroductionBefore delving into JIT, it’s essential to have a basic understanding of the compilation process. In compiler theory, translating source code into machine instructions generally involves several crucial steps: JIT OverviewJIT stands for Just-In-Time compiler. Through JIT technology, it’s possible to accelerate the execution speed of Java programs. But how is this achieved? Java is an interpreted language (or semi-compiled, semi-interpreted language). Java compiles the source code into platform-independent Java bytecode files (.class) using the javac compiler. These bytecode files are then interpreted and executed by the Java Virtual Machine (JVM), ensuring platform independence. However, interpreting bytecode involves translating it into corresponding machine instructions, which inevitably slows down the execution speed compared to directly executing binary bytecode files. To enhance execution speed, JIT technology is introduced. When the JVM identifies a method or code block that is executed frequently, it recognizes it as a “Hot Spot Code.” JIT compiles these “Hot Spot Codes” into native machine-specific machine code, optimizes it, and caches the compiled machine code for future use. Hot Spot CompilationWhen the JVM executes code, it doesn’t immediately start compiling it. There are two main reasons for this: Firstly, if a piece of code is expected to be executed only once in the future, compiling it immediately is essentially a waste of resources. Compiling code into Java bytecode is much faster than both compiling and executing the code. However, if a piece of code, such as a method call or a loop, is executed multiple times, compiling it becomes worthwhile. The compiler has the ability to discern which methods are frequently called to ensure efficient compilation. Hot Spot VM employs JIT compilation technology to directly compile high-frequency bytecode into machine instructions (with the method as the compilation unit). These compiled machine instructions are executed directly when bytecode is JIT-compiled, providing a performance boost. The second reason involves optimization. As a method or loop is executed more frequently, the JVM gains a better understanding of the code structure. Therefore, the JVM can make corresponding optimizations during the compilation process. How JavaScript is Compiled - How JIT (Just-In-Time) Compiler WorksIn general, there are two ways to translate programs into machine-executable instructions: using a Compiler or an Interpreter. InterpreterAn interpreter translates and executes code line by line as it encounters it. Pros: Fast execution, no compilation delay.Cons: Same code might be translated multiple times, especially within loops. CompilerA compiler translates the code in advance and generates an executable program. Pros: No need for repeated compilation; can optimize code during compilation.Cons: Requires upfront compilation. JITWhen JavaScript first emerged, it was a typical interpreted language, resulting in slow execution speeds. Later, browsers introduced JIT compilers, significantly improving JavaScript’s execution speed. Principle: They added a new component to the JavaScript engine, known as a monitor (or profiler). This monitor observes the running code, noting how many times it runs and the types used. In essence, browsers added a monitor to the JavaScript engine to observe the running code, recording how many times each code segment is executed and the variable types used. Now, why does this approach speed up the execution? Let’s consider a function for illustration: function arraySum(arr) &#123; var sum = 0; for (var i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; &#125; &#125; 1st Step - Interpreter Initially, the code is executed using an interpreter. When a line of code is executed several times, it is marked as “Warm,” and if executed frequently, it is labeled as “Hot.” 2nd Step - Baseline Compiler Warm-labeled code is passed to the Baseline Compiler, which compiles and stores it. The compiled code is indexed based on line numbers and variable types (why variable types are important will be explained shortly). When the index matches, the corresponding compiled code is directly executed without recompilation, eliminating the need to recompile already compiled code. 3rd Step - Optimizing Compiler Hot-labeled code is sent to the Optimizing Compiler, where further optimizations are applied. How are these optimizations performed? This is the key: due to JavaScript’s dynamic typing, a single line of code can have multiple possible compilations, exposing the drawback of dynamic typing. For instance: sum is Int, arr is Array, i is Int; the + operation is simple addition, corresponding to one compilation result. sum is string, arr is Array, i is Int; the + operation is string concatenation, requiring the conversion of i to a string type.… As illustrated in the diagram below, such a simple line of code has 16 possible compilation results. The Baseline Compiler handles this complexity, and thus, the compiled code needs to be indexed using both line numbers and variable types. Different variable types lead to different compilation results. If the code is “Warm,” the JIT’s job ends here. Each subsequent execution involves type checks and uses the corresponding compiled result. However, when the code becomes “Hot,” more optimizations are performed. Here, optimization means JIT makes a specific assumption. For example, assuming sum and i are both Integers and arr is an Array, only one compilation result is needed. In practice, type checks are performed before execution. If the assumptions are incorrect, the execution is “deoptimized,” reverting to the interpreter or baseline compiler versions. This process is called “deoptimization.” As evident, the speed of execution relies on the accuracy of these assumptions. If the assumption success rate is high, the code executes faster. Conversely, low success rates lead to slower execution than without any optimization (due to the optimize =&gt; deoptimize process). ConclusionIn summary, this is what JIT does at runtime. It monitors running code, identifies hot code paths for optimization, making JavaScript run faster. This significantly improves the performance of most JavaScript applications. However, JavaScript performance remains unpredictable. To make it faster, JIT adds some overhead at runtime, including: Optimization and Deoptimization Memory for monitoring and recovering lost information Memory for storing baseline and optimized versions of functions There’s room for improvement, notably in eliminating overhead to make performance more predictable.","link":"/2023/11/04/en/just-in-time-compilers/"},{"title":"Hadoop 2.0 Architecture - Yarn Distributed File System","text":"What is Hadoop Yarn?In the ancient Hadoop 1.0, the JobTracker of MapReduce was responsible for too many tasks, including resource scheduling and managing numerous TaskTrackers. This was naturally unreasonable. Therefore, during the upgrade process from 1.0 to 2.0, Hadoop separated the resource scheduling work of JobTracker and made it an independent resource management framework, which directly made Hadoop the most stable cornerstone in big data. This independent resource management framework is Yarn. Before we introduce Yarn in detail, let’s briefly talk about Yarn. The full name of Yarn is “Yet Another Resource Negotiator”, which means “another resource scheduler”. This naming is similar to “Have a Nice Inn”. Here’s a little more information: there used to be a Java project compilation tool called Ant, which was named similarly, “Another Neat Tool” in abbreviation, which means “another organizing tool”. Since it is called a resource scheduler, its function is naturally responsible for resource management and scheduling. Next, let’s take a closer look at Yarn. Yarn Architecture ① Client: The client is responsible for submitting jobs to the cluster. ② ResourceManager: The main process of the cluster, the arbitration center, is responsible for cluster resource management and task scheduling. ③ Scheduler: Resource arbitration module. ④ ApplicationManager: Selects, starts, and supervises the ApplicationMaster. ⑤ NodeManager: The cluster’s secondary process, which manages and monitors Containers and executes specific tasks. ⑥ Container: A collection of local resources, such as a Container with 4 CPUs and 8GB of memory. ⑦ ApplicationMaster: The task execution and supervision center. Three Main ComponentsLooking at the top of the figure, we can intuitively see two main components, ResourceManager and NodeManager, but there is actually an ApplicationMaster that is not displayed in the figure. Let’s take a look at these three components separately. ResourceManagerLet’s start with the ResourceManager in the center of the figure. From the name, we can know that this component is responsible for resource management, and there is only one ResourceManager in the entire system to be responsible for resource scheduling. It also includes two main components: the Scheduler and the ApplicationManager. The Scheduler: Essentially, the Scheduler is a strategy or algorithm. When a client submits a task, it allocates resources based on the required resources and the current state of the cluster. Note that it only allocates resources to the application and does not monitor the status of the application. ApplicationManager: Similarly, you can roughly guess what it does from its name. The ApplicationManager is responsible for managing the applications submitted by the client. Didn’t we say that the Scheduler does not monitor the program submitted by the user? In fact, the monitoring of the application is done by the ApplicationManager. ApplicationMasterEvery time a client submits an Application, a new ApplicationMaster is created. This ApplicationMaster applies to the ResourceManager for container resources, sends the program to be run to the container after obtaining the resources, and then performs distributed computing. This may be a bit difficult to understand. Why send the running program to the container? If you look at it from a traditional perspective, the program runs still, and data flows in and out constantly. But when the data volume is large, it cannot be done because the cost of moving massive data is too high and takes too long. However, there is an old Chinese saying that “if the mountain will not come to Muhammad, then Muhammad must go to the mountain.” This is the idea of big data distributed computing. Since big data is difficult to move, I will publish the application program that is easy to move to each node for calculation. This is the idea of big data distributed computing. NodeManagerThe NodeManager is a proxy for the ResourceManager on each machine, responsible for container management, monitoring their resource usage (CPU, memory, disk, and network, etc.), and providing these resource usage reports to the ResourceManager&#x2F;Scheduler. The main idea of Yarn is to split the two functions of resource management and task scheduling of MRv1 JobTracker into two independent processes: Yarn is still a master&#x2F;slave structure. The main process ResourceManager is the resource arbitration center of the entire cluster. The secondary process NodeManager manages local resources. ResourceManager and the subordinate node process NodeManager form the Hadoop 2.0 distributed data computing framework. The Process of Submitting an Application to Yarn This figure shows the process of submitting a program, and we will discuss the process of each step in detail below. The client submits an application to Yarn, assuming it is a MapReduce job. The ResourceManager communicates with the NodeManager to allocate the first container for the application and runs the ApplicationMaster corresponding to the application in this container. After the ApplicationMaster is started, it splits the job (i.e., the application) into tasks that can run in one or more containers. Then it applies to the ResourceManager for containers to run the program and sends heartbeats to the ResourceManager regularly. After obtaining the container, the ApplicationMaster communicates with the NodeManager corresponding to the container and distributes the job to the container in the NodeManager. The MapReduce that has been split will be distributed here, and the container may run Map tasks or Reduce tasks. The task running in the container sends heartbeats to the ApplicationMaster to report its status. When the program is finished, the ApplicationMaster logs out and releases the container resources to the ResourceManager.The above is the general process of running a job. Typical Topology of Yarn ArchitectureIn addition to the two entities of ResourceManager and NodeManager, Yarn also includes two entities of WebAppProxyServer and JobHistoryServer. JobHistoryServer: Manages completed Yarn tasks The logs and various statistical information of historical tasks are managed by JobTracker. Yarn abstracts the function of managing historical tasks into an independent entity, JobHistoryServer. WebAppProxyServer: Web page proxy during task execution By using a proxy, not only the pressure on ResourceManager is further reduced, but also the Web attacks on Yarn can be reduced. Responsible for supervising the entire MapReduce task execution process, collecting the task execution information from the Container, and displaying it on a Web interface. Yarn Scheduling StrategyCapacity Scheduling AlgorithmCapacityScheduler is a multi-user and multi-task scheduling strategy that divides tasks into queues and allocates resources in Container units. Fair Scheduling StrategyFairScheduler is a pluggable scheduling strategy that allows multiple Yarn tasks to use cluster resources fairly.","link":"/2023/11/09/en/hadoop-4/"},{"title":"pack-tool-preview","text":"I tried migrating a real project from Vite to Rspack. The build time reduced from 125 seconds to 17 seconds, and the page refresh speed during development increased by 64%. However, the HMR (Hot Module Replacement) in Rspack is much slower compared to Vite. If you frequently trigger HMR during development and refresh the page less often, Vite still offers a better development experience. For complex projects where refreshing the page is more common, Rspack provides a better development experience. There are so many frontend build tools out there: RollDown, Rollup, Rspack, Vite… Just a sneak peek; stay tuned for the detailed comparison. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/27/en/pack-tool-preview/"},{"title":"Implementation Principles of Vue.js Components","text":"PrefaceIn a previous discussion, we explored the Vue renderer, where the renderer is primarily responsible for rendering the virtual DOM into the real DOM. We only need to use the virtual DOM to describe the final content to be rendered. However, when creating more complex pages, the code for the virtual DOM used to describe the page structure becomes larger, or we can say that the page template becomes more extensive. In such cases, the ability to modularize components becomes essential. With components, we can break down a large page into multiple parts, each part serving as an independent component. These components collectively form the complete page. The implementation of components also requires support from the renderer. Rendering ComponentsFrom the user’s perspective, a stateful component is simply an options object, as shown in the code below: // MyComponent is a component, and its value is an options object const MyComponent = &#123; name: 'MyComponent', data() &#123; return &#123; foo: 1 &#125; &#125; &#125; However, from the internal implementation of the renderer, a component is a special type of virtual DOM node. For example, to describe a regular tag, we use the vnode.type property of the virtual node to store the tag name, as shown in the code below: // This vnode is used to describe a regular tag const vnode = &#123; type: 'div' // ... &#125; To describe a fragment, we set the vnode.type property of the virtual node to ‘Fragment’, and to describe text, we set the vnode.type property to ‘Text’. Do you recall the patch function we discussed earlier in the renderer? In the patch function, Vue processes different logic based on the differences between the new and old virtual DOM nodes. If it’s the initial rendering, it executes the logic for the initial rendering; if it’s an update rendering, it updates the actual DOM nodes based on the differences. function patch(n1, n2, container, anchor) &#123; // 02. If the types of new and old nodes are different, perform unmount operation if (n1 &amp;&amp; n1.type !== n2.type) &#123; unmount(n1); n1 = null; &#125; // 07. Get the node type const &#123; type &#125; = n2; // 09. Check the node type if (typeof type === 'string') &#123; // 10. Process regular elements // TODO: Logic for regular elements &#125; else if (type === Text) &#123; // 11. Process text nodes // TODO: Logic for text nodes &#125; else if (type === Fragment) &#123; // 13. Process fragments // TODO: Logic for fragments &#125; // Return the actual DOM element of the new node return n2.el; &#125; As you can see, the renderer uses the vnode.type property of the virtual node to differentiate its type. Different types of nodes require different methods for mounting and updating. In reality, this applies to components as well. To use virtual nodes to describe components, we can use the vnode.type property to store the component’s options object. function patch(n1, n2, container, anchor) &#123; // 02. If the types of new and old nodes are different, perform unmount operation if (n1 &amp;&amp; n1.type !== n2.type) &#123; unmount(n1); n1 = null; &#125; // 07. Get the node type const type = n2.type; // 09. Check the node type if (typeof type === 'string') &#123; // 10. Process regular elements // TODO: Logic for regular elements &#125; else if (type === Text) &#123; // 11. Process text nodes // TODO: Logic for text nodes &#125; else if (type === Fragment) &#123; // 13. Process fragments // TODO: Logic for fragments &#125; else if (typeof type === 'object') &#123; // 16. The value of vnode.type is an options object, treated as a component if (!n1) &#123; // 17. Mount the component mountComponent(n2, container, anchor); &#125; else &#123; // 21. Update the component patchComponent(n1, n2, anchor); &#125; &#125; // Return the actual DOM element of the new node return n2.el; &#125; In the above code, we added an else if branch to handle the case where the vnode.type property value is an object, treating it as a virtual node describing a component. We call the mountComponent and patchComponent functions to mount and update the component. Now let’s focus on the basics of writing components—what the user should write when creating components, what the options object of a component must include, and what capabilities a component possesses. Therefore, a component must include a rendering function, i.e., the render function, and the return value of the render function should be a virtual DOM. In other words, the render function of a component is an interface for describing the content that the component renders, as shown in the code below: // Definition of the MyComponent component const MyComponent = &#123; // Component name, optional name: 'MyComponent', // Rendering function of the component, its return value must be a virtual DOM render() &#123; // Return a virtual DOM return &#123; type: 'div', children: ['text content'] &#125;; &#125; &#125;; // Vnode object describing the component, type property value is the options object of the component const CompNode = &#123; type: MyComponent &#125;; // Call the renderer to render the component renderer.render(CompNode, document.querySelector('#app')); // The mountComponent function in the renderer is responsible for the actual rendering of the component function mountComponent(vnode, container, anchor) &#123; // Get the options object of the component, i.e., vnode.type const componentOptions = vnode.type; // Get the rendering function of the component, render const render = componentOptions.render; // Execute the rendering function, get the content the component should render, i.e., the virtual DOM returned by the render function const subTree = render(); // Finally, call the patch function to mount the content described by the component, i.e., subTree patch(null, subTree, container, anchor); &#125; ConclusionLet’s recap the basics of components. We explicitly declared an object instance for a component, and the options object of the component must include a rendering function, i.e., the render function. The return value of the render function should be a virtual DOM. We then call render and patch to update and replace components. **In other words, the render function of a component is an interface for describing","link":"/2023/12/08/en/vue-component-1/"},{"title":"Vue Proxy and Reflect","text":"IntroductionSince Vue.js 3’s reactive data is based on Proxy, it’s essential to understand Proxy and its associated concept, Reflect. What is Proxy? In simple terms, Proxy allows you to create a proxy object. It can proxy other objects, emphasizing that Proxy can only proxy objects and not non-object values like strings, booleans, etc. So, what does proxying mean? Proxying refers to the act of creating a basic semantic representation of an object. It allows us to intercept and redefine the basic operations on an object. Create object proxies with Proxy Built-in object Reflect When we talk about “basic semantics” in programming languages, we mean the fundamental operations for reading and modifying data. In JavaScript, these operations typically include reading property values and setting property values. For example, given an object obj, the following operations are considered basic semantics: Read property value: obj.foo (reads the value of property foo) Set property value: obj.foo = newValue (sets the value of property foo) In the above code, Proxy objects allow us to intercept (or redefine) these basic semantic operations. The Proxy constructor takes two parameters: the object being proxied and an object containing interceptors (also known as traps). In the interceptor object, we can define the get method to intercept property read operations and the set method to intercept property set operations. This way, we can execute custom logic when these operations occur. Understanding these basic semantic operations and how to use Proxy and Reflect to intercept and handle them is crucial for implementing reactive data in JavaScript. In reactive data, we can use Proxy and Reflect to track reads and modifications of object properties, enabling reactive updates of data. Basic Usage of ProxyWhen we talk about basic semantics, we refer to fundamental operations in JavaScript, such as reading object property values and setting object property values. Consider the following object obj: const obj = &#123; foo: 1 &#125;; Here, obj.foo is a basic semantic operation for reading property values, and obj.foo = newValue is a basic semantic operation for setting property values. Now, we can use Proxy to intercept these basic semantic operations. const handler = &#123; get(target, key) &#123; console.log(`Reading property $&#123;key&#125;`); return target[key]; &#125;, set(target, key, value) &#123; console.log(`Setting property $&#123;key&#125; to $&#123;value&#125;`); target[key] = value; &#125; &#125;; const proxyObj = new Proxy(obj, handler); proxyObj.foo; // Outputs: Reading property foo proxyObj.foo = 2; // Outputs: Setting property foo to 2 In the above code, we created a handler object that defines get and set methods to intercept property reads and sets. Then, we created a proxy object proxyObj using the Proxy constructor, which intercepts read and set operations on the obj object. When we access proxyObj.foo, the get method is triggered, outputting the corresponding message. When we set the value of proxyObj.foo, the set method is triggered, again outputting the corresponding message. This way, Proxy allows us to execute custom logic when basic semantic operations occur, without directly manipulating the original object. In practical applications, this capability can be used to implement reactive data, data validation, logging, and more. When intercepting object property reads with Proxy, special attention is required for accessor properties because accessor properties are defined using getter functions. The this keyword inside these getter functions changes based on the method of invocation. To solve this issue, we use Reflect.get(target, key, receiver) instead of target[key] when accessing property values. This ensures that the receiver parameter correctly points to the proxy object, not the original object. Consequently, within the getter function of accessor properties, the this keyword refers to the proxy object, establishing the correct reactive relationship. Here is the corrected code using Reflect.get: const handler = &#123; get(target, key, receiver) &#123; track(target, key); // Reactive data dependency tracking return Reflect.get(target, key, receiver); // Use Reflect.get to get property value &#125;, // Other interceptor methods... &#125;; const proxyObj = new Proxy(obj, handler); effect(() => &#123; console.log(proxyObj.bar); // Access the bar property inside the side effect function &#125;); proxyObj.foo++; // Triggers re-execution of the side effect function In this code, we use Reflect.get with the receiver parameter to ensure that this points to the proxy object within the get interceptor function. This establishes the correct reactive relationship, allowing proper dependency tracking when accessing object properties. Usage of Reflect in ReactivityIn interceptor functions, we aim to establish a connection between side-effect functions and reactive data. This ensures that when properties are accessed, the correct dependencies are tracked, enabling re-execution of side-effect functions when properties change. However, if we directly use target[key] to access property values, the this keyword inside the getter function of accessor properties points to the original object, not the proxy object. This prevents the establishment of the correct reactive relationship. To address this issue, we use Reflect.get(target, key, receiver) instead of target[key]. By doing so, the receiver parameter correctly points to the proxy object, allowing the this keyword inside the getter function to refer to the proxy object. This establishes the proper reactive relationship. Here is an example demonstrating the use of the receiver parameter and comparing it with the scenario where the receiver parameter is not used: 1. Using the receiver parameter:const data = &#123; foo: 1 &#125;; const proxy = new Proxy(data, &#123; get(target, key, receiver) &#123; // Use Reflect.get to ensure `this` points to the proxy object const result = Reflect.get(target, key, receiver); // Additional processing, such as triggering update operations, can be performed in practical applications console.log(`Accessed $&#123;key&#125; property with value $&#123;result&#125;`); return result; &#125; &#125;); console.log(proxy.foo); // Outputs: Accessed foo property with value 1 In this example, we use the receiver parameter passed to Reflect.get to ensure that this inside the get interceptor function refers to the proxy object proxy. When you access proxy.foo, the get interceptor function is triggered, and this points to the proxy object. 2. Not using the receiver parameter:const data = &#123; foo: 1 &#125;; const proxy = new Proxy(data, &#123; get(target, key) &#123; // Without using the receiver parameter, 'this' refers to the original object 'data' const result = target[key]; // In practical applications, additional processing might be required, such as triggering update operations console.log(`Accessed $&#123;key&#125; property with value $&#123;result&#125;`); return result; &#125; &#125;); console.log(proxy.foo); // Output: Accessed foo property with value 1 In this example, we did not use the receiver parameter. Since the receiver parameter was not passed, this inside the get interceptor function points to the original object data. Although the proxy object proxy is used, the this inside the get interceptor function does not refer to proxy but instead refers to the original object data. Therefore, in this scenario, the reactive relationship is not established. While the output of the two functions is the same, it’s evident that without using the receiver parameter, the reactive relationship is not established. This means that within the effect function, the object will not receive the correct reactivity. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-Proxy-and-Reflect/"},{"title":"Implementation of Vue.js Component Events and Emit","text":"IntroductionTo explain the implementation of Vue.js component events and emit, it’s essential to first understand the application of component events and emit. This will provide a clearer understanding of how component events and emit are implemented. For component events, we’ve previously discussed the browser’s event mechanism: Browser Event ReviewBrowser events refer to various signals of interaction and state changes that occur on a web page. These events can be triggered by user actions, changes in the browser’s state, or other factors. Here are some common DOM events and their descriptions: Mouse Events: click: Triggered when the mouse clicks on an element. For touch-screen devices, it’s triggered when there’s a tap on the screen. contextmenu: Triggered when the right mouse button clicks on an element. mouseover &#x2F; mouseout: Triggered when the mouse pointer enters or leaves an element. mousedown &#x2F; mouseup: Triggered when the mouse button is pressed or released over an element. mousemove: Triggered when the mouse is moved. Keyboard Events: keydown and keyup: Triggered when a key is pressed down or released. Form Element Events: submit: Triggered when a visitor submits a &lt;form&gt;. focus: Triggered when a visitor focuses on an element, such as an &lt;input&gt;. Document Events: DOMContentLoaded: Triggered when HTML is fully loaded and processed, and the DOM is completely constructed. CSS Events: transitionend: Triggered when a CSS animation is completed. These events can be captured and processed using event listeners in JavaScript. By adding event listeners to elements, specific code can be executed when certain events occur. Events form the foundation of interaction and responsiveness in web development. Now, how does component events differ from browser events? In the context of events, we can understand them as specific occurrences at a given point in time, signaling that something has happened. An event is a signal that can be received and processed, and that’s what events are. In the case of component events, it refers to the communication between Vue components, achieved through events. In this context, events serve as a means of communication between components, and this communication is implemented using events. Here, events specifically refer to what we call component events. The primary mechanism for communication between Vue components using events is through the use of the emit and on methods. When a child component needs to communicate with a parent component, it triggers an event and uses the emit method to send the event. The parent component, in turn, uses the v-on directive in the template to listen for this event and execute the corresponding logic. &lt;!-- Child Component ChildComponent.vue --> &lt;template> &lt;button @click=\"sendMessage\">Notify Parent Component&lt;/button> &lt;/template> &lt;script> export default &#123; methods: &#123; sendMessage() &#123; // Use emit to send a custom event this.$emit('child-event', 'Hello from child!'); &#125; &#125; &#125; &lt;/script> &lt;!-- Parent Component ParentComponent.vue --> &lt;template> &lt;div> &lt;child-component @child-event=\"handleChildEvent\">&lt;/child-component> &lt;p>Message received from child: &#123;&#123; messageFromChild &#125;&#125;&lt;/p> &lt;/div> &lt;/template> &lt;script> import ChildComponent from './ChildComponent.vue'; export default &#123; components: &#123; ChildComponent &#125;, data() &#123; return &#123; messageFromChild: '' &#125;; &#125;, methods: &#123; handleChildEvent(message) &#123; // Listen to the child component's event using v-on this.messageFromChild = message; &#125; &#125; &#125; &lt;/script> Vue’s event mechanism is built on the publish&#x2F;subscribe pattern. $emit is used to publish (trigger) an event, while v-on is used to subscribe (listen) to events. This enables decoupling between different components. Firstly, we notice that the child component uses this.$emit(&#39;child-event&#39;, &#39;Hello from child!&#39;) to send a custom event named ‘child-event’. The parent component, on the other hand, listens to this event using @child-event=&quot;handleChildEvent&quot;. Here, ‘child-event’ is the custom event name, and ‘handleChildEvent’ is a method defined in the parent component to handle the message sent by the child component. When using a custom MyComponent component, we can listen to the emitted custom event using the v-on directive: You can see that the custom event ‘change’ is compiled into a property named ‘onChange’ and stored in the props data object. This is essentially a convention. As a framework designer, you can design the compilation result of events according to your expectations. In the actual implementation, emitting a custom event boils down to finding the corresponding event handling function in the props data object based on the event name, as shown in the following code: Implementation of emit// Definition of MyComponent component const MyComponent = &#123; name: 'MyComponent', setup(props, &#123; emit &#125;) &#123; // Emit the 'change' event and pass two parameters to the event handler emit('change', 1, 2); return () => &#123; // Render logic of the component return /* ... */; &#125;; &#125; &#125;; The above code will be compiled into a node CompNode, with the type being the MyComponent component. The emit function is passed to the setup function of the component, as shown below: // Definition of a Vue component node CompNode const CompNode = &#123; // Specify the type of the node as the MyComponent component type: MyComponent, // Props object passed to the component props: &#123; // In the MyComponent component, there will be an onChange event handling function, whose value is handler onChange: handler &#125; &#125;; As you can see, the custom event ‘change’ is compiled into a property named ‘onChange’ and stored in the props data object. This is essentially a convention. As a framework designer, you can design the compilation result of events according to your expectations. In the actual implementation, emitting a custom event essentially involves finding the corresponding event handling function in the props data object based on the event name, as shown in the following code: // Mounting the component function mountComponent(vnode, container, anchor) &#123; // Omitted code const instance = &#123; state, // State props: shallowReactive(props), // Reactive handling of props isMounted: false, subTree: null &#125;; // Define the emit function, which takes two parameters // event: Event name // payload: Parameters passed to the event handling function function emit(event, ...payload) &#123; // Process the event name according to the convention, e.g., change --> onChange const eventName = `on$&#123;event[0].toUpperCase()&#125;$&#123;event.slice(1)&#125;`; // Find the corresponding event handling function in props based on the processed event name const handler = instance.props[eventName]; if (handler) &#123; // Call the event handling function and pass the parameters handler(...payload); &#125; else &#123; console.error('Event does not exist'); &#125; &#125; // Add the emit function to setupContext const setupContext = &#123; attrs, emit &#125;; // Omitted code &#125; // Function to parse props data, with special handling for event-type props function resolveProps(options, propsData) &#123; const props = &#123;&#125;; const attrs = &#123;&#125;; for (const key in propsData) &#123; // Add props starting with 'on' as strings to props data, otherwise add to attrs if (key in options || key.startsWith('on')) &#123; props[key] = propsData[key]; &#125; else &#123; attrs[key] = propsData[key]; &#125; &#125; return [props, attrs]; &#125; The implementation principle of emit involves two aspects: the injection of setupContext and the convention-based transformation of event names. Injection of setupContext: In Vue components, the setup function receives two parameters: props and context. context includes a series of properties and methods, one of which is the emit function. In Vue 3 components, the setup function returns an object, and you can add the emit function to setupContext so that users can access it within the component using setupContext.emit. Here’s a simple example demonstrating how to add emit to setupContext in the setup function: setup(props, context) &#123; // Add emit to setupContext context.emit = emit; // Other setup logic // ... // Return the object returned by setup return &#123;&#125;; &#125; This way, users can call the emit function within the component using setupContext.emit. Convention-based transformation of event names: Inside the emit function, to match event handling functions in the component template, event names need to be conventionally transformed. Vue uses a convention of converting event names to camelCase. For example, the change event becomes onChange. This allows users to listen to events in the component template using camelCase. Here’s a simple example illustrating the convention-based transformation of event names: function emit(event, ...payload) &#123; // Process the event name according to the convention, e.g., change --> onChange const eventName = `on$&#123;event[0].toUpperCase()&#125;$&#123;event.slice(1)&#125;`; // Find the corresponding event handling function in props based on the processed event name const handler = instance.props[eventName]; if (handler) &#123; // Call the event handling function and pass the parameters handler(...payload); &#125; else &#123; console.error('Event does not exist'); &#125; &#125; In this example, the emit function transforms the event name into camelCase with “on” at the beginning. For instance, change becomes onChange. It then looks for the corresponding event handling function in instance.props and executes it. It’s important to note that event-type props are not found in instance.props, so they are stored in attrs. To address this, when parsing props data, event-type props are specially handled to ensure they are correctly added to props instead of attrs. This allows the emit function to correctly find event handling functions in instance.props.","link":"/2023/12/08/en/vue-component-2/"},{"title":"Nested Effects and Effect Stack","text":"IntroductionEffect functions can be nested. But why is this design choice made? Nested Effectseffect(function effectFn1() &#123; effect(function effectFn2() &#123; /* ... */ &#125;) /* ... */ &#125;) In the code above, effectFn1 is nested within effectFn2. The execution of effectFn1 will trigger the execution of effectFn2. So, when do we encounter nested effects? Take Vue.js, for example; Vue.js’s rendering function is executed within an effect. When components are nested, for instance, when the Foo component renders the Bar component: // Bar component const Bar = &#123; render() &#123;/* ... */ &#125;, &#125; // Foo component renders the Bar component const Foo = &#123; render() &#123; return &lt;Bar /> &#125;// JSX syntax &#125; Nested effects occur in this scenario. It can be visualized as follows: effect(() => &#123; Foo.render() // Nested effect(() => &#123; Bar.render()&#125; )&#125; ) The effect function can be nested, meaning an effect function can contain another effect function. When an outer effect function depends on reactive data created inside an inner effect function, the inner effect function is automatically tracked, ensuring that the outer effect function is executed when the inner effect function changes. This nested structure of effect functions establishes a dependency chain, ensuring that when reactive data changes, all the effect functions dependent on it are triggered, thereby maintaining the responsiveness of the application. The “effect stack” in Vue 3’s internal implementation is crucial. Vue uses an effect stack to track the currently executing effect functions, similar to a function call stack. This stack manages the execution order and dependency relationships of effect functions. Now, let’s consider an example of a nested effect function without using a stack structure. However, it cannot achieve the desired nesting functionality. Let’s assume we have two reactive data, count1 and count2, where the value of count2 depends on the value of count1. We can use nested effect functions to establish this dependency relationship. // Original data const data = &#123; foo: true, bar: true &#125;; // Proxy object const obj = new Proxy(data, &#123; get(target, key) &#123; console.log(`Reading property: $&#123;key&#125;`); return target[key]; &#125; &#125;); // Global variables let temp1, temp2; // effectFn1 is nested within effectFn2 effect(function effectFn1() &#123; console.log('effectFn1 executed'); effect(function effectFn2() &#123; console.log('effectFn2 executed'); // Access obj.bar property within effectFn2 temp2 = obj.bar; &#125;); // Access obj.foo property within effectFn1 temp1 = obj.foo; &#125;); effectFn1 is the outer effect function, which depends on the value of obj.foo. It contains an innerEffect within it. The inner effect function, effectFn2, depends on the value of obj.bar. When we modify obj.foo, the outer effect function should be triggered and output the value of obj.foo. When we modify obj.bar, the inner effect function should be triggered and output the value of obj.bar. We use the global variable activeEffect to store the effect functions registered through the effect function. This means that at any given time, activeEffect stores only one effect function. // Use a global variable to store the currently active effect function let activeEffect; function effect(fn) &#123; // Define the effect function const effectFn = () => &#123; // Call the cleanup function; specific implementation needs to be provided based on requirements cleanup(effectFn); // Assign the effect function to activeEffect activeEffect = effectFn; // Execute the effect function fn(); // Store the dependencies of the current effect function in effectFn.deps (Needs to be implemented based on the actual logic) effectFn.deps = []; // Set dependencies collection based on the actual logic &#125;; // Execute the effect function effectFn(); &#125; However, by only using a single variable for storage without employing a stack structure, when nested effects occur, the execution of the inner effect function will overwrite the value of activeEffect. It will never be restored to its original value. If there is a reactive data performing dependency collection, even if it is read within the outer effect function, the collected effect functions will all be from the inner effect function. In other words, when I read obj.foo, activeEffect still refers to the value of innerEffect, and only the innerEffect is triggered. To solve this issue, we need an effect function stack called effectStack. When an effect function is executed, the current effect function is pushed onto the stack. After the execution of the effect function is completed, it is popped from the stack. The activeEffect is always set to the top effect function on the stack. This ensures that a reactive data will only collect the effect function that directly reads its value, preventing mutual interference: // Use a global variable to store the currently active effect function let activeEffect; // Effect stack const effectStack = []; function effect(fn) &#123; const effectFn = () => &#123; cleanup(effectFn); // Call the cleanup function; specific implementation needs to be provided based on requirements activeEffect = effectFn; // Push the current effect function onto the stack effectStack.push(effectFn); fn(); // After the execution of the current effect function is completed, pop it from the stack, and restore activeEffect to its previous value effectStack.pop(); activeEffect = effectStack[effectStack.length - 1]; &#125;; // Initialize the dependencies collection of the effect function effectFn.deps = []; // Execute the effect function effectFn(); &#125; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.","link":"/2023/10/31/en/vue-effect/"},{"title":"vue-expired-side-effects","text":"IntroductionWhen we talk about race conditions, it typically refers to a concurrency problem in multi-process or multi-threaded programming. However, in frontend development, we might not directly encounter multi-threaded programming frequently, but we often face similar situations related to race conditions. A common example is in asynchronous programming, especially when dealing with asynchronous events, callback functions, or Promises. For instance, consider the following asynchronous code: let data; function fetchData() &#123; setTimeout(() => &#123; data = 'Fetched data'; &#125;, 1000); &#125; fetchData(); console.log(data); // Outputs undefined In this example, the fetchData function is asynchronous, and it assigns the data to the data variable after 1 second. However, due to JavaScript’s single-threaded nature, the fetchData function waits in the main thread’s event queue for 1 second. Within this 1 second, the console.log(data) statement executes immediately, and at that point, the value of data is undefined because the fetchData function has not completed yet. In asynchronous programming, due to the non-blocking nature of the code, similar race condition issues can arise. When dealing with asynchronous operations, it’s crucial to ensure data consistency and correctness, avoiding accessing or modifying related data before the asynchronous operation is completed. Race Conditions and ReactivitySo, how are race conditions related to reactivity? Consider the following example: let finalData; watch(obj, async () => &#123; // Send and wait for a network request const res = await fetch('/path/to/request'); // Assign the request result to data finalData = res; &#125;); In this code snippet, we use the watch function to observe changes to the obj object. Every time the obj object changes, a network request, such as an API call, is sent. After the data request is successful, the result is assigned to the finalData variable. At first glance, this code might seem fine. However, upon closer inspection, you’ll realize that this code can lead to race condition problems. Let’s assume we modify a field of the obj object for the first time, triggering the callback function and sending the first request A. As time passes, before the result of request A returns, we modify a field of the obj object again, triggering the second request B. Now, both request A and request B are in progress. Which request will return its result first? We don’t know. If request B completes before request A, the finalData variable will store the result of request B, making request A’s result outdated. However, because request B was sent later, we consider its data as the “latest.” Request A is deemed “expired,” and its result should be invalidated. By ensuring that request B’s result is considered the latest, we can prevent errors caused by race conditions. Essentially, what we need is a way to expire side effects. To illustrate this concept further, let’s replicate the scenario using the watch function in Vue.js to see how Vue.js helps developers address this problem. Later, we’ll attempt to implement this functionality ourselves. watch(obj, async (newValue, oldValue, onInvalidate) => &#123; // Define a flag to indicate whether the current side effect has expired, initially set to false (not expired) let expired = false; // Call the onInvalidate() function to register an expiration callback onInvalidate(() => &#123; // When expired, set the expired flag to true expired = true; &#125;); // Send a network request const res = await fetch('/path/to/request'); // Perform subsequent operations only if the side effect has not expired if (!expired) &#123; finalData = res; // Subsequent operations... &#125; &#125;); As shown in the code above, before sending the request, we define an expired flag variable to indicate whether the current side effect has expired. We then call the onInvalidate function to register an expiration callback. When the side effect expires, the expired flag is set to true. Finally, we use the request result only if the side effect has not expired, effectively avoiding the issue described earlier. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-expired-side-effects/"},{"title":"The Role and Implementation of Vue.js Reactive System","text":"IntroductionThe concept of reactivity is not difficult to understand. It refers to triggering certain events when JavaScript operates on an object or a value. This is achieved by implementing a reactive system, where operations elicit specific responses. Reactive Data and Side Effect FunctionsSide effect functions refer to functions that produce side effects. Consider the following code snippet: function effect() &#123; document.body.innerText = 'hello vue3'; &#125; When the effect function is executed, it sets the text content of the body. Any function other than effect can read or modify the body’s text content. In other words, the execution of the effect function directly or indirectly affects the execution of other functions, indicating that the effect function has side effects. Side effect functions are common and can impact various aspects, such as the effectiveness of “tree shaking” in webpack, a topic we discussed earlier. I won’t delve into this here. Side effects can easily occur; for example, if a function modifies a global variable, it creates a side effect, as shown in the following code: // Global variable let val = 1; function effect() &#123; val = 2; // Modifying a global variable creates a side effect &#125; Understanding side effect functions, let’s now discuss reactive data. Suppose a property of an object is read within a side effect function: const obj = &#123; text: 'hello world' &#125;; function effect() &#123; // The execution of the effect function reads obj.text document.body.innerText = obj.text; &#125; The side effect function effect sets the innerText property of the body element to obj.text. When the value of obj.text changes, we want the side effect function effect to re-execute. Our approach is as follows: we want to store the effect function in a bucket when reading the obj.text value, and when setting the obj.text value, we want to retrieve the effect function from the bucket and execute it. Basic Implementation of Reactive DataHow can we make obj reactive data? By observing, we find that: When the side effect function effect is executed, it triggers the read operation of the obj.text property. When the obj.text value is modified, it triggers the write operation of the obj.text property. We need to intercept the read and write operations of an object property. Before ES2015, this could only be done using the Object.defineProperty function, which was also the approach used in Vue.js 2. In ES2015 and later, we can use the Proxy object to achieve this, and this is the approach adopted in Vue.js 3. function createReactiveObject(target, proxyMap, baseHandlers) &#123; // The core is the proxy // The goal is to detect user get or set actions const existingProxy = proxyMap.get(target); if (existingProxy) &#123; return existingProxy; &#125; const proxy = new Proxy(target, baseHandlers); // Store the created proxy, proxyMap.set(target, proxy); return proxy; &#125; Here, we create a reactive object based on the Proxy object. We have a proxyMap, which is a container for storing various types of proxies. We define get and set intercept functions to intercept read and write operations. Designing a Comprehensive Reactive SystemFrom the above examples, it’s clear that the workflow of a reactive system is as follows: When a read operation occurs, collect the side effect functions into a “bucket”. When a write operation occurs, retrieve the side effect functions from the “bucket” and execute them. Next, I’ll explain the principles through a simple implementation of a reactive system. We know that the Proxy object can accept an object with getters and setters for handling get or set operations. Therefore, we can create a baseHandlers to manage getters and setters. // baseHandlers function createGetter(isReadonly = false, shallow = false) &#123; return function get(target, key, receiver) &#123; const res = Reflect.get(target, key, receiver); if (!isReadonly) &#123; // Collect dependencies when triggering get track(target, \"get\", key); &#125; return res; &#125;; &#125; function createSetter() &#123; return function set(target, key, value, receiver) &#123; const result = Reflect.set(target, key, value, receiver); // Trigger dependencies when setting values trigger(target, \"set\", key); return result; &#125;; &#125; We also need to establish a clear link between side effect functions and the target fields being operated upon. For instance, when reading a property, it doesn’t matter which property is being read; all side effect functions should be collected into the “bucket.” Similarly, when setting a property, regardless of which property is being set, the side effect functions from the “bucket” should be retrieved and executed. There is no explicit connection between side effect functions and the operated fields. The solution is simple: we need to establish a connection between side effect functions and the operated fields. This requires redesigning the data structure of the “bucket” and cannot be as simple as using a Set type for the “bucket.” We use WeakMap to implement the bucket for storing effects as discussed earlier. If you are not familiar with the characteristics of the WeakMap class, you can learn more about it here. // Map to store different types of proxies export const reactiveMap = new WeakMap(); export const readonlyMap = new WeakMap(); export const shallowReadonlyMap = new WeakMap(); Once we have defined the buckets as above, we can proceed to implement the core part of the reactive system, which is the proxy: function createReactiveObject(target, proxyMap, baseHandlers) &#123; // The core is the proxy // The goal is to detect user get or set actions const existingProxy = proxyMap.get(target); if (existingProxy) &#123; return existingProxy; &#125; const proxy = new Proxy(target, baseHandlers); // Store the created proxy proxyMap.set(target, proxy); return proxy; &#125; For the previous track and trigger functions: export function track(target, type, key) &#123; if (!isTracking()) &#123; return; &#125; console.log(`Trigger track -> target: $&#123;target&#125; type:$&#123;type&#125; key:$&#123;key&#125;`); // 1. Find the corresponding dep based on the target // Initialize depsMap if it's the first time let depsMap = targetMap.get(target); if (!depsMap) &#123; // Initialize depsMap logic depsMap = new Map(); targetMap.set(target, depsMap); &#125; let dep = depsMap.get(key); if (!dep) &#123; dep = createDep(); depsMap.set(key, dep); &#125; trackEffects(dep); &#125; export function trigger(target, type, key) &#123; // 1. Collect all deps and put them into deps array, // which will be processed later let deps: Array&lt;any> = []; const depsMap = targetMap.get(target); if (!depsMap) return; // Currently only GET type is implemented // for get type, just retrieve it const dep = depsMap.get(key); // Collect into deps array deps.push(dep); const effects: Array&lt;any> = []; deps.forEach((dep) => &#123; // Destructure dep to get the stored effects effects.push(...dep); &#125;); // The purpose here is to have only one dep that contains all effects // Currently, it should be reused for the triggerEffects function triggerEffects(createDep(effects)); &#125; Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/30/en/vue-reactive-1/"},{"title":"Vue Shallow Reactivity vs Deep Reactivity","text":"IntroductionExplore the differences between reactive and shallowReactive, delving into the concepts of deep reactivity and shallow reactivity in Vue.js. Shallow Reactivity vs Deep Reactivityconst obj = reactive(&#123; foo: &#123; bar: 1 &#125; &#125;) effect(() =>&#123; console.log(obj.foo.bar) &#125;) // Modifying obj.foo.bar value does not trigger reactivity obj.foo.bar = 2 Initially, an object obj is created with a property foo containing another object &#123; bar: 1 &#125;. When accessing obj.foo.bar inside an effect function, it is noticed that modifying obj.foo.bar does not trigger the effect function again. Why does this happen? Let’s take a look at the current implementation: function reactive(obj) &#123; return new Proxy(obj ,&#123; get(target, key, receiver) &#123; if (key === 'raw') return target; track(target, key); // When reading the property value, return it directly return Reflect.get(target, key, receiver) &#125; // Other trapping functions are omitted &#125;) &#125; In the given code, when accessing obj.foo.bar, it first reads the value of obj.foo. Here, Reflect.get is used to directly return the result of obj.foo. Since the result obtained through Reflect.get is a plain object, namely &#123; bar: 1 &#125;, it is not a reactive object. Therefore, when accessing obj.foo.bar inside the effect function, no reactivity is established. To address this, the result returned by Reflect.get needs to be wrapped: function reactive(obj) &#123; return new Proxy(obj, &#123; get(target, key, receiver) &#123; const result = Reflect.get(target, key, receiver); // If the result is an object, make it reactive if (typeof result === 'object') &#123; return reactive(result); &#125; return result; &#125;, // Other traps... &#125;); &#125; In this code snippet, the reactive function is defined. It takes an object as a parameter and returns a proxy of that object. The proxy uses a get trap function that triggers when accessing a property of the object. In the get trap, Reflect.get is used to retrieve the property value. If the result is an object, it is made reactive by calling the reactive function recursively. This ensures that nested objects also possess reactive properties, allowing modifications to trigger the reactivity system. Shallow ReactivityHowever, there are scenarios where deep reactivity is not desired, leading to the concept of shallowReactive or shallow reactivity. Shallow reactivity means that only the top-level properties of an object are reactive. For example: Suppose we have an object with a nested object as its property: let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; If we apply deep reactivity to obj: let reactiveObj = reactive(obj); Any modifications to obj or innerObj properties will trigger the reactivity system: reactiveObj.innerObj.key = 'new value'; // Triggers reactivity However, if we want only the top-level properties of obj to be reactive, meaning modifications to obj trigger reactivity but modifications to innerObj do not, we use the shallowReactive function: let shallowReactiveObj = shallowReactive(obj); With shallowReactive, only modifications to obj will trigger reactivity: shallowReactiveObj.innerObj = &#123;&#125;; // Triggers reactivity shallowReactiveObj.innerObj.key = 'new value'; // Does not trigger reactivity Vue.js and reactive vs shallowReactiveIn Vue.js, both reactive and shallowReactive functions are used to create reactive objects. Let’s explore their differences. The reactive function creates deeply reactive objects. This means that both the object itself and all its nested objects become reactive. Any modifications to the object or its nested objects’ properties will trigger the reactivity system. On the other hand, the shallowReactive function creates shallowly reactive objects. This means that only the top-level properties of the object are reactive. If the object contains nested objects, modifications to those nested objects’ properties will not trigger the reactivity system. let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; let reactiveObj &#x3D; Vue.reactive(obj);reactiveObj.innerObj.key &#x3D; ‘new value’; &#x2F;&#x2F; Triggers reactivity let shallowReactiveObj &#x3D; Vue.shallowReactive(obj);shallowReactiveObj.innerObj.key &#x3D; ‘new value’; &#x2F;&#x2F; Does not trigger reactivity Readonly and Shallow ReadonlyAfter discussing reactivity and shallow reactivity, let’s talk about readonly and shallow readonly: Vue.js provides readonly and shallowReadonly functions to create readonly reactive objects. The readonly function creates deeply readonly reactive objects. This means that both the object itself and all its nested objects are readonly. Any attempts to modify the object or its nested objects’ properties will fail. The shallowReadonly function creates shallow readonly reactive objects. This means that only the top-level properties of the object are readonly. If the object contains nested objects, properties of these nested objects can be modified. let obj = &#123; innerObj: &#123; key: 'value' &#125; &#125; let readonlyObj = Vue.readonly(obj); readonlyObj.innerObj.key = 'new value'; // This will fail because the object is readonly let shallowReadonlyObj = Vue.shallowReadonly(obj); shallowReadonlyObj.innerObj.key = 'new value'; // This will succeed because only top-level properties are readonly Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/01/en/vue-reactive-shallowReactive/"},{"title":"vue-renderer-1","text":"PrefaceIn Vue.js, many functionalities rely on renderers to be implemented, such as Transition components, Teleport components, Suspense components, as well as template refs and custom directives. Moreover, the renderer is the core of the framework’s performance, as its implementation directly affects the framework’s performance. Vue.js 3’s renderer not only includes the traditional Diff algorithm but also introduces a fast path update method, leveraging the information provided by the compiler, significantly improving update performance. In Vue.js, the renderer is responsible for executing rendering tasks. On the browser platform, it renders the virtual DOM into real DOM elements. The renderer can render not only real DOM elements but also plays a key role in the framework’s cross-platform capabilities. When designing a renderer, its customizable capabilities need to be considered. Basic Concepts and Meanings of RendererBefore implementing a basic renderer, we need to understand a few fundamental concepts: In Vue.js, a renderer is a component responsible for rendering virtual DOM (or virtual nodes) into real elements on a specific platform. On the browser platform, the renderer renders virtual DOM into real DOM elements. RendererThe renderer is responsible for rendering virtual DOM (or virtual nodes) into real elements on a specific platform. On the browser platform, the renderer renders virtual DOM into real DOM elements. Virtual DOM (vnode)The virtual DOM (also known as virtual nodes, abbreviated as vnode) is a tree-like structure, similar to real DOM, consisting of various nodes. The renderer’s task is to render the virtual DOM into real DOM elements. MountingMounting refers to rendering the virtual DOM into real DOM elements and adding them to a specified mounting point. In Vue.js, the mounted lifecycle hook of a component is triggered when the mounting is completed, and it can access the real DOM element at this point. ContainerThe container specifies the mounting position’s DOM element. The renderer renders the virtual DOM into real DOM elements and adds them to the specified container. In the renderer’s render function, a container parameter is usually passed in, indicating which DOM element the virtual DOM is mounted to. Creation and Usage of RendererThe renderer is usually created using the createRenderer function, which returns an object containing rendering and hydration functions. The hydration function is used in server-side rendering (SSR) to hydrate virtual DOM into existing real DOM elements. Here’s an example of creating and using a renderer: function createRenderer() &#123; function render(vnode, container) &#123; // Render logic &#125; function hydrate(vnode, container) &#123; // Hydration logic &#125; return &#123; render, hydrate &#125;; &#125; const &#123; render, hydrate &#125; = createRenderer(); // Initial rendering render(vnode, document.querySelector('#app')); // Server-side rendering hydrate(vnode, document.querySelector('#app')); In the above code, the createRenderer function creates a renderer object that contains the render and hydrate functions. The render function is used to render the virtual DOM into real DOM elements, while the hydrate function is used to hydrate the virtual DOM into existing real DOM elements. Now that we have a basic understanding of the renderer, let’s dive deeper step by step. The implementation of the renderer can be represented by the following function, where domString is the HTML string to be rendered, and container is the DOM element to mount to: function renderer(domString, container) &#123; container.innerHTML = domString; &#125; Example usage of the renderer: renderer('&lt;h1>Hello&lt;/h1>', document.getElementById('app')); In the above code, &lt;h1&gt;Hello&lt;/h1&gt; is inserted into the DOM element with the id app. The renderer can not only render static strings but also dynamic HTML content: let count = 1; renderer(`&lt;h1>$&#123;count&#125;&lt;/h1>`, document.getElementById('app')); If count is a reactive data, the reactivity system can automate the entire rendering process. First, define a reactive data count and then call the renderer function inside the side effect function to render: const count = ref(1); effect(() => &#123; renderer(`&lt;h1>$&#123;count.value&#125;&lt;/h1>`, document.getElementById('app')); &#125;); count.value++; In the above code, count is a ref reactive data. When modifying the value of count.value, the side effect function will be re-executed, triggering re-rendering. The final content rendered to the page is &lt;h1&gt;2&lt;/h1&gt;. Here, the reactive API provided by Vue 3’s @vue/reactivity package is used. It can be included in the HTML file using the &lt;script&gt; tag: &lt;script src=\"https://unpkg.com/@vue/reactivity@3.0.5/dist/reactivity.global.js\">&lt;/script> The basic implementation of the render function is given in the above code. Let’s analyze its execution flow in detail. Suppose we call the renderer.render function three times consecutively for rendering: const renderer = createRenderer(); // Initial rendering renderer.render(vnode1, document.querySelector('#app')); // Second rendering renderer.render(vnode2, document.querySelector('#app')); // Third rendering renderer.render(null, document.querySelector('#app')); During the initial rendering, the renderer renders vnode1 into real DOM and stores vnode1 in the container element’s container.vnode property as the old vnode. During the second rendering, the old vnode exists (container.vnode has a value), and the renderer takes vnode2 as the new vnode, passing both the new and old vnodes to the patch function to perform patching. During the third rendering, the new vnode’s value is null, indicating that no content should be rendered. However, at this point, the container already contains the content described by vnode2, so the renderer needs to clear the container. In the code above, container.innerHTML = &#39;&#39; is used to clear the container. It’s important to note that clearing the container this way is not the best practice but is used here for demonstration purposes. Regarding the patch function, it serves as the core entry point of the renderer. It takes three parameters: the old vnode n1, the new vnode n2, and the container container. During the initial rendering, the old vnode n1 is undefined, indicating a mounting action. The patch function not only serves for patching but can also handle mounting actions. Custom RendererThe implementation of a custom renderer involves abstracting the core rendering logic, making it independent of specific platform APIs. The following example demonstrates the implementation of a custom renderer using configuration options to achieve platform-independent rendering: // Create a renderer function, accepting options as parameters function createRenderer(options) &#123; // Retrieve DOM manipulation APIs from options const &#123; createElement, insert, setElementText &#125; = options; // Define the function to mount elements function mountElement(vnode, container) &#123; // Call createElement function to create an element const el = createElement(vnode.type); // If children are a string, use setElementText to set text content if (typeof vnode.children === 'string') &#123; setElementText(el, vnode.children); &#125; // Call insert function to insert the element into the container insert(el, container); &#125; // Define the function to patch elements function patch(n1, n2, container) &#123; // Implement patching logic, this part is omitted in the example &#125; // Define the render function, accepting virtual nodes and a container as parameters function render(vnode, container) &#123; // If the old virtual node exists, execute patching logic; otherwise, execute mounting logic if (container.vnode) &#123; patch(container.vnode, vnode, container); &#125; else &#123; mountElement(vnode, container); &#125; // Store the current virtual node in the container's vnode property container.vnode = vnode; &#125; // Return the render function return render; &#125; // Create configuration options for the custom renderer const customRendererOptions = &#123; // Function for creating elements createElement(tag) &#123; console.log(`Creating element $&#123;tag&#125;`); // In a real application, you can return a custom object to simulate a DOM element return &#123; type: tag &#125;; &#125;, // Function for setting an element's text content setElementText(el, text) &#123; console.log(`Setting text content of $&#123;JSON.stringify(el)&#125;: $&#123;text&#125;`); // In a real application, set the object's text content el.textContent = text; &#125;, // Function for inserting an element under a given parent insert(el, parent, anchor = null) &#123; console.log(`Adding $&#123;JSON.stringify(el)&#125; to $&#123;JSON.stringify(parent)&#125;`); // In a real application, insert el into parent parent.children = el; &#125;, &#125;; // Create a render function using the custom renderer's configuration options const customRenderer = createRenderer(customRendererOptions); // Create a virtual node describing &lt;h1>hello&lt;/h1> const vnode = &#123; type: 'h1', children: 'hello', &#125;; // Use an object to simulate a mounting point const container = &#123; type: 'root' &#125;; // Render the virtual node to the mounting point using the custom renderer customRenderer(vnode, container); In the above code, we create a custom renderer using the createRenderer function, which takes configuration options as parameters. The configuration options include functions for creating elements, setting text content, and inserting elements into a parent. The customRenderer function takes a virtual node and a container as parameters, and it can handle both mounting and patching logic based on the existence of the old virtual node (container.vnode). This custom renderer allows platform-independent rendering by abstracting the core logic and making it adaptable to different platforms through configuration options. Please note that the code above demonstrates the concept of a custom renderer and focuses on its implementation logic. In a real-world scenario, you might need additional error handling, optimizations, and proper DOM manipulations based on the specific platform requirements.","link":"/2023/11/05/en/vue-renderer-1/"},{"title":"Vue Render Mounting and Updating","text":"IntroductionVue.js templates are powerful and can meet most of our application needs. However, in certain scenarios, such as creating dynamic components based on input or slot values, the render function can be a more flexible solution. Developers familiar with the React ecosystem might already be acquainted with render functions, commonly used in JSX to construct React components. While Vue render functions can also be written in JSX, this discussion focuses on using plain JavaScript. This approach simplifies understanding the fundamental concepts of the Vue component system. Every Vue component includes a render function. Most of the time, this function is created by the Vue compiler. When a template is specified for a component, the Vue compiler processes the template’s content, ultimately generating a render function. This render function produces a virtual DOM node, which Vue renders in the browser DOM. This brings us to the concept of the virtual DOM. But what exactly is the virtual DOM? The virtual Document Object Model (or “DOM”) enables Vue to render components in its memory before updating the browser. This approach enhances speed and avoids the high cost associated with re-rendering the DOM. Since each DOM node object contains numerous properties and methods, pre-rendering them in memory using the virtual DOM eliminates the overhead of creating DOM nodes directly in the browser. When Vue updates the browser DOM, it compares the updated virtual DOM with the previous virtual DOM. Only the modified parts of the virtual DOM are used to update the actual DOM, reducing the number of element changes and enhancing performance. The render function returns virtual DOM nodes, often referred to as VNodes in the Vue ecosystem. These objects enable Vue to write these nodes into the browser DOM. They contain all the necessary information Vue needs. Mounting Child Nodes and Element AttributesWhen vnode.children is a string, it sets the element’s text content. An element can have multiple child elements besides text nodes. To describe an element’s child nodes, vnode.children needs to be defined as an array: const vnode = &#123; type: 'div', children: [ &#123; type: 'p', children: 'hello' &#125; ] &#125;; In the above code, we describe “a div tag with a child node, which is a p tag.” As seen, vnode.children is an array, and each element of the array is an independent virtual node object. This creates a tree-like structure, or a virtual DOM tree. To render child nodes, we need to modify the mountElement function, as shown in the following code: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); if (typeof vnode.children === 'string') &#123; setElementText(el, vnode.children); &#125; else if (Array.isArray(vnode.children)) &#123; // If `children` is an array, iterate through each child node and call the `patch` function to mount them vnode.children.forEach(child => &#123; patch(null, child, el); &#125;); &#125; insert(el, container); &#125; In this code, we have added a new conditional branch. We use the Array.isArray function to check if vnode.children is an array. If it is, we loop through each child node and call the patch function to mount the virtual nodes in the array. During mounting of child nodes, we need to pay attention to two points: The first argument passed to the patch function is null. Since this is the mounting phase and there is no old vnode, we only need to pass null. This way, when the patch function is executed, it will recursively call the mountElement function to mount the child nodes. The third argument passed to the patch function is the mounting point. Since the child elements being mounted are child nodes of the div element, the div element created earlier serves as the mounting point to ensure that these child nodes are mounted in the correct position. After mounting the child nodes, let’s look at how to describe the attributes of an element using vnode and how to render these attributes. We know that HTML elements have various attributes, some of which are common, such as id and class, while others are specific to certain elements, such as the action attribute for form elements. In this discussion, we will focus on the basic attribute handling. To describe the attributes of an element, we need to define a new field in the virtual DOM called vnode.props, as shown in the following code: const vnode = &#123; type: 'div', props: &#123; id: 'foo' &#125;, children: [ &#123; type: 'p', children: 'hello' &#125; ] &#125;; vnode.props is an object where the keys represent the attribute names of the element, and the values represent the corresponding attribute values. This way, we can iterate through the props object and render these attributes onto the element, as shown in the following code: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); // Skip children handling for now // Only handle `vnode.props` if it exists if (vnode.props) &#123; // Iterate through `vnode.props` object for (const key in vnode.props) &#123; // Use `setAttribute```javascript // Use `setAttribute` to set attributes on the element el.setAttribute(key, vnode.props[key]); &#125; &#125; insert(el, container); &#125; In this code snippet, we first check if vnode.props exists. If it does, we iterate through the vnode.props object and use the setAttribute function to set attributes on the element. This approach ensures that the attributes are rendered onto the element during the mounting process. When dealing with attributes, it’s essential to understand the distinction between HTML Attributes and DOM Properties. HTML Attributes are the attributes defined in the HTML tags, such as id=&quot;my-input&quot;, type=&quot;text&quot;, and value=&quot;foo&quot;. When the browser parses this HTML code, it creates a corresponding DOM element object, which we can access using JavaScript code: const el = document.querySelector('#my-input'); Now, let’s talk about DOM Properties. Many HTML Attributes have corresponding DOM Properties on the DOM element object, such as id=&quot;my-input&quot; corresponding to el.id, type=&quot;text&quot; corresponding to el.type, and value=&quot;foo&quot; corresponding to el.value. However, the names of DOM Properties don’t always exactly match HTML Attributes: &lt;div class=\"foo\">&lt;/div> In this case, class=&quot;foo&quot; corresponds to the DOM Property el.className. Additionally, not all HTML Attributes have corresponding DOM Properties: &lt;div aria-valuenow=\"75\">&lt;/div> Attributes with the aria-* prefix do not have corresponding DOM Properties. Similarly, not all DOM Properties have corresponding HTML Attributes. For example, you can use el.textContent to set the element’s text content, but there is no equivalent HTML Attribute for this operation. The values of HTML Attributes and DOM Properties are related. For example, consider the following HTML snippet: &lt;div id=\"foo\">&lt;/div> This snippet defines a div element with an id attribute. The corresponding DOM Property is el.id, and its value is the string &#39;foo&#39;. We consider this situation as a direct mapping, where the HTML Attribute and DOM Property have the same name (id in this case). However, not all HTML Attributes and DOM Properties have a direct mapping relationship. For example: &lt;input value=\"foo\" /> Here, the input element has a value attribute set to &#39;foo&#39;. If the user does not modify the input field, accessing el.value would return the string &#39;foo&#39;. If the user changes the input value to &#39;bar&#39;, accessing el.value would return &#39;bar&#39;. But if you run the following code: console.log(el.getAttribute('value')); // Still 'foo' console.log(el.value); // 'bar' You’ll notice that modifying the input value does not affect the return value of el.getAttribute(&#39;value&#39;). This behavior indicates the meaning behind HTML Attributes. Essentially, HTML Attributes are used to set the initial value of corresponding DOM Properties. Once the value changes, the DOM Properties always store the current value, while getAttribute retrieves the initial value. However, you can still access the initial value using el.defaultValue, as shown below: el.getAttribute('value'); // Still 'foo' el.value; // 'bar' el.defaultValue; // 'foo' This example illustrates that an HTML Attribute can be associated with multiple DOM Properties. In this case, value=&quot;foo&quot; is related to both el.value and el.defaultValue. Although HTML Attributes are considered as setting the initial values of corresponding DOM Properties, some values are restricted. It’s as if the browser internally checks for default value validity. If the value provided through HTML Attributes is invalid, the browser uses a built-in valid value for the corresponding DOM Properties. For example: &lt;input type=\"foo\" /> We know that specifying the string &#39;foo&#39; for the type attribute of the &lt;input/&gt; tag is invalid. Therefore, the browser corrects this invalid value. When you try to read el.type, you actually get the corrected value, which is &#39;text&#39;, not &#39;foo&#39;: console.log(el.type); // 'text' From the analysis above, we can see that the relationship between HTML Attributes and DOM Properties is complex. However, the core principle to remember is this: HTML Attributes are used to set the initial values of corresponding DOM Properties. How to Properly Set Element AttributesIn the previous discussion, we explored how HTML Attributes and DOM Properties are handled in Vue.js single-file components’ templates. In regular HTML files, the browser automatically parses HTML Attributes and sets the corresponding DOM Properties. However, in Vue.js templates, the framework needs to handle the setting of these attributes manually. Firstly, let’s consider a disabled button as an example in plain HTML: &lt;button disabled>Button&lt;/button> The browser automatically disables this button and sets its corresponding DOM Property el.disabled to true. However, if the same code appears in a Vue.js template, the behavior would be different. In Vue.js templates, the HTML template is compiled into virtual nodes (vnode). The value of props.disabled in the virtual node is an empty string. If you use the setAttribute function directly to set the attribute, unexpected behavior occurs, and the button becomes disabled. For example, in the following template: &lt;button disabled=\"false\">Button&lt;/button> The corresponding virtual node is: const button = &#123; type: 'button', props: &#123; disabled: false &#125; &#125;; If you use the setAttribute function to set the attribute value to an empty string, it is equivalent to: el.setAttribute('disabled', ''); However, the el.disabled property is of boolean type and does not care about the specific value of HTML Attributes; it only checks for the existence of the disabled attribute. So, the button becomes disabled. Therefore, renderers should not always use the setAttribute function to set attributes from the vnode.props object. To solve this issue, a better approach is to prioritize setting the element’s DOM Properties. However, if the value is an empty string, manually correct it to true. Here is an implementation example: function mountElement(vnode, container) &#123; const el = createElement(vnode.type); if (vnode.props) &#123; for (const key in vnode.props) &#123; if (key in el) &#123; const type = typeof el[key]; const value = vnode.props[key]; if (type === 'boolean' &amp;&amp; value === '') &#123; el[key] = true; &#125; else &#123; el[key] = value; &#125; &#125; else &#123; el.setAttribute(key, vnode.props[key]); &#125; &#125; &#125; insert(el, container); &#125; In this code, we first check if the property exists on the DOM element. If it does, we determine the type of the property and the value from vnode.props. If the property is of boolean type and the value is an empty string, we correct it to true. If the property does notexist on the DOM element, we use the setAttribute function to set the attribute. However, there are still issues with this implementation. Some DOM Properties are read-only, such as el.form. To address this problem, we can create a helper function, shouldSetAsProps, to determine whether an attribute should be set as DOM Properties. If the property is read-only or requires special handling, we should use the setAttribute function to set the attribute. Finally, to make the attribute setting operation platform-agnostic, we can extract the attribute-related operations into the renderer options. Here is the updated code: const renderer = createRenderer(&#123; createElement(tag) &#123; return document.createElement(tag); &#125;, setElementText(el, text) &#123; el.textContent = text; &#125;, insert(el, parent, anchor = null) &#123; parent.insertBefore(el, anchor); &#125;, patchProps(el, key, prevValue, nextValue) &#123; if (shouldSetAsProps(el, key, nextValue)) &#123; const type = typeof el[key]; if (type === 'boolean' &amp;&amp; nextValue === '') &#123; el[key] = true; &#125; else &#123; el[key] = nextValue; &#125; &#125; else &#123; el.setAttribute(key, nextValue); &#125; &#125; &#125;); In the mountElement function, we only need to call the patchProps function and pass the appropriate parameters. This way, we’ve extracted the attribute-related rendering logic from the core renderer, making it more maintainable and flexible. Please note that the shouldSetAsProps function should be implemented according to your specific requirements and the DOM properties you want to handle differently.","link":"/2023/11/05/en/vue-renderer-2/"},{"title":"Handling Event Rendering in Vue","text":"IntroductionIn this section, we will discuss how to handle events in Vue, including how to describe events in virtual nodes, how to add events to DOM elements, and how to update events. Let’s start by addressing the first question, which is how to describe events in virtual nodes. Events can be considered as special attributes, so we can agree that any attribute starting with the string “on” in the vnode.props object should be treated as an event. For example: const vnode = &#123; type: 'p', props: &#123; // Describe events using onXxx onClick: () => &#123; alert('clicked'); &#125; &#125;, children: 'text' &#125;; Once we have resolved how events are described in virtual nodes, let’s see how to add events to DOM elements. This is very simple, just call the addEventListener function in the patchProps method to bind the event, as shown in the following code: function patchProps(el, key, prevValue, nextValue) &#123; // Match attributes starting with on as events if (/^on/.test(key)) &#123; // Get the corresponding event name based on the attribute name, e.g., onClick ---> click const name = key.slice(2).toLowerCase(); // Remove the previously bound event handler prevValue &amp;&amp; el.removeEventListener(name, prevValue); // Bind the new event handler el.addEventListener(name, nextValue); &#125; else if (key === 'class') &#123; // Omitted code (handling class attribute logic) &#125; else if (shouldSetAsProps(el, key, nextValue)) &#123; // Omitted code (handling other attribute logic) &#125; else &#123; // Omitted code (handling other attribute logic) &#125; &#125; In fact, the event update mechanism can be further optimized to avoid multiple calls to removeEventListener and addEventListener. function patchProps(el, key, prevValue, nextValue) &#123; if (/^on/.test(key)) &#123; const name = key.slice(2).toLowerCase(); let invoker = el.__vei || (el.__vei = &#123;&#125;); if (nextValue) &#123; if (!invoker[name]) &#123; // If there is no invoker, create a fake invoker function invoker[name] = (e) => &#123; invoker[name].value(e); &#125;; &#125; // Assign the actual event handler to the value property of the invoker function invoker[name].value = nextValue; // Bind the invoker function as the event handler el.addEventListener(name, invoker[name]); &#125; else if (invoker[name]) &#123; // If the new event handler does not exist and the previously bound invoker exists, remove the binding el.removeEventListener(name, invoker[name]); invoker[name] = null; &#125; &#125; else if (key === 'class') &#123; // Omitted code (handling class attribute logic) &#125; else if (shouldSetAsProps(el, key, nextValue)) &#123; // Omitted code (handling other attribute logic) &#125; else &#123; // Omitted code (handling other attribute logic) &#125; &#125; Looking at the above code, event binding is divided into two steps. First, read the corresponding invoker from el._vei. If invoker does not exist, create a fake invoker function and cache it in el._vei. Assign the actual event handler to the invoker.value property, and then bind the fake invoker function as the event handler to the element. When the event is triggered, the fake event handler is executed, indirectly invoking the actual event handler invoker.value(e). When updating events, since el._vei already exists, we only need to modify the value of invoker.value to the new event handler. This way, updating events can avoid a call to removeEventListener, improving performance. However, the current implementation still has issues. The problem is that el._vei currently caches only one event handler at a time. This means that if an element binds multiple events simultaneously, event override will occur. const vnode = &#123; type: 'p', props: &#123; // Describe events using onXxx onClick: () => &#123; alert('clicked'); &#125;, onContextmenu: () => &#123; alert('contextmenu'); &#125; &#125;, children: 'text' &#125;; // Assume renderer is your renderer object renderer.render(vnode, document.querySelector('#app')); When the renderer tries to render the vnode provided in the above code, it first binds the click event and then binds the contextmenu event. The contextmenu event handler bound later will override the click event handler. To solve the event override problem, we need to redesign the data structure of el._vei. We should design el._vei as an object, where the keys are event names and the values are corresponding event handler functions. This way, event override issues will be resolved. Based on the code snippet you provided, this code is mainly used for handling attribute updates on DOM elements, including the logic for event binding and unbinding. In this code, it uses an el._vei object to cache event handler functions.","link":"/2023/11/05/en/vue-renderer-3/"},{"title":"vue-watch-computed","text":"IntroductionPreviously, we discussed the effect function, which is used to register side-effect functions. It allows specifying options parameters, such as the scheduler to control the timing and manner of side-effect function execution. We also explored the track function for dependency tracking and the trigger function to re-execute side-effect functions. Combining these concepts, we can implement a fundamental and distinctive feature of Vue.js – computed properties. Computed Properties and the lazy OptionIn Vue.js, the effect function is used to create reactive side-effect functions. By default, side-effect functions passed to effect are executed immediately. For example, in the code below, the side-effect function passed to the effect function is executed immediately: effect(() => &#123; console.log(obj.foo); &#125;); However, in certain cases, we want the side-effect function to execute only when needed, not immediately. A typical scenario is with computed properties. To achieve this delayed execution, we can add a lazy property to the options object and set it to true. When lazy is true, the side-effect function is not executed during initialization but only when necessary. The modified code looks like this: effect( // This function will not execute immediately () => &#123; console.log(obj.foo); &#125;, // options &#123; lazy: true &#125; ); In the implementation, the side-effect function effectFn is returned as the result of the effect function. This means that when we call the effect function, we get the corresponding side-effect function and can manually execute it when needed. This mechanism gives us more control, allowing us to decide when to trigger the execution of the side-effect function rather than executing it immediately. This design pattern is particularly suitable for specific scenarios like computed properties. In computed properties, we might want to trigger the side-effect function’s execution at a specific moment rather than immediately during initialization. By returning the side-effect function from the effect function, we can flexibly control when the side-effect function is executed to meet different requirements in various scenarios. function effect(fn, options = &#123;&#125;) &#123; const effectFn = () => &#123; cleanup(effectFn); activeEffect = effectFn; effectStack.push(effectFn); fn(); effectStack.pop(); activeEffect = effectStack[effectStack.length - 1]; &#125;; // Set options and dependencies for the side-effect function effectFn.options = options; effectFn.deps = []; // Execute the side-effect function only if it's not lazy if (!options.lazy) &#123; effectFn(); &#125; // Return the side-effect function as the result return effectFn; &#125; In this code, the effect function’s second parameter is an options object, where the lazy property is set to true. This means that the side-effect function passed to effect will be executed only when necessary, such as when accessing a computed property. The lazy property allows us to control the immediate execution of the side-effect function. Now that we have achieved lazy computation through computed properties, how do we implement data caching? function computed(getter) &#123; // value is used to cache the last computed value let value; // dirty flag indicates whether a recalculation is needed; if true, it means \"dirty\" and needs computation let dirty = true; const effectFn = effect(getter, &#123; lazy: true &#125;); const obj = &#123; get value() &#123; // Compute the value only if it's \"dirty,\" and cache the computed value in the value variable if (dirty) &#123; value = effectFn(); // Set dirty to false, so the cached value can be used directly next time dirty = false; &#125; return value; &#125; &#125;; return obj; &#125; With lazy computation resolved, the value is calculated only when it is truly needed, executing effectFn only when necessary. Additionally, a dirty flag is introduced to indicate whether the current computation needs to be recalculated. If dirty is true, the value is recalculated, and the dirty flag is set to false so that the cached value can be used directly next time. Implementation Principle of watchThe concept of watch essentially involves observing a reactive data and executing the corresponding callback function when the data changes. For example: watch(obj, () => &#123; console.log('Data changed'); &#125;) // Modifying the reactive data triggers the execution of the callback function obj.foo++ Suppose obj is a reactive data, watched using the watch function with a provided callback function. When modifying the reactive data’s value, the callback function is triggered. In fact, the implementation of watch essentially utilizes the effect and the options.scheduler option, as shown in the following code: effect(() => &#123; console.log(obj.foo) &#125;, &#123; scheduler() &#123; // The scheduler function is executed when obj.foo's value changes &#125; &#125;) In a side-effect function accessing the reactive data obj.foo, based on the previous discussion, we know that this establishes a connection between the side-effect function and the reactive data. When the reactive data changes, the side-effect function is re-executed. However, there is an exception: if the side-effect function has a scheduler option, when the reactive data changes, the scheduler function is executed instead of directly triggering the side-effect function. From this perspective, the scheduler function acts as a callback function, and the implementation of watch utilizes this characteristic. Below is the simplest implementation of the watch function: // The watch function receives two parameters: source (reactive data) and cb (callback function) function watch(source, cb) &#123; effect( // Trigger a read operation to establish a connection () => source.foo, &#123; scheduler: scheduler(), // Call the callback function cb when the data changes fn: () => &#123; cb(); &#125;, &#125; ); &#125; In this code, we first define an original data object named data, which contains a property foo with an initial value of 1. Next, we create a proxy object obj using Proxy, intercepting operations on the data. When obj.foo++ is executed, the set interceptor of Proxy is triggered. In the set interceptor, we first set the property value on the target object and then call the watch function, passing obj and a callback function. In the watch function, we use a hypothetical effect function (which might be provided by a framework) to listen for data changes. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/31/en/vue-watch-computed/"},{"title":"Approach for Coexistence Development of Vue 2/3","text":"IntroductionAfter December 31, 2023, the functionality of Vue 2 will still be available, but no further updates will be provided, including security updates and browser compatibility. Evan announced that the first RC of Vue 3 will be released in mid-July. This article suggests that library&#x2F;plugin authors start migrating their support to Vue 3. However, due to significant changes in API and behavior, is it possible to make our libraries support both Vue 2 and 3 simultaneously? Universal CodeThe simplest approach is to write universal code that works for both versions, without any additional modifications, similar to what people have done with Python 2 and 3. However, simplicity does not mean it’s easy. Writing such components requires avoiding newly introduced features in Vue 3 and deprecated features in Vue 2. In other words, you cannot use: Composition API .sync and .native modifiers Filters 3rd-party vendor objects Using BranchesThe response from core team members suggests using different branches to separate support for each targeted version. This is a good solution for existing and mature libraries, as their codebases are usually more stable, and version targeting optimizations may require better code isolation. The downside of this approach is that you need to maintain two codebases, which doubles your workload. It is not ideal for small libraries or new libraries that want to support both versions and avoid duplicating bug fixes or feature additions. I do not recommend using this approach from the beginning of a project. Build ScriptsIn VueUse, I wrote some build scripts to import code from the target version’s API during the build. After that, I need to publish two tags, vue2 and vue3, to differentiate the support for different versions. With this, I can write the code once and make the library support both Vue versions. The issue is that I need to build twice on each version and guide users to install the corresponding plugin versions (manually install @vue/composition-api for Vue 2). Approach for Coexistence Development of Vue 2&#x2F;3Simultaneous Support for Vue 2&#x2F;3 ProjectsPossible Scenarios for Vue 2&#x2F;3 ProjectsProgressive Migration: If there is a large Vue 2 project but you want to gradually migrate to Vue 3, you can choose to introduce Vue 3 into the project and then gradually migrate Vue 2 components to Vue 3. Compatibility with Dependency Libraries and Plugins: If the project depends on some Vue 2 plugins or libraries that have not been upgraded to Vue 3 yet, it may be necessary to use both Vue 2 and Vue 3 to ensure compatibility. Adopting Vue 3 for New Features: You may want to use Vue 3 in the project to take advantage of its new features and performance benefits while still keeping Vue 2 for old components or features. Project Integration: Based on experience requirements within the company, there is a need to present Vue 2&#x2F;3 projects on the same page. Internal Component Asset Maintainer: Support is needed for both Vue 2 and 3 projects, and the capabilities must be consistent. Legacy Application Developer: Need to use a third-party chart component, but only the Vue 3 version is available, not the Vue 2 version. Solutions1. Coexistence of Vue 2&#x2F;3 Projects Directly create a Vue 3 root instance using createApp and mount it to the Vue 2 root instance using the mount method. This allows using Vue 3 components in a Vue 2 project. The related code repository can be found here: vue5 // Vue 3 项⽬ import &#123; createApp &#125; from 'vue' import App from './App.vue' createApp(App).mount('#vue3') // Vue 2 项⽬ import Vue from 'vue2' import App from './App.vue' new Vue(&#123; render: h => h(App as any), &#125;).$mount('#vue2') The important thing about this approach is that we use the vite.config.ts to solve the compilation problem of different modules: I wrote some build scripts to import code from the target version’s API during the build. After that, I need to publish two tags vue2 and vue3 to differentiate the support for different versions. But the issue is that it requires guiding users to install the corresponding plugin versions on each version. This is not very friendly for developers to deal with package conflicts. import path from 'path' import &#123; defineConfig &#125; from 'vite' import Vue2 from '@vitejs/plugin-vue2' import Vue3 from '@vitejs/plugin-vue' import Inspect from 'vite-plugin-inspect' import compiler from 'vue2/compiler-sfc' const src = path.resolve(__dirname, 'src') export default defineConfig(&#123; plugins: [ Vue3(&#123; include: [/vue3[/\\\\].*\\.vue$/], &#125;), Vue2(&#123; include: [/vue2[/\\\\].*\\.vue$/], compiler: compiler as any, &#125;), Inspect(), ], &#125;) Here we have separated Vue 2 and Vue 3 into two independent packages and configured different compilation rules in vite.config.ts, allowing us to use Vue 2 and Vue 3 in the same page. 2. JessicaSachs&#x2F;petite SolutionLet’s briefly introduce petite: Petite is a subjective GitHub template built for Vue component authors. It sets up the tools needed for developing, documenting, and testing common SFC components and is backward compatible with Vue 2.7 runtime. This is achieved through some runtime helper functions and a very opinionated monolithic library structure. Petite sets up Vite, Volar, Linting, Vitepress, TypeScript, and Testing, allowing you to choose to write Vue 3-style code while easily maintaining backward compatibility with Vue 2.x users. This also means that you will be publishing two versions of your package on npm instead of breaking major versions to support Vue 2 or Vue 3. The downside of this approach is that your users will need to install the new version when upgrading and changing imports. The benefit is that you can write backward-compatible code more easily and provide regular updates for users. Additionally, you can also separate dependencies for Vue 2-only and Vue 3-only. If you use lodash in your shared code, you will need to run pnpm build in the workspace root directory, and each package (lib-vue3, lib-vue2) should be deployed independently. 3. vue-bridge Solution4. vue-demi SolutionRepository example: vue-demi Vue Demi is a development utility that allows you to write universal Vue libraries for both Vue 2 and 3 without worrying about the version installed by the user. When creating a Vue plugin&#x2F;library, simply install vue-demi as a dependency and import any Vue-related content from it. Publish your plugin&#x2F;library as usual, and your package will become universal! &#123; &quot;dependencies&quot;: &#123; &quot;vue-demi&quot;: &quot;latest&quot; &#125; &#125; import Vue, &#123; reactive, ref &#125; from 'vue-demi' At the underlying level, it uses the postinstall npm hook. After installing all the packages, the script will start checking the installed Vue version and redirect the exports based on the local Vue version. When using Vue 2, it will also automatically install @vue&#x2F;composition-api if it is not already installed. Points to note about libraries&#x2F;components: Libraries&#x2F;Components Single repository - Multiple package builds Dependency management Alias configuration NPM package names Build tool configuration Importing Vue 3 Components in a Vue 2 ApplicationThere are limitations to component interoperability Shared context Scoped slots Events Approach for importing Vue 3 components in a Vue 2 application Vue 3 can have multiple global instances Prerequisite: Upgrade Vue 2 to 2.7, remove outdated plugins from Vue CLI Interoperability layer: Custom Elements Build tool: Vite","link":"/2023/11/28/en/vue5/"},{"title":"A Brief Discussion on WebAssembly","text":"IntroductionAfter reading numerous articles on WebAssembly and conducting some performance tests, I’d like to share my insights on this technology. Is WASM Equivalent to Assembly-Level Performance?Certainly not. The assembly in WASM does not mean actual assembly code; it is a new bytecode with its own conventions that need an interpreter to run. This interpreter is much faster than a JavaScript interpreter but still falls short of native machine code performance. As a reference point, when JavaScript is optimized with Just-In-Time (JIT) compilation, its overall performance is roughly 1&#x2F;20th of machine code. In comparison, WASM can achieve about 1&#x2F;3rd of machine code performance (these figures vary depending on the context and are for reference purposes only). Even if you write code in languages like C++ and Rust, the performance you get is comparable to Java and C#, not native machine code. This explains why WASM does not demonstrate overwhelming performance advantages in all application scenarios: if you know how to optimize JS to run efficiently, it can compete with Rust in the browser environment. A classic case of performance comparison between WASM and JS occurred in a debate between Mozilla developers and V8 developers. Mozilla Hacks published an article titled “Optimizing Source Maps Performance with Rust and WebAssembly”, optimizing the performance of the source-map JavaScript package by five times. V8 core developer Vyacheslav Egorov responded with an article titled “You Might Not Need Rust and WebAssembly to Speed Up Your JS”, achieving astonishing optimizations in pure JS that outperformed Rust. The debate was intense, and the performance comparison chart after three rounds clearly showed Rust’s superiority, although JS managed to outperform in one round: Additionally, Milo Yip conducted performance tests on different languages for ray tracing (a highly intensive computation task), supporting the conclusion about performance comparisons between languages and machine code. C++, Java, and JS, without specific optimizations, can represent three typical performance levels: Language Rendering Comparison (C++&#x2F;C#&#x2F;F#&#x2F;Java&#x2F;JS&#x2F;Lua&#x2F;Python&#x2F;Ruby) Is WASM Faster Than JS, So It Should Be Used for Compute-Intensive Applications?This assumption is a bit biased. WASM is still processed on the CPU. For tasks that can be highly parallelized, using WebGL for GPU acceleration is often much faster. For instance, algorithms for image processing, as I discussed in my article [“Practical WebGL Image Processing Introduction”](link to the article), can easily be several tens of times faster by using WebGL than by looping through canvas pixels in JS. Rewriting such a nested loop in WASM to achieve a few times improvement over JS is already considered quite good. Regarding AI computations in the browser, community evaluations show that WebGL and WebMetal offer the highest performance levels, followed by WASM. Refer to this article: “Browser-Based AI Evaluations” However, WebGL acceleration has precision issues. For example, the core of the frontend image resizing library Pica uses the Lanczos sampling algorithm. I implemented this algorithm with WebGL shaders; it is not complicated. The early version of Pica once included optional WebGL optimizations, but now it has shifted to WASM. The reason is that WASM can ensure consistent computation results with JS for the same parameters, whereas WebGL cannot. For related discussions, see Issue #114 · nodeca&#x2F;pica Moreover, there are not many compute-intensive scenarios in frontend development. Tasks like encryption, compression, and mining are not high-frequency requirements. As for potentially essential AI applications in the future, I personally have confidence in WebGPU, the next-generation standard that can fully unleash GPU potential. However, WASM is already a good alternative. Does Embedding a WASM Function in JS Automatically Improve Performance?Not necessarily. Modern JS engines have powerful tools for performance optimization, namely JIT (Just-In-Time compilation). Simply put, if a function like the add function in the code const add = (a, b) =&gt; a + b consistently performs integer addition, the JS engine will automatically compile machine code to compute int a + int b, replacing the original JS function. This optimization significantly enhances the performance of frequent calls to this function. This is the magic of JIT (Just-In-Time) compilation. So, don’t assume that JS is slow and think of manually replacing such JS functions with C compiled to WASM to improve performance. Modern JS engines automatically “translate JS to C” like this for you! If you can rewrite a JS function into equivalent C code, it’s highly likely that this function, when inlined, will achieve similar performance through JIT compilation. This is probably why V8 developers confidently challenged Rust with JS in the debate I mentioned earlier. In the article “Calls between JavaScript and WebAssembly are Finally Fast 🎉”, Lin Clark eloquently discusses the optimization process. In the end, function calls between JS and WASM are faster than non-inlined JS function calls. However, the comparison between these calls and JS functions that are inlined by JIT is not mentioned in the article. It’s worth mentioning that Mozilla often promotes their massively optimized work, much of which might have stemmed from apparent design issues (let’s be honest; we all have our moments). For instance, the significant power-saving optimization in Firefox 70 for Mac was rooted in what exactly? A rough understanding is that the previous version of Firefox on Mac updated the window pixels for every frame! Of course, these articles contain substantial information, and I highly recommend reading the original texts after building a good foundation. It opens up a broader world and often inspires insights into software architecture design. If WASM supports garbage collection (GC) in the future, managing object lifecycles between JS and WASM may become more complicated. For example, I recently attempted to synchronize large objects between Dart in Flutter and Java in Android, hoping to “embed some Android platform capabilities into the Flutter ecosystem.” However, this approach led to a lot of verbose and low-performance glue code. Objects had to be deep-copied asynchronously through messages, with very low controllability. Although WASM currently does not have GC, once it’s added, I have reasons to suspect that managing object lifecycles between WASM and JS will face similar challenges. However, this problem mainly concerns Mozilla and Google developers; it’s not something we need to worry about. Is WASM Just Like Calling C from Python in Terms of Simplicity?This question can only be answered by practical experience. For example, I have recently attempted the following: Calling C++ from Java classes in Android Calling C from Dart in Flutter Calling C&#x2F;C++ from QuickJS, an embedded JS engine All of these cases involve creating native objects in the engine and passing them to C&#x2F;C++ functions by reference. This method is generally referred to as FFI (Foreign Function Interface), allowing native code to be embedded in language runtimes. However, if you are dealing with two different runtimes, things are not that simple. For instance, in the Quack project, which aims to bind QuickJS with Java, marshaling (similar to serialization and deserialization like JSON) has to be done between JS and Java objects; you cannot simply pass references. So, how does it work with WASM? Essentially, WASM’s linear memory can be freely read and written by JS, without the hassle of deep copying. However, WASM does present some challenges in terms of data flow. It only supports basic data types such as integers and floats; there is no support for complex data structures like strings. Thus, for slightly more complex objects, it’s challenging to manually define corresponding structures on both the JS and WASM sides. This difficulty makes it complicated to directly perform complex object transformations using WASM. Currently, this dirty work is left to tools like wasm-bindgen, which handles complex object transformations between languages. wasm-pack uses another tool called wasm-bindgen to bridge JavaScript and Rust, among other types. However, this process is not the same as directly embedding C&#x2F;C++ functions in JS runtime, as with traditional FFI compiled to machine code. For example, if you frequently manipulate JS objects with WASM, it can almost certainly impact performance. A typical pitfall in this regard is porting OpenGL applications to WASM. For example, a function like glTexImage2D in C++ now needs to go through two layers: first, it goes from WASM to JS in the glue layer, and then from JS to WebGL API like gl.texImage2D through C++ binding. This adds an extra layer of complexity compared to directly writing the equivalent JS code. Can this approach match the performance of writing JS directly instead of two layers of glue code? Of course, Mozilla is aware of this issue. Hence, they are exploring how to better expose Web IDL (the bindings of browser-native APIs) to WASM. In this process, they introduced the concept of WASM Interface Types: since WASM is already an intermediate bytecode, why not establish a universal Intermediate Representation (IR) specification that can unify all types across programming language runtimes? However, this specification hopes to solve problems mainly through protocolization and structured deep copying, with only the anyref type allowing passing by reference. anyref behaves somewhat like file descriptors in Unix; I won’t delve into this here. Is WASM Part of the Frontend Ecosystem?I do not agree with this statement. It’s essential to note that the toolchains for compiling WASM applications and the libraries they depend on have little to do with JS. A toolchain that supports cross-compilation typically comes with libraries supporting the target platform. For example, after including &lt;GLES2/gl2.h&gt;, the glTexImage2D API you call is provided by the dynamic library. This API can run consistently on x86, ARM, MIPS, WASM, etc., platforms (similar to .so files in Android). Emscripten provides a set of dynamic libraries specifically for the WASM platform, compiling them into JS format. However, it only guarantees that these APIs are available; performance is a different story. Emscripten also provides many optimization suggestions for porting WebGL applications. So, I’d like to reiterate that the dependencies and toolchains required to compile WASM applications are almost entirely unrelated to JS. JS is just a format produced by these toolchains, similar to machine code. From the perspective of JS developers, these toolchains may seem quite unfamiliar. Still, from the perspective of native application developers, everything is quite standard. ConclusionWebAssembly is undoubtedly a revolutionary technology, representing a new cross-platform direction, especially valuable for native application developers. However, for frontend developers, it’s just a bytecode virtual machine embedded in the browser. I hope this article clarifies some misconceptions and provides a better understanding of WebAssembly’s capabilities and limitations. While it’s a powerful tool, it’s essential to use it judiciously and consider its advantages and trade-offs within the context of your specific use case. Remember, WebAssembly is not a magic solution that automatically improves performance in all scenarios. It’s another option in the toolkit, providing a balance between performance, development cost, and effectiveness. As the technology evolves, it will be interesting to see how it integrates further into the broader web ecosystem. Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/11/03/en/wasm-1/"},{"title":"Webpack Custom Loader/Plugin","text":"IntroductionA loader is a node module exported as a function. This function is called when transforming resources in the loader. The given function will utilize the Loader API and can be accessed through the this context. Here is an official link on loader usage and examples, including local development and testing of custom loaders. Simple Usage of Webpack LoaderWhen a loader is used in a resource, it can only take one parameter - a string containing the content of the resource file. Synchronous loaders can return a single value representing the transformed module. Loaders can return one or two values. The first value is a string or buffer containing JavaScript code. The optional second value is a SourceMap, which is a JavaScript object. Here’s a simple example of using a loader. It matches all JavaScript files and processes them using loader.js: // webpack.config.js const path = require('path'); module.exports = &#123; //... module: &#123; rules: [ &#123; test: /\\.js$/, use: [ &#123; loader: path.resolve('path/to/loader.js'), options: &#123; /* ... */ &#125;, &#125;, ], &#125;, ], &#125;, &#125;; From the above, we can understand how loaders are used. But this only scratches the surface. What does a specific loader look like? For example, a simple loader could be like this: module.exports = function (content) &#123; // content is the source content string passed in return content &#125; A loader is just a node module exposing a function that can only receive one parameter: a string containing the content of the resource file. The function’s return value is the processed content. Creating a Custom Webpack LoaderGuidelines for Using Custom LoadersWhen writing loaders, you should follow these guidelines. They are listed in order of importance, and some apply only to specific scenarios. Please read the detailed sections below for more information. Keep it simple. Use chaining. Output should be modular. Ensure it’s stateless. Use loader utilities. Record loader dependencies. Resolve module dependencies. Extract common code. Avoid absolute paths. Use peer dependencies. Step 1: Create Project Directory and FilesFirst, create the following files in a folder within your webpack project directory: src/loader/custom-loader.js: The source file for your custom loader. src/index.js: JavaScript entry file for testing the custom loader. Step 2: Write the Custom LoaderIn the custom-loader.js file, write your custom loader code. This loader adds a comment at the top of each loaded JavaScript file. // src/loader/custom-loader.js module.exports = function(source) &#123; // Add a custom comment at the top of the source code const updatedSource = `/** Custom Comment added by Custom Loader */\\n$&#123;source&#125;`; return updatedSource; &#125;; Step 3: Configure WebpackCreate a Webpack configuration file webpack.config.js in the project root directory. Use the custom loader you just created in the configuration file. // webpack.config.js const path = require('path'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: path.resolve(__dirname, 'dist'), &#125;, module: &#123; rules: [ &#123; test: /\\.js$/, use: ['custom-loader'], // Use the custom loader to process .js files exclude: /node_modules/, &#125;, ], &#125;, &#125;; This configuration achieves a simple functionality. Now let’s discuss how to test the local loader. There are two ways to do this: one is through Npm link for testing, a convenient method where you can create a symbolic link for local testing. Here is a link to npm-link. Another way is to configure the path directly in the project: Single Loader Configuration// webpack.config.js &#123; test: /\\.js$/ use: [ &#123; loader: path.resolve('path/to/custom-loader.js'), options: &#123;/* ... */&#125; &#125; ] &#125; Multiple Loader ConfigurationYou can also configure it using an array: // webpack.config.js resolveLoader: &#123; // Look for loaders first in the node_modules directory; if not found, search in the loaders directory modules: [ 'node_modules', path.resolve(__dirname, 'custom-loader') ] &#125; Step 4: Test the Custom LoaderIn the index.js file, write some JavaScript code, for example: // src/index.js console.log('Hello, Webpack Loader!'); Step 5: Run Webpack BuildRun the following command to build your project: npx webpack --config webpack.config.js After the build is complete, you will find the generated bundle.js file in the dist folder. In this file, you can see JavaScript code with a custom comment added at the top. ## Simple Usage of Webpack Plugin Plugins provide complete control over the webpack engine for third-party developers. By introducing custom behaviors into the webpack build process through stage-based build callbacks, developers can customize webpack&#39;s behavior. Here&#39;s the simplest example: &#96;&#96;&#96;javascript &#x2F;&#x2F; webpack.config.js const HtmlWebpackPlugin &#x3D; require(&#39;html-webpack-plugin&#39;); module.exports &#x3D; &#123; entry: &#39;.&#x2F;src&#x2F;index.js&#39;, output: &#123; filename: &#39;bundle.js&#39;, path: __dirname + &#39;&#x2F;dist&#39;, &#125;, plugins: [ new HtmlWebpackPlugin(&#123; template: &#39;.&#x2F;src&#x2F;index.html&#39;, &#x2F;&#x2F; Specify the HTML template file filename: &#39;index.html&#39;, &#x2F;&#x2F; Generated HTML file name &#125;), &#x2F;&#x2F; You can add more plugins here ], &#125;; In this example, the HtmlWebpackPlugin is used. It generates a new HTML file based on the specified HTML template and automatically adds the bundled JavaScript file to the generated HTML file. A basic webpack plugin consists of the following components: A JavaScript named function or JavaScript class. Define an apply method on the plugin function’s prototype. The apply method is called when webpack loads the plugin and is passed the compiler object. Specify an event hook bound to webpack itself. Process specific data from webpack’s internal instances. Call the callback provided by webpack after the functionality is completed. A plugin structure looks like this: class HelloWorldPlugin &#123; apply(compiler) &#123; compiler.hooks.done.tap( 'Hello World Plugin', ( stats /* After binding the done hook, stats is passed as a parameter. */ ) => &#123; console.log('Hello World!'); &#125; ); &#125; &#125; module.exports = HelloWorldPlugin; Compiler and CompilationThe two most important resources in plugin development are the compiler and compilation objects. Plugin development revolves around hooks on these objects. The compiler object is essentially bound to the entire webpack environment. It contains all the environment configurations, including options, loaders, and plugins. When webpack starts, this object is instantiated and it is globally unique. The parameters passed into the apply method are properties of this object. The compilation object is created each time resources are built. It represents the current module resources, compiled generated resources, changed files, and tracked dependency status. It also provides many hooks. Creating a Custom Webpack PluginStep 1: Create Project Directory and FilesFirst, create the following file in a folder within your webpack project directory: src/plugins/CustomPlugin.js: Source file for your custom plugin. Step 2: Write the Custom PluginIn the CustomPlugin.js file, write a plugin that outputs a message when the webpack build process is completed. // src/plugins/CustomPlugin.js class CustomPlugin &#123; apply(compiler) &#123; compiler.hooks.done.tap('CustomPlugin', () => &#123; console.log('CustomPlugin: Webpack build process is done!'); &#125;); &#125; &#125; module.exports = CustomPlugin; Step 3: Configure WebpackIn the configuration file, use the custom plugin you just created. // webpack.config.js const CustomPlugin = require('./src/plugins/CustomPlugin'); module.exports = &#123; entry: './src/index.js', output: &#123; filename: 'bundle.js', path: __dirname + '/dist', &#125;, plugins: [ new CustomPlugin(), // You can add more plugins here ], &#125;; Step 4: Run Webpack BuildNow, run the webpack build: npx webpack --config webpack.config.js Note: This article is a translated version of the original post. For the most accurate and up-to-date information, please refer to the original source.&#96;&#96;&#96;","link":"/2023/10/29/en/webpack-plugin-design/"}],"tags":[{"name":"Essay","slug":"Essay","link":"/en/tags/Essay/"},{"name":"Vue,Browser,FE","slug":"Vue-Browser-FE","link":"/en/tags/Vue-Browser-FE/"},{"name":"http","slug":"http","link":"/en/tags/http/"},{"name":"Cloud-Computing","slug":"Cloud-Computing","link":"/en/tags/Cloud-Computing/"},{"name":"Vue,FE","slug":"Vue-FE","link":"/en/tags/Vue-FE/"},{"name":"Chrome,devtools","slug":"Chrome-devtools","link":"/en/tags/Chrome-devtools/"},{"name":"Webpack,FE","slug":"Webpack-FE","link":"/en/tags/Webpack-FE/"},{"name":"Ajax","slug":"Ajax","link":"/en/tags/Ajax/"},{"name":"JavaScript","slug":"JavaScript","link":"/en/tags/JavaScript/"},{"name":"Hadoop,Cloud-Computing","slug":"Hadoop-Cloud-Computing","link":"/en/tags/Hadoop-Cloud-Computing/"},{"name":"Vue","slug":"Vue","link":"/en/tags/Vue/"},{"name":"Wasm","slug":"Wasm","link":"/en/tags/Wasm/"},{"name":"Webpack,Front-end","slug":"Webpack-Front-end","link":"/en/tags/Webpack-Front-end/"},{"name":"Front-end","slug":"Front-end","link":"/en/tags/Front-end/"},{"name":"JIT compiler,JavaScript,WebAssembly","slug":"JIT-compiler-JavaScript-WebAssembly","link":"/en/tags/JIT-compiler-JavaScript-WebAssembly/"},{"name":"Build","slug":"Build","link":"/en/tags/Build/"}],"categories":[{"name":"Essay","slug":"Essay","link":"/en/categories/Essay/"},{"name":"FE","slug":"FE","link":"/en/categories/FE/"},{"name":"http","slug":"http","link":"/en/categories/http/"},{"name":"Cloud-Computing","slug":"Cloud-Computing","link":"/en/categories/Cloud-Computing/"},{"name":"Chrome","slug":"Chrome","link":"/en/categories/Chrome/"},{"name":"Webpack","slug":"Webpack","link":"/en/categories/Webpack/"},{"name":"Hadoop","slug":"Hadoop","link":"/en/categories/Hadoop/"},{"name":"devtools","slug":"Chrome/devtools","link":"/en/categories/Chrome/devtools/"},{"name":"Build","slug":"Build","link":"/en/categories/Build/"},{"name":"Vue","slug":"Vue","link":"/en/categories/Vue/"},{"name":"Cloud-Computing","slug":"Hadoop/Cloud-Computing","link":"/en/categories/Hadoop/Cloud-Computing/"},{"name":"Wasm","slug":"Wasm","link":"/en/categories/Wasm/"},{"name":"JIT compiler","slug":"JIT-compiler","link":"/en/categories/JIT-compiler/"}]}